{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf5KrEb6vrkR"
      },
      "source": [
        "**Art-Sytles-AI**\n",
        "\n",
        "Wir verwenden Google Colabs T4-GPU für das Trainieren der Daten via CycleGAN und zur Model Evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t63nj3g1PPon",
        "outputId": "7c192c67-80e6-4bc0-941c-1e7890400845"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Verbinden mit Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhhkYJNFQIZ2",
        "outputId": "86736d02-e0e6-42ab-8a22-7675a873d6aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'pytorch-CycleGAN-and-pix2pix'...\n",
            "remote: Enumerating objects: 2516, done.\u001b[K\n",
            "remote: Total 2516 (delta 0), reused 0 (delta 0), pack-reused 2516 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2516/2516), 8.20 MiB | 29.06 MiB/s, done.\n",
            "Resolving deltas: 100% (1575/1575), done.\n",
            "/content/pytorch-CycleGAN-and-pix2pix\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (0.21.0+cu124)\n",
            "Collecting dominate>=2.4.0 (from -r requirements.txt (line 3))\n",
            "  Downloading dominate-2.9.1-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting visdom>=0.1.8.8 (from -r requirements.txt (line 4))\n",
            "  Downloading visdom-0.2.4.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.4.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.4.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.4.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.4.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.4.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.4.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.4.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.4.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.4.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.4.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.4.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision>=0.5.0->-r requirements.txt (line 2)) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision>=0.5.0->-r requirements.txt (line 2)) (11.2.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.15.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (2.32.3)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.11/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (6.4.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.17.0)\n",
            "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.11/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.33)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.8.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (3.1.44)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (24.2)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (5.29.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (2.11.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (6.0.2)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (2.30.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (1.3.6)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 5)) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 5)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 5)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 5)) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (2025.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.4.0->-r requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch->visdom>=0.1.8.8->-r requirements.txt (line 4)) (3.0.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 5)) (5.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m119.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dominate-2.9.1-py2.py3-none-any.whl (29 kB)\n",
            "Building wheels for collected packages: visdom\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.2.4-py3-none-any.whl size=1408195 sha256=1f1f7ded36bb7ebcde23f7bdaef66bb680b441ea2f09c5386592dec08521d58b\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/a4/bb/2be445c295d88a74f9c0a4232f04860ca489a5c7c57eb959d9\n",
            "Successfully built visdom\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dominate, visdom, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed dominate-2.9.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 visdom-0.2.4\n"
          ]
        }
      ],
      "source": [
        "# Lade von CycleGAN und allen wichtigen Bibliotheken\n",
        "!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix.git\n",
        "\n",
        "%cd pytorch-CycleGAN-and-pix2pix\n",
        "!pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZC3CQ4qGRPji",
        "outputId": "2ef29619-d14d-44f8-d8de-377046e47be2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TrainA Pfad: /content/drive/MyDrive/ML4B_cycleGAN/trainA\n",
            "TrainB Pfad: /content/drive/MyDrive/ML4B_cycleGAN/trainB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Datenpfad definieren\n",
        "dataroot = \"/content/drive/MyDrive/ML4B_cycleGAN\"\n",
        "\n",
        "# Pfad zu trainA (Barock) und trainB (Realismus) erstellen\n",
        "trainA_path = os.path.join(dataroot, \"trainA\")\n",
        "trainB_path = os.path.join(dataroot, \"trainB\")\n",
        "\n",
        "# Pfade ausgeben\n",
        "print(f\"TrainA Pfad: {trainA_path}\")\n",
        "print(f\"TrainB Pfad: {trainB_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3kQ9KPXW74-",
        "outputId": "091d15fd-63ee-4279-d305-671fbe899e4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dominate in /usr/local/lib/python3.11/dist-packages (2.9.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install dominate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2L7xL1iYLx-",
        "outputId": "de940c76-ee3f-4efa-b12b-0ef604bc2563"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainA\ttrainB\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/ML4B_cycleGAN/data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "curltOIIZ23C",
        "outputId": "43cd6acc-78b8-4fa8-a64a-2031ea9a1f28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'pytorch-CycleGAN-and-pix2pix'\n",
            "/content/pytorch-CycleGAN-and-pix2pix\n"
          ]
        }
      ],
      "source": [
        "%cd pytorch-CycleGAN-and-pix2pix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG0uXzvyad0O",
        "outputId": "da109694-4758-42fe-c389-56491c0db8fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resuming training from epoch 61\n",
            "----------------- Options ---------------\n",
            "               batch_size: 1                             \n",
            "                    beta1: 0.5                           \n",
            "          checkpoints_dir: /content/drive/MyDrive/ML4B_cycleGAN/checkpoints\t[default: ./checkpoints]\n",
            "           continue_train: True                          \t[default: False]\n",
            "                crop_size: 256                           \n",
            "                 dataroot: /content/drive/MyDrive/ML4B_cycleGAN/data\t[default: None]\n",
            "             dataset_mode: unaligned                     \n",
            "                direction: AtoB                          \n",
            "              display_env: main                          \n",
            "             display_freq: 400                           \n",
            "               display_id: 1                             \n",
            "            display_ncols: 4                             \n",
            "             display_port: 8097                          \n",
            "           display_server: http://localhost              \n",
            "          display_winsize: 256                           \n",
            "                    epoch: latest                        \n",
            "              epoch_count: 61                            \t[default: 1]\n",
            "                 gan_mode: lsgan                         \n",
            "                  gpu_ids: 0                             \n",
            "                init_gain: 0.02                          \n",
            "                init_type: normal                        \n",
            "                 input_nc: 3                             \n",
            "                  isTrain: True                          \t[default: None]\n",
            "                 lambda_A: 10.0                          \n",
            "                 lambda_B: 10.0                          \n",
            "          lambda_identity: 0.5                           \n",
            "                load_iter: 0                             \t[default: 0]\n",
            "                load_size: 286                           \n",
            "                       lr: 0.0002                        \n",
            "           lr_decay_iters: 50                            \n",
            "                lr_policy: linear                        \n",
            "         max_dataset_size: inf                           \n",
            "                    model: cycle_gan                     \n",
            "                 n_epochs: 100                           \n",
            "           n_epochs_decay: 100                           \n",
            "               n_layers_D: 3                             \n",
            "                     name: baroque2realism               \t[default: experiment_name]\n",
            "                      ndf: 64                            \n",
            "                     netD: basic                         \n",
            "                     netG: resnet_9blocks                \n",
            "                      ngf: 64                            \n",
            "               no_dropout: True                          \n",
            "                  no_flip: False                         \n",
            "                  no_html: False                         \n",
            "                     norm: instance                      \n",
            "              num_threads: 4                             \n",
            "                output_nc: 3                             \n",
            "                    phase: train                         \n",
            "                pool_size: 50                            \n",
            "               preprocess: resize_and_crop               \n",
            "               print_freq: 100                           \n",
            "             save_by_iter: False                         \n",
            "          save_epoch_freq: 5                             \n",
            "         save_latest_freq: 5000                          \n",
            "           serial_batches: False                         \n",
            "                   suffix:                               \n",
            "         update_html_freq: 1000                          \n",
            "                use_wandb: False                         \n",
            "                  verbose: False                         \n",
            "       wandb_project_name: CycleGAN-and-pix2pix          \n",
            "----------------- End -------------------\n",
            "dataset [UnalignedDataset] was created\n",
            "The number of training images = 500\n",
            "initialize network with normal\n",
            "initialize network with normal\n",
            "initialize network with normal\n",
            "initialize network with normal\n",
            "model [CycleGANModel] was created\n",
            "loading the model from /content/drive/MyDrive/ML4B_cycleGAN/checkpoints/baroque2realism/latest_net_G_A.pth\n",
            "loading the model from /content/drive/MyDrive/ML4B_cycleGAN/checkpoints/baroque2realism/latest_net_G_B.pth\n",
            "loading the model from /content/drive/MyDrive/ML4B_cycleGAN/checkpoints/baroque2realism/latest_net_D_A.pth\n",
            "loading the model from /content/drive/MyDrive/ML4B_cycleGAN/checkpoints/baroque2realism/latest_net_D_B.pth\n",
            "---------- Networks initialized -------------\n",
            "[Network G_A] Total number of parameters : 11.378 M\n",
            "[Network G_B] Total number of parameters : 11.378 M\n",
            "[Network D_A] Total number of parameters : 2.765 M\n",
            "[Network D_B] Total number of parameters : 2.765 M\n",
            "-----------------------------------------------\n",
            "Setting up a new session...\n",
            "Exception in user code:\n",
            "------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\", line 198, in _new_conn\n",
            "    sock = connection.create_connection(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
            "    raise err\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/util/connection.py\", line 73, in create_connection\n",
            "    sock.connect(sa)\n",
            "ConnectionRefusedError: [Errno 111] Connection refused\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\", line 787, in urlopen\n",
            "    response = self._make_request(\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\", line 493, in _make_request\n",
            "    conn.request(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\", line 445, in request\n",
            "    self.endheaders()\n",
            "  File \"/usr/lib/python3.11/http/client.py\", line 1298, in endheaders\n",
            "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
            "  File \"/usr/lib/python3.11/http/client.py\", line 1058, in _send_output\n",
            "    self.send(msg)\n",
            "  File \"/usr/lib/python3.11/http/client.py\", line 996, in send\n",
            "    self.connect()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\", line 276, in connect\n",
            "    self.sock = self._new_conn()\n",
            "                ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\", line 213, in _new_conn\n",
            "    raise NewConnectionError(\n",
            "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7b730e08bd50>: Failed to establish a new connection: [Errno 111] Connection refused\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/adapters.py\", line 667, in send\n",
            "    resp = conn.urlopen(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\", line 841, in urlopen\n",
            "    retries = retries.increment(\n",
            "              ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/urllib3/util/retry.py\", line 519, in increment\n",
            "    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=8097): Max retries exceeded with url: /env/main (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b730e08bd50>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/visdom/__init__.py\", line 756, in _send\n",
            "    return self._handle_post(\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/visdom/__init__.py\", line 720, in _handle_post\n",
            "    r = self.session.post(url, data=data)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 637, in post\n",
            "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/adapters.py\", line 700, in send\n",
            "    raise ConnectionError(e, request=request)\n",
            "requests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8097): Max retries exceeded with url: /env/main (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7b730e08bd50>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
            "[Errno 99] Cannot assign requested address\n",
            "Visdom.setup_socket.<locals>.on_close() takes 1 positional argument but 3 were given\n",
            "[Errno 99] Cannot assign requested address\n",
            "Visdom.setup_socket.<locals>.on_close() takes 1 positional argument but 3 were given\n",
            "[Errno 99] Cannot assign requested address\n",
            "Visdom.setup_socket.<locals>.on_close() takes 1 positional argument but 3 were given\n",
            "Visdom python client failed to establish socket to get messages from the server. This feature is optional and can be disabled by initializing Visdom with `use_incoming_socket=False`, which will prevent waiting for this request to timeout.\n",
            "\n",
            "\n",
            "Could not connect to Visdom server. \n",
            " Trying to start a server....\n",
            "Command: /usr/bin/python3 -m visdom.server -p 8097 &>/dev/null &\n",
            "create web directory /content/drive/MyDrive/ML4B_cycleGAN/checkpoints/baroque2realism/web...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "[Errno 99] Cannot assign requested address\n",
            "Visdom.setup_socket.<locals>.on_close() takes 1 positional argument but 3 were given\n",
            "(epoch: 61, iters: 100, time: 0.606, data: 1.799) D_A: 0.167 G_A: 0.603 cycle_A: 0.553 idt_A: 0.236 D_B: 0.066 G_B: 0.476 cycle_B: 0.598 idt_B: 0.333 \n",
            "(epoch: 61, iters: 200, time: 0.645, data: 0.000) D_A: 0.376 G_A: 0.216 cycle_A: 0.736 idt_A: 0.569 D_B: 0.182 G_B: 0.337 cycle_B: 1.428 idt_B: 0.398 \n",
            "(epoch: 61, iters: 300, time: 0.634, data: 0.002) D_A: 0.083 G_A: 0.696 cycle_A: 0.597 idt_A: 0.426 D_B: 0.068 G_B: 0.491 cycle_B: 0.919 idt_B: 0.250 \n",
            "(epoch: 61, iters: 400, time: 5.225, data: 0.002) D_A: 0.288 G_A: 0.208 cycle_A: 0.752 idt_A: 0.222 D_B: 0.206 G_B: 0.207 cycle_B: 0.528 idt_B: 0.370 \n",
            "(epoch: 61, iters: 500, time: 0.635, data: 0.001) D_A: 0.316 G_A: 0.266 cycle_A: 0.584 idt_A: 0.365 D_B: 0.056 G_B: 0.797 cycle_B: 0.877 idt_B: 0.234 \n",
            "End of epoch 61 / 200 \t Time Taken: 262 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 62, iters: 100, time: 0.635, data: 0.085) D_A: 0.282 G_A: 1.180 cycle_A: 0.970 idt_A: 0.250 D_B: 0.351 G_B: 0.891 cycle_B: 0.544 idt_B: 0.611 \n",
            "(epoch: 62, iters: 200, time: 0.637, data: 0.001) D_A: 0.120 G_A: 0.259 cycle_A: 0.487 idt_A: 0.205 D_B: 0.162 G_B: 0.471 cycle_B: 0.596 idt_B: 0.231 \n",
            "(epoch: 62, iters: 300, time: 1.390, data: 0.003) D_A: 0.183 G_A: 0.334 cycle_A: 0.934 idt_A: 0.394 D_B: 0.158 G_B: 0.355 cycle_B: 0.837 idt_B: 0.372 \n",
            "(epoch: 62, iters: 400, time: 0.632, data: 0.002) D_A: 0.165 G_A: 0.811 cycle_A: 0.624 idt_A: 0.433 D_B: 0.098 G_B: 0.158 cycle_B: 1.133 idt_B: 0.349 \n",
            "(epoch: 62, iters: 500, time: 0.636, data: 0.001) D_A: 0.351 G_A: 0.278 cycle_A: 0.466 idt_A: 0.328 D_B: 0.345 G_B: 0.076 cycle_B: 0.658 idt_B: 0.247 \n",
            "End of epoch 62 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 63, iters: 100, time: 0.634, data: 0.080) D_A: 0.134 G_A: 0.290 cycle_A: 0.633 idt_A: 0.348 D_B: 0.063 G_B: 0.597 cycle_B: 0.774 idt_B: 0.219 \n",
            "(epoch: 63, iters: 200, time: 1.661, data: 0.002) D_A: 0.212 G_A: 0.435 cycle_A: 0.771 idt_A: 0.471 D_B: 0.115 G_B: 0.417 cycle_B: 0.613 idt_B: 0.250 \n",
            "(epoch: 63, iters: 300, time: 0.634, data: 0.001) D_A: 0.316 G_A: 0.071 cycle_A: 1.240 idt_A: 0.351 D_B: 0.113 G_B: 0.494 cycle_B: 0.701 idt_B: 0.440 \n",
            "(epoch: 63, iters: 400, time: 0.635, data: 0.001) D_A: 0.116 G_A: 0.333 cycle_A: 0.761 idt_A: 0.315 D_B: 0.126 G_B: 0.215 cycle_B: 0.597 idt_B: 0.344 \n",
            "(epoch: 63, iters: 500, time: 0.637, data: 0.001) D_A: 0.166 G_A: 0.567 cycle_A: 0.561 idt_A: 0.266 D_B: 0.140 G_B: 0.325 cycle_B: 0.594 idt_B: 0.245 \n",
            "End of epoch 63 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 64, iters: 100, time: 1.468, data: 0.091) D_A: 0.093 G_A: 0.562 cycle_A: 0.378 idt_A: 0.307 D_B: 0.039 G_B: 0.759 cycle_B: 0.864 idt_B: 0.234 \n",
            "(epoch: 64, iters: 200, time: 0.636, data: 0.002) D_A: 0.394 G_A: 0.466 cycle_A: 0.587 idt_A: 0.264 D_B: 0.105 G_B: 0.558 cycle_B: 0.521 idt_B: 0.294 \n",
            "(epoch: 64, iters: 300, time: 0.636, data: 0.002) D_A: 0.093 G_A: 0.379 cycle_A: 0.515 idt_A: 0.394 D_B: 0.072 G_B: 0.663 cycle_B: 0.731 idt_B: 0.224 \n",
            "(epoch: 64, iters: 400, time: 0.632, data: 0.002) D_A: 0.227 G_A: 0.275 cycle_A: 1.118 idt_A: 0.428 D_B: 0.255 G_B: 0.152 cycle_B: 0.677 idt_B: 0.456 \n",
            "(epoch: 64, iters: 500, time: 1.492, data: 0.002) D_A: 0.057 G_A: 0.641 cycle_A: 0.800 idt_A: 0.481 D_B: 0.069 G_B: 0.819 cycle_B: 1.001 idt_B: 0.295 \n",
            "End of epoch 64 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 65, iters: 100, time: 0.632, data: 0.092) D_A: 0.088 G_A: 0.226 cycle_A: 1.192 idt_A: 0.326 D_B: 0.374 G_B: 0.497 cycle_B: 1.283 idt_B: 0.679 \n",
            "(epoch: 65, iters: 200, time: 0.638, data: 0.002) D_A: 0.166 G_A: 0.365 cycle_A: 0.800 idt_A: 0.306 D_B: 0.038 G_B: 0.418 cycle_B: 0.825 idt_B: 0.373 \n",
            "(epoch: 65, iters: 300, time: 0.635, data: 0.002) D_A: 0.128 G_A: 0.319 cycle_A: 0.514 idt_A: 0.401 D_B: 0.156 G_B: 0.194 cycle_B: 1.132 idt_B: 0.241 \n",
            "(epoch: 65, iters: 400, time: 1.467, data: 0.002) D_A: 0.257 G_A: 0.068 cycle_A: 1.009 idt_A: 0.406 D_B: 0.299 G_B: 0.288 cycle_B: 0.734 idt_B: 0.463 \n",
            "(epoch: 65, iters: 500, time: 0.634, data: 0.002) D_A: 0.244 G_A: 0.466 cycle_A: 0.493 idt_A: 0.239 D_B: 0.258 G_B: 0.140 cycle_B: 0.423 idt_B: 0.261 \n",
            "saving the model at the end of epoch 65, iters 2500\n",
            "End of epoch 65 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 66, iters: 100, time: 0.638, data: 0.108) D_A: 0.173 G_A: 0.413 cycle_A: 0.997 idt_A: 0.358 D_B: 0.098 G_B: 0.490 cycle_B: 0.749 idt_B: 0.365 \n",
            "(epoch: 66, iters: 200, time: 0.635, data: 0.001) D_A: 0.332 G_A: 0.358 cycle_A: 0.659 idt_A: 0.299 D_B: 0.205 G_B: 0.236 cycle_B: 0.593 idt_B: 0.237 \n",
            "(epoch: 66, iters: 300, time: 1.416, data: 0.001) D_A: 0.211 G_A: 0.128 cycle_A: 0.687 idt_A: 0.290 D_B: 0.633 G_B: 0.137 cycle_B: 0.556 idt_B: 0.254 \n",
            "(epoch: 66, iters: 400, time: 0.634, data: 0.001) D_A: 0.188 G_A: 0.505 cycle_A: 1.150 idt_A: 0.381 D_B: 0.260 G_B: 0.761 cycle_B: 0.755 idt_B: 0.699 \n",
            "(epoch: 66, iters: 500, time: 0.633, data: 0.001) D_A: 0.167 G_A: 0.243 cycle_A: 0.682 idt_A: 0.667 D_B: 0.129 G_B: 0.575 cycle_B: 0.901 idt_B: 0.311 \n",
            "End of epoch 66 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 67, iters: 100, time: 0.632, data: 0.113) D_A: 0.241 G_A: 0.435 cycle_A: 0.861 idt_A: 0.193 D_B: 0.227 G_B: 0.316 cycle_B: 0.476 idt_B: 0.337 \n",
            "(epoch: 67, iters: 200, time: 1.459, data: 0.001) D_A: 0.261 G_A: 0.255 cycle_A: 0.921 idt_A: 0.456 D_B: 0.091 G_B: 0.139 cycle_B: 0.955 idt_B: 0.435 \n",
            "(epoch: 67, iters: 300, time: 0.637, data: 0.001) D_A: 0.145 G_A: 0.348 cycle_A: 0.523 idt_A: 0.557 D_B: 0.099 G_B: 0.212 cycle_B: 1.105 idt_B: 0.251 \n",
            "(epoch: 67, iters: 400, time: 0.637, data: 0.001) D_A: 0.270 G_A: 0.247 cycle_A: 0.821 idt_A: 0.400 D_B: 0.175 G_B: 0.448 cycle_B: 0.697 idt_B: 0.445 \n",
            "(epoch: 67, iters: 500, time: 0.634, data: 0.001) D_A: 0.230 G_A: 0.159 cycle_A: 1.308 idt_A: 0.268 D_B: 0.379 G_B: 0.644 cycle_B: 0.634 idt_B: 0.515 \n",
            "End of epoch 67 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 68, iters: 100, time: 1.371, data: 0.096) D_A: 0.241 G_A: 0.430 cycle_A: 0.635 idt_A: 0.355 D_B: 0.279 G_B: 0.233 cycle_B: 1.006 idt_B: 0.343 \n",
            "(epoch: 68, iters: 200, time: 0.638, data: 0.002) D_A: 0.330 G_A: 0.383 cycle_A: 1.041 idt_A: 0.314 D_B: 0.230 G_B: 0.116 cycle_B: 0.551 idt_B: 0.570 \n",
            "(epoch: 68, iters: 300, time: 0.638, data: 0.001) D_A: 0.170 G_A: 0.311 cycle_A: 0.500 idt_A: 0.310 D_B: 0.202 G_B: 0.427 cycle_B: 0.641 idt_B: 0.198 \n",
            "(epoch: 68, iters: 400, time: 0.632, data: 0.002) D_A: 0.458 G_A: 0.135 cycle_A: 0.720 idt_A: 0.390 D_B: 0.514 G_B: 0.079 cycle_B: 0.979 idt_B: 0.511 \n",
            "(epoch: 68, iters: 500, time: 1.630, data: 0.001) D_A: 0.228 G_A: 0.269 cycle_A: 0.966 idt_A: 0.363 D_B: 0.153 G_B: 0.334 cycle_B: 0.675 idt_B: 0.336 \n",
            "End of epoch 68 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 69, iters: 100, time: 0.637, data: 0.099) D_A: 0.135 G_A: 0.269 cycle_A: 0.548 idt_A: 0.279 D_B: 0.258 G_B: 0.483 cycle_B: 0.651 idt_B: 0.248 \n",
            "(epoch: 69, iters: 200, time: 0.637, data: 0.002) D_A: 0.267 G_A: 0.495 cycle_A: 0.569 idt_A: 0.492 D_B: 0.126 G_B: 0.805 cycle_B: 0.987 idt_B: 0.318 \n",
            "(epoch: 69, iters: 300, time: 0.634, data: 0.001) D_A: 0.238 G_A: 0.307 cycle_A: 0.602 idt_A: 0.223 D_B: 0.173 G_B: 0.245 cycle_B: 0.584 idt_B: 0.259 \n",
            "(epoch: 69, iters: 400, time: 1.506, data: 0.001) D_A: 0.077 G_A: 0.453 cycle_A: 0.526 idt_A: 0.283 D_B: 0.137 G_B: 0.900 cycle_B: 0.548 idt_B: 0.236 \n",
            "(epoch: 69, iters: 500, time: 0.634, data: 0.002) D_A: 0.230 G_A: 0.313 cycle_A: 1.748 idt_A: 0.446 D_B: 0.229 G_B: 0.327 cycle_B: 1.122 idt_B: 0.874 \n",
            "End of epoch 69 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 70, iters: 100, time: 0.638, data: 0.094) D_A: 0.291 G_A: 0.074 cycle_A: 0.805 idt_A: 0.299 D_B: 0.076 G_B: 0.063 cycle_B: 0.645 idt_B: 0.442 \n",
            "(epoch: 70, iters: 200, time: 0.638, data: 0.001) D_A: 0.289 G_A: 0.293 cycle_A: 1.483 idt_A: 0.327 D_B: 0.345 G_B: 0.158 cycle_B: 0.665 idt_B: 0.748 \n",
            "(epoch: 70, iters: 300, time: 1.450, data: 0.001) D_A: 0.165 G_A: 0.216 cycle_A: 0.958 idt_A: 0.303 D_B: 0.229 G_B: 0.328 cycle_B: 0.761 idt_B: 0.495 \n",
            "(epoch: 70, iters: 400, time: 0.635, data: 0.002) D_A: 0.409 G_A: 0.267 cycle_A: 0.553 idt_A: 0.248 D_B: 0.225 G_B: 0.387 cycle_B: 0.774 idt_B: 0.326 \n",
            "(epoch: 70, iters: 500, time: 0.634, data: 0.001) D_A: 0.148 G_A: 0.391 cycle_A: 0.472 idt_A: 0.262 D_B: 0.222 G_B: 0.618 cycle_B: 0.630 idt_B: 0.213 \n",
            "saving the latest model (epoch 70, total_iters 5000)\n",
            "saving the model at the end of epoch 70, iters 5000\n",
            "End of epoch 70 / 200 \t Time Taken: 257 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 71, iters: 100, time: 0.637, data: 0.109) D_A: 0.200 G_A: 0.851 cycle_A: 0.707 idt_A: 0.230 D_B: 0.185 G_B: 0.432 cycle_B: 0.757 idt_B: 0.349 \n",
            "(epoch: 71, iters: 200, time: 1.469, data: 0.001) D_A: 0.131 G_A: 1.392 cycle_A: 0.942 idt_A: 0.381 D_B: 0.512 G_B: 0.955 cycle_B: 0.877 idt_B: 0.441 \n",
            "(epoch: 71, iters: 300, time: 0.636, data: 0.001) D_A: 0.276 G_A: 0.585 cycle_A: 0.894 idt_A: 0.292 D_B: 0.210 G_B: 0.153 cycle_B: 0.685 idt_B: 0.298 \n",
            "(epoch: 71, iters: 400, time: 0.637, data: 0.002) D_A: 0.132 G_A: 0.339 cycle_A: 0.743 idt_A: 0.409 D_B: 0.187 G_B: 0.458 cycle_B: 0.874 idt_B: 0.287 \n",
            "(epoch: 71, iters: 500, time: 0.631, data: 0.002) D_A: 0.385 G_A: 0.459 cycle_A: 0.518 idt_A: 0.392 D_B: 0.201 G_B: 0.268 cycle_B: 0.707 idt_B: 0.244 \n",
            "End of epoch 71 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 72, iters: 100, time: 1.532, data: 0.102) D_A: 0.187 G_A: 0.398 cycle_A: 0.564 idt_A: 0.399 D_B: 0.208 G_B: 0.425 cycle_B: 0.817 idt_B: 0.418 \n",
            "(epoch: 72, iters: 200, time: 0.636, data: 0.001) D_A: 0.210 G_A: 1.311 cycle_A: 1.860 idt_A: 0.351 D_B: 0.145 G_B: 0.290 cycle_B: 0.890 idt_B: 1.129 \n",
            "(epoch: 72, iters: 300, time: 0.633, data: 0.002) D_A: 0.110 G_A: 0.598 cycle_A: 1.274 idt_A: 0.402 D_B: 0.082 G_B: 0.515 cycle_B: 0.889 idt_B: 0.604 \n",
            "(epoch: 72, iters: 400, time: 0.633, data: 0.001) D_A: 0.229 G_A: 0.410 cycle_A: 0.614 idt_A: 0.328 D_B: 0.185 G_B: 0.266 cycle_B: 0.912 idt_B: 0.283 \n",
            "(epoch: 72, iters: 500, time: 1.375, data: 0.001) D_A: 0.200 G_A: 0.383 cycle_A: 0.821 idt_A: 0.188 D_B: 0.239 G_B: 0.357 cycle_B: 0.493 idt_B: 0.350 \n",
            "End of epoch 72 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 73, iters: 100, time: 0.636, data: 0.103) D_A: 0.246 G_A: 0.499 cycle_A: 0.870 idt_A: 0.244 D_B: 0.380 G_B: 0.076 cycle_B: 0.446 idt_B: 0.415 \n",
            "(epoch: 73, iters: 200, time: 0.636, data: 0.001) D_A: 0.257 G_A: 0.541 cycle_A: 0.562 idt_A: 0.314 D_B: 0.151 G_B: 0.194 cycle_B: 0.505 idt_B: 0.261 \n",
            "(epoch: 73, iters: 300, time: 0.636, data: 0.001) D_A: 0.199 G_A: 0.595 cycle_A: 0.801 idt_A: 0.320 D_B: 0.191 G_B: 0.236 cycle_B: 0.738 idt_B: 0.365 \n",
            "(epoch: 73, iters: 400, time: 1.712, data: 0.001) D_A: 0.121 G_A: 0.404 cycle_A: 0.751 idt_A: 0.354 D_B: 0.250 G_B: 0.715 cycle_B: 0.588 idt_B: 0.303 \n",
            "(epoch: 73, iters: 500, time: 0.635, data: 0.002) D_A: 0.093 G_A: 0.536 cycle_A: 1.066 idt_A: 0.322 D_B: 0.063 G_B: 0.519 cycle_B: 0.792 idt_B: 0.540 \n",
            "End of epoch 73 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 74, iters: 100, time: 0.635, data: 0.121) D_A: 0.216 G_A: 0.204 cycle_A: 0.866 idt_A: 0.295 D_B: 0.062 G_B: 0.586 cycle_B: 0.656 idt_B: 0.447 \n",
            "(epoch: 74, iters: 200, time: 0.635, data: 0.002) D_A: 0.285 G_A: 0.718 cycle_A: 0.498 idt_A: 0.281 D_B: 0.190 G_B: 0.228 cycle_B: 0.570 idt_B: 0.170 \n",
            "(epoch: 74, iters: 300, time: 1.469, data: 0.002) D_A: 0.092 G_A: 0.494 cycle_A: 0.594 idt_A: 0.356 D_B: 0.121 G_B: 0.276 cycle_B: 0.587 idt_B: 0.224 \n",
            "(epoch: 74, iters: 400, time: 0.637, data: 0.002) D_A: 0.374 G_A: 0.267 cycle_A: 0.808 idt_A: 0.413 D_B: 0.206 G_B: 0.428 cycle_B: 0.782 idt_B: 0.311 \n",
            "(epoch: 74, iters: 500, time: 0.637, data: 0.002) D_A: 0.155 G_A: 0.079 cycle_A: 1.139 idt_A: 0.419 D_B: 0.213 G_B: 0.143 cycle_B: 0.862 idt_B: 0.320 \n",
            "End of epoch 74 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 75, iters: 100, time: 0.633, data: 0.124) D_A: 0.187 G_A: 0.488 cycle_A: 0.960 idt_A: 0.283 D_B: 0.127 G_B: 0.098 cycle_B: 0.764 idt_B: 0.451 \n",
            "(epoch: 75, iters: 200, time: 1.585, data: 0.002) D_A: 0.261 G_A: 0.310 cycle_A: 0.545 idt_A: 0.519 D_B: 0.339 G_B: 0.125 cycle_B: 0.865 idt_B: 0.378 \n",
            "(epoch: 75, iters: 300, time: 0.638, data: 0.002) D_A: 0.155 G_A: 0.477 cycle_A: 0.667 idt_A: 0.419 D_B: 0.225 G_B: 0.211 cycle_B: 0.903 idt_B: 0.296 \n",
            "(epoch: 75, iters: 400, time: 0.637, data: 0.002) D_A: 0.158 G_A: 0.707 cycle_A: 0.805 idt_A: 0.284 D_B: 0.119 G_B: 0.716 cycle_B: 0.587 idt_B: 0.346 \n",
            "(epoch: 75, iters: 500, time: 0.636, data: 0.001) D_A: 0.093 G_A: 0.375 cycle_A: 0.889 idt_A: 0.219 D_B: 0.214 G_B: 0.574 cycle_B: 0.691 idt_B: 0.372 \n",
            "saving the model at the end of epoch 75, iters 7500\n",
            "End of epoch 75 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 76, iters: 100, time: 1.565, data: 0.112) D_A: 0.144 G_A: 0.377 cycle_A: 0.731 idt_A: 0.327 D_B: 0.042 G_B: 0.716 cycle_B: 0.681 idt_B: 0.233 \n",
            "(epoch: 76, iters: 200, time: 0.636, data: 0.001) D_A: 0.274 G_A: 0.259 cycle_A: 0.692 idt_A: 0.425 D_B: 0.326 G_B: 0.159 cycle_B: 0.882 idt_B: 0.338 \n",
            "(epoch: 76, iters: 300, time: 0.637, data: 0.001) D_A: 0.187 G_A: 0.336 cycle_A: 2.067 idt_A: 0.204 D_B: 0.195 G_B: 0.257 cycle_B: 0.524 idt_B: 1.154 \n",
            "(epoch: 76, iters: 400, time: 0.634, data: 0.002) D_A: 0.122 G_A: 0.205 cycle_A: 0.726 idt_A: 0.275 D_B: 0.199 G_B: 0.382 cycle_B: 0.648 idt_B: 0.824 \n",
            "(epoch: 76, iters: 500, time: 1.526, data: 0.002) D_A: 0.178 G_A: 0.766 cycle_A: 0.836 idt_A: 0.195 D_B: 0.054 G_B: 0.612 cycle_B: 0.616 idt_B: 0.331 \n",
            "End of epoch 76 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 77, iters: 100, time: 0.635, data: 0.112) D_A: 0.384 G_A: 0.330 cycle_A: 0.974 idt_A: 0.407 D_B: 0.147 G_B: 0.282 cycle_B: 0.927 idt_B: 0.339 \n",
            "(epoch: 77, iters: 200, time: 0.635, data: 0.001) D_A: 0.079 G_A: 0.641 cycle_A: 0.933 idt_A: 0.204 D_B: 0.145 G_B: 0.322 cycle_B: 0.418 idt_B: 0.424 \n",
            "(epoch: 77, iters: 300, time: 0.637, data: 0.001) D_A: 0.234 G_A: 0.814 cycle_A: 0.418 idt_A: 0.204 D_B: 0.157 G_B: 0.249 cycle_B: 0.443 idt_B: 0.186 \n",
            "(epoch: 77, iters: 400, time: 1.533, data: 0.001) D_A: 0.362 G_A: 0.265 cycle_A: 0.607 idt_A: 0.339 D_B: 0.200 G_B: 0.318 cycle_B: 0.764 idt_B: 0.287 \n",
            "(epoch: 77, iters: 500, time: 0.633, data: 0.001) D_A: 0.163 G_A: 0.503 cycle_A: 1.381 idt_A: 0.395 D_B: 0.253 G_B: 0.288 cycle_B: 0.783 idt_B: 0.271 \n",
            "End of epoch 77 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 78, iters: 100, time: 0.638, data: 0.108) D_A: 0.212 G_A: 0.517 cycle_A: 1.029 idt_A: 0.298 D_B: 0.252 G_B: 0.385 cycle_B: 0.484 idt_B: 0.534 \n",
            "(epoch: 78, iters: 200, time: 0.635, data: 0.001) D_A: 0.052 G_A: 0.292 cycle_A: 1.053 idt_A: 0.523 D_B: 0.490 G_B: 0.637 cycle_B: 1.239 idt_B: 0.508 \n",
            "(epoch: 78, iters: 300, time: 1.827, data: 0.001) D_A: 0.206 G_A: 0.542 cycle_A: 0.517 idt_A: 0.215 D_B: 0.035 G_B: 0.687 cycle_B: 0.611 idt_B: 0.241 \n",
            "(epoch: 78, iters: 400, time: 0.638, data: 0.002) D_A: 0.150 G_A: 0.469 cycle_A: 0.507 idt_A: 0.244 D_B: 0.273 G_B: 0.417 cycle_B: 0.458 idt_B: 0.212 \n",
            "(epoch: 78, iters: 500, time: 0.634, data: 0.002) D_A: 0.125 G_A: 0.218 cycle_A: 1.414 idt_A: 0.417 D_B: 0.372 G_B: 0.594 cycle_B: 0.856 idt_B: 0.607 \n",
            "End of epoch 78 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 79, iters: 100, time: 0.639, data: 0.103) D_A: 0.206 G_A: 0.293 cycle_A: 0.775 idt_A: 0.251 D_B: 0.328 G_B: 0.272 cycle_B: 0.410 idt_B: 0.480 \n",
            "(epoch: 79, iters: 200, time: 1.622, data: 0.001) D_A: 0.255 G_A: 0.426 cycle_A: 1.133 idt_A: 0.305 D_B: 0.113 G_B: 0.053 cycle_B: 0.691 idt_B: 0.594 \n",
            "(epoch: 79, iters: 300, time: 0.635, data: 0.001) D_A: 0.177 G_A: 0.261 cycle_A: 0.524 idt_A: 0.318 D_B: 0.294 G_B: 0.261 cycle_B: 0.593 idt_B: 0.245 \n",
            "(epoch: 79, iters: 400, time: 0.636, data: 0.001) D_A: 0.114 G_A: 0.827 cycle_A: 0.856 idt_A: 0.315 D_B: 0.205 G_B: 0.171 cycle_B: 0.620 idt_B: 0.511 \n",
            "(epoch: 79, iters: 500, time: 0.638, data: 0.001) D_A: 0.113 G_A: 0.715 cycle_A: 0.649 idt_A: 0.216 D_B: 0.204 G_B: 0.238 cycle_B: 0.873 idt_B: 0.376 \n",
            "End of epoch 79 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 80, iters: 100, time: 1.542, data: 0.112) D_A: 0.217 G_A: 0.204 cycle_A: 0.615 idt_A: 0.368 D_B: 0.233 G_B: 0.261 cycle_B: 0.662 idt_B: 0.536 \n",
            "(epoch: 80, iters: 200, time: 0.636, data: 0.001) D_A: 0.314 G_A: 0.505 cycle_A: 0.429 idt_A: 0.315 D_B: 0.277 G_B: 0.228 cycle_B: 0.736 idt_B: 0.160 \n",
            "(epoch: 80, iters: 300, time: 0.637, data: 0.002) D_A: 0.260 G_A: 0.459 cycle_A: 0.580 idt_A: 0.564 D_B: 0.239 G_B: 0.260 cycle_B: 1.254 idt_B: 0.273 \n",
            "(epoch: 80, iters: 400, time: 0.634, data: 0.002) D_A: 0.047 G_A: 0.697 cycle_A: 0.853 idt_A: 0.209 D_B: 0.121 G_B: 0.504 cycle_B: 0.416 idt_B: 0.513 \n",
            "(epoch: 80, iters: 500, time: 1.546, data: 0.001) D_A: 0.328 G_A: 0.194 cycle_A: 0.518 idt_A: 0.277 D_B: 0.269 G_B: 0.648 cycle_B: 0.551 idt_B: 0.258 \n",
            "saving the latest model (epoch 80, total_iters 10000)\n",
            "saving the model at the end of epoch 80, iters 10000\n",
            "End of epoch 80 / 200 \t Time Taken: 258 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 81, iters: 100, time: 0.640, data: 0.110) D_A: 0.258 G_A: 0.294 cycle_A: 1.080 idt_A: 0.396 D_B: 0.250 G_B: 0.470 cycle_B: 0.696 idt_B: 0.307 \n",
            "(epoch: 81, iters: 200, time: 0.636, data: 0.001) D_A: 0.156 G_A: 0.400 cycle_A: 0.558 idt_A: 0.369 D_B: 0.175 G_B: 0.399 cycle_B: 0.782 idt_B: 0.255 \n",
            "(epoch: 81, iters: 300, time: 0.633, data: 0.001) D_A: 0.140 G_A: 0.110 cycle_A: 1.099 idt_A: 0.236 D_B: 0.277 G_B: 0.564 cycle_B: 0.461 idt_B: 0.492 \n",
            "(epoch: 81, iters: 400, time: 1.528, data: 0.001) D_A: 0.374 G_A: 0.746 cycle_A: 0.903 idt_A: 0.314 D_B: 0.211 G_B: 0.429 cycle_B: 0.604 idt_B: 0.460 \n",
            "(epoch: 81, iters: 500, time: 0.633, data: 0.001) D_A: 0.263 G_A: 0.282 cycle_A: 0.823 idt_A: 0.404 D_B: 0.319 G_B: 0.072 cycle_B: 0.548 idt_B: 0.420 \n",
            "End of epoch 81 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 82, iters: 100, time: 0.638, data: 0.107) D_A: 0.153 G_A: 0.072 cycle_A: 0.775 idt_A: 0.258 D_B: 0.087 G_B: 0.564 cycle_B: 0.499 idt_B: 0.505 \n",
            "(epoch: 82, iters: 200, time: 0.633, data: 0.001) D_A: 0.107 G_A: 0.358 cycle_A: 0.732 idt_A: 0.229 D_B: 0.361 G_B: 0.245 cycle_B: 0.496 idt_B: 0.455 \n",
            "(epoch: 82, iters: 300, time: 1.461, data: 0.001) D_A: 0.163 G_A: 0.662 cycle_A: 0.880 idt_A: 0.283 D_B: 0.155 G_B: 0.253 cycle_B: 0.651 idt_B: 0.387 \n",
            "(epoch: 82, iters: 400, time: 0.638, data: 0.001) D_A: 0.198 G_A: 0.215 cycle_A: 1.038 idt_A: 0.287 D_B: 0.221 G_B: 0.690 cycle_B: 0.627 idt_B: 0.494 \n",
            "(epoch: 82, iters: 500, time: 0.639, data: 0.001) D_A: 0.331 G_A: 0.195 cycle_A: 0.706 idt_A: 0.290 D_B: 0.381 G_B: 0.409 cycle_B: 0.573 idt_B: 0.329 \n",
            "End of epoch 82 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 83, iters: 100, time: 0.636, data: 0.115) D_A: 0.280 G_A: 0.123 cycle_A: 0.671 idt_A: 0.351 D_B: 0.207 G_B: 0.721 cycle_B: 0.894 idt_B: 0.272 \n",
            "(epoch: 83, iters: 200, time: 1.632, data: 0.001) D_A: 0.269 G_A: 0.358 cycle_A: 0.589 idt_A: 0.420 D_B: 0.278 G_B: 0.132 cycle_B: 0.653 idt_B: 0.278 \n",
            "(epoch: 83, iters: 300, time: 0.633, data: 0.001) D_A: 0.221 G_A: 1.061 cycle_A: 0.902 idt_A: 0.288 D_B: 0.351 G_B: 0.223 cycle_B: 1.051 idt_B: 0.471 \n",
            "(epoch: 83, iters: 400, time: 0.639, data: 0.001) D_A: 0.382 G_A: 0.802 cycle_A: 0.926 idt_A: 0.384 D_B: 0.316 G_B: 0.133 cycle_B: 0.911 idt_B: 0.410 \n",
            "(epoch: 83, iters: 500, time: 0.635, data: 0.001) D_A: 0.499 G_A: 0.372 cycle_A: 1.734 idt_A: 0.291 D_B: 0.212 G_B: 0.182 cycle_B: 0.704 idt_B: 0.844 \n",
            "End of epoch 83 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 84, iters: 100, time: 1.483, data: 0.113) D_A: 0.364 G_A: 0.435 cycle_A: 1.009 idt_A: 0.283 D_B: 0.189 G_B: 0.551 cycle_B: 0.693 idt_B: 0.533 \n",
            "(epoch: 84, iters: 200, time: 0.634, data: 0.001) D_A: 0.209 G_A: 0.551 cycle_A: 0.618 idt_A: 0.343 D_B: 0.474 G_B: 0.039 cycle_B: 0.752 idt_B: 0.236 \n",
            "(epoch: 84, iters: 300, time: 0.634, data: 0.001) D_A: 0.190 G_A: 1.008 cycle_A: 1.052 idt_A: 0.640 D_B: 0.203 G_B: 0.101 cycle_B: 1.153 idt_B: 0.516 \n",
            "(epoch: 84, iters: 400, time: 0.635, data: 0.002) D_A: 0.307 G_A: 0.422 cycle_A: 0.575 idt_A: 0.263 D_B: 0.154 G_B: 0.321 cycle_B: 0.538 idt_B: 0.277 \n",
            "(epoch: 84, iters: 500, time: 1.534, data: 0.001) D_A: 0.070 G_A: 0.472 cycle_A: 0.651 idt_A: 0.219 D_B: 0.358 G_B: 0.850 cycle_B: 0.399 idt_B: 0.296 \n",
            "End of epoch 84 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 85, iters: 100, time: 0.639, data: 0.129) D_A: 0.210 G_A: 0.497 cycle_A: 1.384 idt_A: 0.380 D_B: 0.053 G_B: 0.339 cycle_B: 0.984 idt_B: 0.558 \n",
            "(epoch: 85, iters: 200, time: 0.634, data: 0.001) D_A: 0.175 G_A: 0.390 cycle_A: 0.638 idt_A: 0.379 D_B: 0.258 G_B: 0.431 cycle_B: 0.883 idt_B: 0.373 \n",
            "(epoch: 85, iters: 300, time: 0.636, data: 0.001) D_A: 0.214 G_A: 0.288 cycle_A: 0.555 idt_A: 0.386 D_B: 0.205 G_B: 0.051 cycle_B: 0.683 idt_B: 0.251 \n",
            "(epoch: 85, iters: 400, time: 1.592, data: 0.001) D_A: 0.449 G_A: 0.042 cycle_A: 0.534 idt_A: 0.349 D_B: 0.292 G_B: 0.377 cycle_B: 0.623 idt_B: 0.290 \n",
            "(epoch: 85, iters: 500, time: 0.636, data: 0.001) D_A: 0.382 G_A: 0.431 cycle_A: 0.678 idt_A: 0.497 D_B: 0.174 G_B: 0.291 cycle_B: 0.898 idt_B: 0.285 \n",
            "saving the model at the end of epoch 85, iters 12500\n",
            "End of epoch 85 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 86, iters: 100, time: 0.634, data: 0.118) D_A: 0.210 G_A: 0.218 cycle_A: 0.616 idt_A: 0.183 D_B: 0.224 G_B: 0.174 cycle_B: 0.456 idt_B: 0.254 \n",
            "(epoch: 86, iters: 200, time: 0.638, data: 0.001) D_A: 0.140 G_A: 0.553 cycle_A: 0.889 idt_A: 0.244 D_B: 0.105 G_B: 0.583 cycle_B: 0.413 idt_B: 0.474 \n",
            "(epoch: 86, iters: 300, time: 1.604, data: 0.002) D_A: 0.318 G_A: 0.999 cycle_A: 0.355 idt_A: 0.226 D_B: 0.058 G_B: 0.254 cycle_B: 0.462 idt_B: 0.219 \n",
            "(epoch: 86, iters: 400, time: 0.637, data: 0.001) D_A: 0.136 G_A: 0.333 cycle_A: 0.420 idt_A: 0.239 D_B: 0.243 G_B: 0.831 cycle_B: 0.980 idt_B: 0.248 \n",
            "(epoch: 86, iters: 500, time: 0.634, data: 0.001) D_A: 0.204 G_A: 0.334 cycle_A: 0.569 idt_A: 0.260 D_B: 0.182 G_B: 0.450 cycle_B: 0.501 idt_B: 0.214 \n",
            "End of epoch 86 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 87, iters: 100, time: 0.635, data: 0.112) D_A: 0.305 G_A: 0.451 cycle_A: 1.015 idt_A: 0.273 D_B: 0.103 G_B: 0.075 cycle_B: 0.538 idt_B: 0.492 \n",
            "(epoch: 87, iters: 200, time: 1.756, data: 0.002) D_A: 0.098 G_A: 0.526 cycle_A: 0.377 idt_A: 0.203 D_B: 0.130 G_B: 0.530 cycle_B: 0.510 idt_B: 0.210 \n",
            "(epoch: 87, iters: 300, time: 0.634, data: 0.001) D_A: 0.110 G_A: 0.664 cycle_A: 0.813 idt_A: 0.507 D_B: 0.101 G_B: 0.608 cycle_B: 1.208 idt_B: 0.373 \n",
            "(epoch: 87, iters: 400, time: 0.636, data: 0.001) D_A: 0.189 G_A: 0.400 cycle_A: 0.977 idt_A: 0.367 D_B: 0.167 G_B: 0.444 cycle_B: 0.711 idt_B: 0.554 \n",
            "(epoch: 87, iters: 500, time: 0.635, data: 0.001) D_A: 0.297 G_A: 0.375 cycle_A: 0.484 idt_A: 0.218 D_B: 0.257 G_B: 0.526 cycle_B: 0.535 idt_B: 0.306 \n",
            "End of epoch 87 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 88, iters: 100, time: 1.700, data: 0.109) D_A: 0.322 G_A: 0.181 cycle_A: 0.509 idt_A: 0.296 D_B: 0.151 G_B: 0.205 cycle_B: 0.491 idt_B: 0.299 \n",
            "(epoch: 88, iters: 200, time: 0.638, data: 0.002) D_A: 0.228 G_A: 0.386 cycle_A: 0.630 idt_A: 0.231 D_B: 0.297 G_B: 0.371 cycle_B: 0.555 idt_B: 0.381 \n",
            "(epoch: 88, iters: 300, time: 0.636, data: 0.001) D_A: 0.237 G_A: 0.436 cycle_A: 0.409 idt_A: 0.276 D_B: 0.255 G_B: 0.403 cycle_B: 0.512 idt_B: 0.193 \n",
            "(epoch: 88, iters: 400, time: 0.636, data: 0.001) D_A: 0.081 G_A: 0.294 cycle_A: 0.529 idt_A: 0.345 D_B: 0.079 G_B: 0.335 cycle_B: 0.485 idt_B: 0.239 \n",
            "(epoch: 88, iters: 500, time: 1.512, data: 0.001) D_A: 0.159 G_A: 0.600 cycle_A: 0.887 idt_A: 0.280 D_B: 0.142 G_B: 0.555 cycle_B: 0.796 idt_B: 0.421 \n",
            "End of epoch 88 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 89, iters: 100, time: 0.633, data: 0.099) D_A: 0.141 G_A: 0.383 cycle_A: 0.930 idt_A: 0.391 D_B: 0.208 G_B: 0.391 cycle_B: 0.816 idt_B: 0.365 \n",
            "(epoch: 89, iters: 200, time: 0.638, data: 0.001) D_A: 0.353 G_A: 0.202 cycle_A: 0.838 idt_A: 0.237 D_B: 0.336 G_B: 0.133 cycle_B: 0.568 idt_B: 0.435 \n",
            "(epoch: 89, iters: 300, time: 0.633, data: 0.001) D_A: 0.314 G_A: 0.571 cycle_A: 0.449 idt_A: 0.270 D_B: 0.219 G_B: 0.225 cycle_B: 0.589 idt_B: 0.250 \n",
            "(epoch: 89, iters: 400, time: 1.514, data: 0.001) D_A: 0.243 G_A: 0.052 cycle_A: 0.784 idt_A: 0.317 D_B: 0.250 G_B: 0.593 cycle_B: 0.689 idt_B: 0.754 \n",
            "(epoch: 89, iters: 500, time: 0.638, data: 0.001) D_A: 0.209 G_A: 0.415 cycle_A: 0.984 idt_A: 0.435 D_B: 0.227 G_B: 0.268 cycle_B: 1.043 idt_B: 0.375 \n",
            "End of epoch 89 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 90, iters: 100, time: 0.635, data: 0.109) D_A: 0.334 G_A: 0.873 cycle_A: 0.711 idt_A: 0.273 D_B: 0.310 G_B: 0.183 cycle_B: 0.616 idt_B: 0.334 \n",
            "(epoch: 90, iters: 200, time: 0.635, data: 0.001) D_A: 0.134 G_A: 0.477 cycle_A: 0.613 idt_A: 0.272 D_B: 0.068 G_B: 0.491 cycle_B: 0.600 idt_B: 0.259 \n",
            "(epoch: 90, iters: 300, time: 1.615, data: 0.001) D_A: 0.282 G_A: 0.356 cycle_A: 0.515 idt_A: 0.288 D_B: 0.231 G_B: 0.242 cycle_B: 0.996 idt_B: 0.302 \n",
            "(epoch: 90, iters: 400, time: 0.636, data: 0.002) D_A: 0.459 G_A: 0.059 cycle_A: 0.566 idt_A: 0.262 D_B: 0.334 G_B: 0.345 cycle_B: 0.545 idt_B: 0.328 \n",
            "(epoch: 90, iters: 500, time: 0.634, data: 0.001) D_A: 0.076 G_A: 0.138 cycle_A: 0.622 idt_A: 0.505 D_B: 0.173 G_B: 0.352 cycle_B: 0.996 idt_B: 0.296 \n",
            "saving the latest model (epoch 90, total_iters 15000)\n",
            "saving the model at the end of epoch 90, iters 15000\n",
            "End of epoch 90 / 200 \t Time Taken: 257 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 91, iters: 100, time: 0.635, data: 0.117) D_A: 0.243 G_A: 0.250 cycle_A: 0.630 idt_A: 0.316 D_B: 0.248 G_B: 0.485 cycle_B: 0.646 idt_B: 0.370 \n",
            "(epoch: 91, iters: 200, time: 1.793, data: 0.001) D_A: 0.164 G_A: 0.101 cycle_A: 1.028 idt_A: 0.281 D_B: 0.206 G_B: 0.697 cycle_B: 0.637 idt_B: 0.473 \n",
            "(epoch: 91, iters: 300, time: 0.636, data: 0.001) D_A: 0.281 G_A: 0.089 cycle_A: 0.379 idt_A: 0.300 D_B: 0.273 G_B: 0.504 cycle_B: 0.625 idt_B: 0.128 \n",
            "(epoch: 91, iters: 400, time: 0.635, data: 0.001) D_A: 0.223 G_A: 0.318 cycle_A: 0.921 idt_A: 0.405 D_B: 0.208 G_B: 0.228 cycle_B: 1.039 idt_B: 0.480 \n",
            "(epoch: 91, iters: 500, time: 0.638, data: 0.001) D_A: 0.342 G_A: 0.326 cycle_A: 0.804 idt_A: 0.265 D_B: 0.180 G_B: 0.418 cycle_B: 0.538 idt_B: 0.285 \n",
            "End of epoch 91 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 92, iters: 100, time: 1.638, data: 0.117) D_A: 0.252 G_A: 0.195 cycle_A: 1.002 idt_A: 0.375 D_B: 0.433 G_B: 0.445 cycle_B: 0.613 idt_B: 0.331 \n",
            "(epoch: 92, iters: 200, time: 0.636, data: 0.002) D_A: 0.095 G_A: 0.412 cycle_A: 0.574 idt_A: 0.301 D_B: 0.198 G_B: 0.390 cycle_B: 0.868 idt_B: 0.263 \n",
            "(epoch: 92, iters: 300, time: 0.636, data: 0.002) D_A: 0.154 G_A: 0.759 cycle_A: 0.910 idt_A: 0.325 D_B: 0.174 G_B: 0.301 cycle_B: 0.644 idt_B: 0.527 \n",
            "(epoch: 92, iters: 400, time: 0.634, data: 0.001) D_A: 0.295 G_A: 0.583 cycle_A: 1.116 idt_A: 0.268 D_B: 0.309 G_B: 0.167 cycle_B: 0.595 idt_B: 0.425 \n",
            "(epoch: 92, iters: 500, time: 1.476, data: 0.001) D_A: 0.135 G_A: 0.073 cycle_A: 0.787 idt_A: 0.276 D_B: 0.206 G_B: 1.006 cycle_B: 0.565 idt_B: 0.392 \n",
            "End of epoch 92 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 93, iters: 100, time: 0.638, data: 0.103) D_A: 0.236 G_A: 0.596 cycle_A: 0.514 idt_A: 0.279 D_B: 0.214 G_B: 0.249 cycle_B: 0.589 idt_B: 0.261 \n",
            "(epoch: 93, iters: 200, time: 0.635, data: 0.001) D_A: 0.264 G_A: 0.565 cycle_A: 0.821 idt_A: 0.222 D_B: 0.145 G_B: 0.172 cycle_B: 0.526 idt_B: 0.236 \n",
            "(epoch: 93, iters: 300, time: 0.636, data: 0.002) D_A: 0.371 G_A: 0.559 cycle_A: 1.163 idt_A: 0.529 D_B: 0.154 G_B: 0.280 cycle_B: 1.299 idt_B: 0.452 \n",
            "(epoch: 93, iters: 400, time: 1.679, data: 0.001) D_A: 0.130 G_A: 0.374 cycle_A: 0.539 idt_A: 0.411 D_B: 0.189 G_B: 0.549 cycle_B: 0.824 idt_B: 0.282 \n",
            "(epoch: 93, iters: 500, time: 0.635, data: 0.002) D_A: 0.181 G_A: 0.122 cycle_A: 1.203 idt_A: 0.462 D_B: 0.226 G_B: 0.299 cycle_B: 0.766 idt_B: 0.466 \n",
            "End of epoch 93 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 94, iters: 100, time: 0.634, data: 0.109) D_A: 0.125 G_A: 0.657 cycle_A: 0.817 idt_A: 0.313 D_B: 0.067 G_B: 0.885 cycle_B: 0.604 idt_B: 0.305 \n",
            "(epoch: 94, iters: 200, time: 0.636, data: 0.002) D_A: 0.190 G_A: 0.699 cycle_A: 0.679 idt_A: 0.262 D_B: 0.204 G_B: 0.371 cycle_B: 0.583 idt_B: 0.335 \n",
            "(epoch: 94, iters: 300, time: 1.636, data: 0.001) D_A: 0.534 G_A: 0.763 cycle_A: 0.567 idt_A: 0.265 D_B: 0.510 G_B: 0.037 cycle_B: 0.619 idt_B: 0.318 \n",
            "(epoch: 94, iters: 400, time: 0.636, data: 0.002) D_A: 0.336 G_A: 0.185 cycle_A: 0.627 idt_A: 0.256 D_B: 0.438 G_B: 0.326 cycle_B: 0.570 idt_B: 0.253 \n",
            "(epoch: 94, iters: 500, time: 0.638, data: 0.002) D_A: 0.204 G_A: 0.191 cycle_A: 0.505 idt_A: 0.282 D_B: 0.238 G_B: 0.554 cycle_B: 0.609 idt_B: 0.310 \n",
            "End of epoch 94 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 95, iters: 100, time: 0.637, data: 0.130) D_A: 0.068 G_A: 0.431 cycle_A: 0.693 idt_A: 0.342 D_B: 0.195 G_B: 0.572 cycle_B: 0.789 idt_B: 0.315 \n",
            "(epoch: 95, iters: 200, time: 1.865, data: 0.002) D_A: 0.307 G_A: 0.517 cycle_A: 0.493 idt_A: 0.285 D_B: 0.121 G_B: 0.546 cycle_B: 0.577 idt_B: 0.259 \n",
            "(epoch: 95, iters: 300, time: 0.638, data: 0.001) D_A: 0.325 G_A: 0.396 cycle_A: 0.580 idt_A: 0.317 D_B: 0.355 G_B: 0.242 cycle_B: 0.621 idt_B: 0.309 \n",
            "(epoch: 95, iters: 400, time: 0.635, data: 0.001) D_A: 0.275 G_A: 0.673 cycle_A: 0.512 idt_A: 0.160 D_B: 0.158 G_B: 0.240 cycle_B: 0.405 idt_B: 0.182 \n",
            "(epoch: 95, iters: 500, time: 0.635, data: 0.001) D_A: 0.129 G_A: 0.388 cycle_A: 0.474 idt_A: 0.232 D_B: 0.080 G_B: 0.441 cycle_B: 0.517 idt_B: 0.207 \n",
            "saving the model at the end of epoch 95, iters 17500\n",
            "End of epoch 95 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 96, iters: 100, time: 1.621, data: 0.113) D_A: 0.130 G_A: 0.203 cycle_A: 0.409 idt_A: 0.284 D_B: 0.058 G_B: 0.723 cycle_B: 0.850 idt_B: 0.134 \n",
            "(epoch: 96, iters: 200, time: 0.636, data: 0.001) D_A: 0.201 G_A: 0.495 cycle_A: 0.297 idt_A: 0.292 D_B: 0.270 G_B: 0.167 cycle_B: 0.719 idt_B: 0.155 \n",
            "(epoch: 96, iters: 300, time: 0.633, data: 0.001) D_A: 0.081 G_A: 0.522 cycle_A: 0.607 idt_A: 0.348 D_B: 0.222 G_B: 0.337 cycle_B: 1.011 idt_B: 0.281 \n",
            "(epoch: 96, iters: 400, time: 0.636, data: 0.001) D_A: 0.141 G_A: 0.279 cycle_A: 0.645 idt_A: 0.327 D_B: 0.193 G_B: 0.269 cycle_B: 0.646 idt_B: 0.242 \n",
            "(epoch: 96, iters: 500, time: 1.557, data: 0.002) D_A: 0.051 G_A: 0.086 cycle_A: 0.996 idt_A: 0.399 D_B: 0.236 G_B: 0.779 cycle_B: 1.312 idt_B: 0.427 \n",
            "End of epoch 96 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 97, iters: 100, time: 0.638, data: 0.099) D_A: 0.039 G_A: 0.579 cycle_A: 0.414 idt_A: 0.381 D_B: 0.058 G_B: 0.629 cycle_B: 0.762 idt_B: 0.226 \n",
            "(epoch: 97, iters: 200, time: 0.635, data: 0.001) D_A: 0.167 G_A: 0.349 cycle_A: 0.930 idt_A: 0.456 D_B: 0.454 G_B: 0.490 cycle_B: 1.013 idt_B: 0.194 \n",
            "(epoch: 97, iters: 300, time: 0.637, data: 0.001) D_A: 0.206 G_A: 0.616 cycle_A: 0.397 idt_A: 0.254 D_B: 0.144 G_B: 0.794 cycle_B: 0.418 idt_B: 0.147 \n",
            "(epoch: 97, iters: 400, time: 1.618, data: 0.001) D_A: 0.175 G_A: 0.285 cycle_A: 0.930 idt_A: 0.318 D_B: 0.122 G_B: 0.637 cycle_B: 0.538 idt_B: 0.457 \n",
            "(epoch: 97, iters: 500, time: 0.634, data: 0.002) D_A: 0.266 G_A: 0.325 cycle_A: 1.419 idt_A: 0.341 D_B: 0.131 G_B: 0.681 cycle_B: 0.961 idt_B: 0.750 \n",
            "End of epoch 97 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 98, iters: 100, time: 0.636, data: 0.108) D_A: 0.095 G_A: 0.463 cycle_A: 0.653 idt_A: 0.337 D_B: 0.071 G_B: 1.412 cycle_B: 0.744 idt_B: 0.316 \n",
            "(epoch: 98, iters: 200, time: 0.634, data: 0.001) D_A: 0.286 G_A: 0.151 cycle_A: 0.572 idt_A: 0.380 D_B: 0.154 G_B: 0.098 cycle_B: 0.889 idt_B: 0.220 \n",
            "(epoch: 98, iters: 300, time: 1.681, data: 0.002) D_A: 0.407 G_A: 0.170 cycle_A: 0.693 idt_A: 0.263 D_B: 0.373 G_B: 0.163 cycle_B: 0.766 idt_B: 0.327 \n",
            "(epoch: 98, iters: 400, time: 0.638, data: 0.002) D_A: 0.252 G_A: 0.497 cycle_A: 0.746 idt_A: 0.304 D_B: 0.152 G_B: 0.165 cycle_B: 0.500 idt_B: 0.326 \n",
            "(epoch: 98, iters: 500, time: 0.637, data: 0.001) D_A: 0.294 G_A: 0.171 cycle_A: 0.537 idt_A: 0.268 D_B: 0.380 G_B: 0.363 cycle_B: 0.516 idt_B: 0.249 \n",
            "End of epoch 98 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0002000 -> 0.0002000\n",
            "(epoch: 99, iters: 100, time: 0.634, data: 0.106) D_A: 0.116 G_A: 0.119 cycle_A: 0.695 idt_A: 0.395 D_B: 0.101 G_B: 1.029 cycle_B: 0.768 idt_B: 0.230 \n",
            "(epoch: 99, iters: 200, time: 1.604, data: 0.002) D_A: 0.193 G_A: 0.174 cycle_A: 0.678 idt_A: 0.322 D_B: 0.152 G_B: 0.273 cycle_B: 0.725 idt_B: 0.294 \n",
            "(epoch: 99, iters: 300, time: 0.637, data: 0.002) D_A: 0.123 G_A: 0.428 cycle_A: 0.622 idt_A: 0.392 D_B: 0.245 G_B: 0.409 cycle_B: 0.669 idt_B: 0.338 \n",
            "(epoch: 99, iters: 400, time: 0.636, data: 0.002) D_A: 0.320 G_A: 0.072 cycle_A: 0.759 idt_A: 0.230 D_B: 0.293 G_B: 0.995 cycle_B: 0.414 idt_B: 0.494 \n",
            "(epoch: 99, iters: 500, time: 0.633, data: 0.002) D_A: 0.135 G_A: 0.497 cycle_A: 0.542 idt_A: 0.167 D_B: 0.158 G_B: 0.463 cycle_B: 0.351 idt_B: 0.248 \n",
            "End of epoch 99 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0002000 -> 0.0001980\n",
            "(epoch: 100, iters: 100, time: 1.654, data: 0.110) D_A: 0.228 G_A: 0.719 cycle_A: 0.726 idt_A: 0.487 D_B: 0.099 G_B: 0.472 cycle_B: 0.549 idt_B: 0.384 \n",
            "(epoch: 100, iters: 200, time: 0.636, data: 0.001) D_A: 0.092 G_A: 0.235 cycle_A: 0.638 idt_A: 0.412 D_B: 0.256 G_B: 0.270 cycle_B: 1.112 idt_B: 0.243 \n",
            "(epoch: 100, iters: 300, time: 0.632, data: 0.001) D_A: 0.176 G_A: 0.571 cycle_A: 0.871 idt_A: 0.433 D_B: 0.261 G_B: 0.506 cycle_B: 1.451 idt_B: 0.489 \n",
            "(epoch: 100, iters: 400, time: 0.636, data: 0.002) D_A: 0.206 G_A: 0.193 cycle_A: 0.471 idt_A: 0.413 D_B: 0.050 G_B: 0.718 cycle_B: 0.809 idt_B: 0.213 \n",
            "(epoch: 100, iters: 500, time: 1.639, data: 0.001) D_A: 0.247 G_A: 0.135 cycle_A: 0.733 idt_A: 0.428 D_B: 0.263 G_B: 0.074 cycle_B: 1.133 idt_B: 0.570 \n",
            "saving the latest model (epoch 100, total_iters 20000)\n",
            "saving the model at the end of epoch 100, iters 20000\n",
            "End of epoch 100 / 200 \t Time Taken: 258 sec\n",
            "learning rate 0.0001980 -> 0.0001960\n",
            "(epoch: 101, iters: 100, time: 0.636, data: 0.117) D_A: 0.219 G_A: 0.633 cycle_A: 0.846 idt_A: 0.722 D_B: 0.223 G_B: 0.207 cycle_B: 1.117 idt_B: 0.443 \n",
            "(epoch: 101, iters: 200, time: 0.636, data: 0.002) D_A: 0.268 G_A: 0.610 cycle_A: 0.807 idt_A: 0.363 D_B: 0.147 G_B: 0.317 cycle_B: 0.782 idt_B: 0.317 \n",
            "(epoch: 101, iters: 300, time: 0.634, data: 0.001) D_A: 0.073 G_A: 0.594 cycle_A: 0.514 idt_A: 0.393 D_B: 0.248 G_B: 0.668 cycle_B: 0.372 idt_B: 0.237 \n",
            "(epoch: 101, iters: 400, time: 1.721, data: 0.002) D_A: 0.270 G_A: 0.355 cycle_A: 0.587 idt_A: 0.238 D_B: 0.183 G_B: 0.207 cycle_B: 0.497 idt_B: 0.267 \n",
            "(epoch: 101, iters: 500, time: 0.632, data: 0.001) D_A: 0.292 G_A: 0.212 cycle_A: 1.545 idt_A: 0.191 D_B: 0.107 G_B: 0.644 cycle_B: 0.533 idt_B: 0.641 \n",
            "End of epoch 101 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0001960 -> 0.0001941\n",
            "(epoch: 102, iters: 100, time: 0.632, data: 0.107) D_A: 0.361 G_A: 0.199 cycle_A: 0.646 idt_A: 0.344 D_B: 0.338 G_B: 0.431 cycle_B: 0.739 idt_B: 0.406 \n",
            "(epoch: 102, iters: 200, time: 0.638, data: 0.002) D_A: 0.147 G_A: 0.279 cycle_A: 0.619 idt_A: 0.289 D_B: 0.325 G_B: 0.395 cycle_B: 0.686 idt_B: 0.382 \n",
            "(epoch: 102, iters: 300, time: 1.914, data: 0.001) D_A: 0.294 G_A: 0.470 cycle_A: 0.751 idt_A: 0.191 D_B: 0.408 G_B: 0.036 cycle_B: 0.333 idt_B: 0.396 \n",
            "(epoch: 102, iters: 400, time: 0.638, data: 0.001) D_A: 0.287 G_A: 0.846 cycle_A: 0.653 idt_A: 0.217 D_B: 0.232 G_B: 0.272 cycle_B: 0.448 idt_B: 0.228 \n",
            "(epoch: 102, iters: 500, time: 0.638, data: 0.001) D_A: 0.466 G_A: 1.236 cycle_A: 1.360 idt_A: 0.254 D_B: 0.238 G_B: 0.137 cycle_B: 0.553 idt_B: 0.428 \n",
            "End of epoch 102 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0001941 -> 0.0001921\n",
            "(epoch: 103, iters: 100, time: 0.632, data: 0.108) D_A: 0.123 G_A: 0.246 cycle_A: 0.870 idt_A: 0.193 D_B: 0.227 G_B: 0.512 cycle_B: 0.446 idt_B: 0.314 \n",
            "(epoch: 103, iters: 200, time: 1.643, data: 0.002) D_A: 0.290 G_A: 0.691 cycle_A: 0.986 idt_A: 0.390 D_B: 0.198 G_B: 0.238 cycle_B: 0.959 idt_B: 0.449 \n",
            "(epoch: 103, iters: 300, time: 0.636, data: 0.002) D_A: 0.178 G_A: 0.581 cycle_A: 1.240 idt_A: 0.328 D_B: 0.128 G_B: 0.516 cycle_B: 0.667 idt_B: 0.562 \n",
            "(epoch: 103, iters: 400, time: 0.635, data: 0.002) D_A: 0.202 G_A: 0.491 cycle_A: 0.547 idt_A: 0.203 D_B: 0.186 G_B: 0.386 cycle_B: 0.630 idt_B: 0.315 \n",
            "(epoch: 103, iters: 500, time: 0.637, data: 0.001) D_A: 0.056 G_A: 0.651 cycle_A: 0.586 idt_A: 0.392 D_B: 0.165 G_B: 0.530 cycle_B: 0.764 idt_B: 0.263 \n",
            "End of epoch 103 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0001921 -> 0.0001901\n",
            "(epoch: 104, iters: 100, time: 1.796, data: 0.118) D_A: 0.233 G_A: 0.354 cycle_A: 0.540 idt_A: 0.180 D_B: 0.311 G_B: 0.185 cycle_B: 0.402 idt_B: 0.267 \n",
            "(epoch: 104, iters: 200, time: 0.638, data: 0.001) D_A: 0.271 G_A: 0.357 cycle_A: 0.375 idt_A: 0.277 D_B: 0.100 G_B: 0.282 cycle_B: 0.534 idt_B: 0.172 \n",
            "(epoch: 104, iters: 300, time: 0.636, data: 0.002) D_A: 0.122 G_A: 0.556 cycle_A: 0.715 idt_A: 0.363 D_B: 0.058 G_B: 0.291 cycle_B: 0.996 idt_B: 0.205 \n",
            "(epoch: 104, iters: 400, time: 0.637, data: 0.002) D_A: 0.359 G_A: 0.315 cycle_A: 0.716 idt_A: 0.293 D_B: 0.448 G_B: 0.237 cycle_B: 0.603 idt_B: 0.408 \n",
            "(epoch: 104, iters: 500, time: 1.731, data: 0.001) D_A: 0.168 G_A: 0.277 cycle_A: 0.403 idt_A: 0.221 D_B: 0.154 G_B: 0.469 cycle_B: 0.758 idt_B: 0.213 \n",
            "End of epoch 104 / 200 \t Time Taken: 257 sec\n",
            "learning rate 0.0001901 -> 0.0001881\n",
            "(epoch: 105, iters: 100, time: 0.637, data: 0.107) D_A: 0.486 G_A: 0.852 cycle_A: 0.665 idt_A: 0.318 D_B: 0.310 G_B: 0.094 cycle_B: 0.616 idt_B: 0.329 \n",
            "(epoch: 105, iters: 200, time: 0.636, data: 0.001) D_A: 0.125 G_A: 0.497 cycle_A: 0.603 idt_A: 0.164 D_B: 0.140 G_B: 0.489 cycle_B: 0.496 idt_B: 0.287 \n",
            "(epoch: 105, iters: 300, time: 0.635, data: 0.002) D_A: 0.139 G_A: 0.690 cycle_A: 0.595 idt_A: 0.188 D_B: 0.454 G_B: 0.441 cycle_B: 0.457 idt_B: 0.334 \n",
            "(epoch: 105, iters: 400, time: 1.688, data: 0.001) D_A: 0.247 G_A: 0.158 cycle_A: 0.896 idt_A: 0.360 D_B: 0.209 G_B: 0.288 cycle_B: 0.731 idt_B: 0.315 \n",
            "(epoch: 105, iters: 500, time: 0.637, data: 0.001) D_A: 0.178 G_A: 0.646 cycle_A: 0.544 idt_A: 0.229 D_B: 0.160 G_B: 0.246 cycle_B: 0.411 idt_B: 0.206 \n",
            "saving the model at the end of epoch 105, iters 22500\n",
            "End of epoch 105 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0001881 -> 0.0001861\n",
            "(epoch: 106, iters: 100, time: 0.636, data: 0.114) D_A: 0.077 G_A: 0.366 cycle_A: 0.561 idt_A: 0.420 D_B: 0.201 G_B: 0.682 cycle_B: 0.858 idt_B: 0.257 \n",
            "(epoch: 106, iters: 200, time: 0.637, data: 0.001) D_A: 0.221 G_A: 0.376 cycle_A: 0.668 idt_A: 0.450 D_B: 0.252 G_B: 0.240 cycle_B: 0.889 idt_B: 0.249 \n",
            "(epoch: 106, iters: 300, time: 1.710, data: 0.001) D_A: 0.059 G_A: 0.105 cycle_A: 0.496 idt_A: 0.280 D_B: 0.307 G_B: 0.464 cycle_B: 0.598 idt_B: 0.245 \n",
            "(epoch: 106, iters: 400, time: 0.638, data: 0.002) D_A: 0.049 G_A: 0.373 cycle_A: 0.563 idt_A: 0.362 D_B: 0.181 G_B: 0.302 cycle_B: 0.811 idt_B: 0.274 \n",
            "(epoch: 106, iters: 500, time: 0.635, data: 0.002) D_A: 0.280 G_A: 0.259 cycle_A: 0.613 idt_A: 0.330 D_B: 0.361 G_B: 0.206 cycle_B: 0.666 idt_B: 0.298 \n",
            "End of epoch 106 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0001861 -> 0.0001842\n",
            "(epoch: 107, iters: 100, time: 0.639, data: 0.106) D_A: 0.060 G_A: 0.327 cycle_A: 0.570 idt_A: 1.394 D_B: 0.220 G_B: 0.198 cycle_B: 1.666 idt_B: 0.282 \n",
            "(epoch: 107, iters: 200, time: 1.611, data: 0.001) D_A: 0.058 G_A: 0.268 cycle_A: 0.643 idt_A: 0.256 D_B: 0.323 G_B: 0.193 cycle_B: 0.517 idt_B: 0.327 \n",
            "(epoch: 107, iters: 300, time: 0.635, data: 0.002) D_A: 0.237 G_A: 0.354 cycle_A: 0.478 idt_A: 0.369 D_B: 0.222 G_B: 0.449 cycle_B: 0.900 idt_B: 0.261 \n",
            "(epoch: 107, iters: 400, time: 0.634, data: 0.001) D_A: 0.207 G_A: 0.189 cycle_A: 1.139 idt_A: 0.285 D_B: 0.081 G_B: 0.485 cycle_B: 0.529 idt_B: 0.602 \n",
            "(epoch: 107, iters: 500, time: 0.632, data: 0.002) D_A: 0.221 G_A: 0.248 cycle_A: 0.713 idt_A: 0.251 D_B: 0.172 G_B: 0.261 cycle_B: 0.455 idt_B: 0.336 \n",
            "End of epoch 107 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0001842 -> 0.0001822\n",
            "(epoch: 108, iters: 100, time: 1.694, data: 0.102) D_A: 0.289 G_A: 0.133 cycle_A: 0.662 idt_A: 0.235 D_B: 0.389 G_B: 0.464 cycle_B: 0.395 idt_B: 0.322 \n",
            "(epoch: 108, iters: 200, time: 0.639, data: 0.001) D_A: 0.071 G_A: 0.533 cycle_A: 0.601 idt_A: 0.297 D_B: 0.103 G_B: 0.363 cycle_B: 0.716 idt_B: 0.263 \n",
            "(epoch: 108, iters: 300, time: 0.637, data: 0.001) D_A: 0.373 G_A: 0.661 cycle_A: 0.395 idt_A: 0.220 D_B: 0.082 G_B: 0.207 cycle_B: 0.456 idt_B: 0.235 \n",
            "(epoch: 108, iters: 400, time: 0.639, data: 0.001) D_A: 0.359 G_A: 0.658 cycle_A: 0.366 idt_A: 0.333 D_B: 0.104 G_B: 0.382 cycle_B: 0.776 idt_B: 0.211 \n",
            "(epoch: 108, iters: 500, time: 1.680, data: 0.001) D_A: 0.335 G_A: 0.397 cycle_A: 0.909 idt_A: 0.260 D_B: 0.405 G_B: 0.055 cycle_B: 0.672 idt_B: 0.308 \n",
            "End of epoch 108 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0001822 -> 0.0001802\n",
            "(epoch: 109, iters: 100, time: 0.637, data: 0.124) D_A: 0.042 G_A: 0.553 cycle_A: 0.578 idt_A: 0.222 D_B: 0.131 G_B: 0.616 cycle_B: 0.871 idt_B: 0.273 \n",
            "(epoch: 109, iters: 200, time: 0.636, data: 0.002) D_A: 0.126 G_A: 0.317 cycle_A: 0.396 idt_A: 0.494 D_B: 0.187 G_B: 1.139 cycle_B: 1.262 idt_B: 0.153 \n",
            "(epoch: 109, iters: 300, time: 0.635, data: 0.002) D_A: 0.247 G_A: 0.239 cycle_A: 0.936 idt_A: 0.223 D_B: 0.330 G_B: 0.130 cycle_B: 0.388 idt_B: 0.406 \n",
            "(epoch: 109, iters: 400, time: 1.985, data: 0.002) D_A: 0.247 G_A: 0.434 cycle_A: 0.670 idt_A: 0.344 D_B: 0.210 G_B: 0.372 cycle_B: 0.588 idt_B: 0.307 \n",
            "(epoch: 109, iters: 500, time: 0.637, data: 0.001) D_A: 0.181 G_A: 0.252 cycle_A: 0.791 idt_A: 0.170 D_B: 0.228 G_B: 0.534 cycle_B: 0.412 idt_B: 0.401 \n",
            "End of epoch 109 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0001802 -> 0.0001782\n",
            "(epoch: 110, iters: 100, time: 0.631, data: 0.110) D_A: 0.228 G_A: 0.235 cycle_A: 0.639 idt_A: 0.252 D_B: 0.098 G_B: 0.811 cycle_B: 0.570 idt_B: 0.302 \n",
            "(epoch: 110, iters: 200, time: 0.635, data: 0.001) D_A: 0.051 G_A: 0.935 cycle_A: 0.396 idt_A: 0.305 D_B: 0.093 G_B: 0.670 cycle_B: 0.544 idt_B: 0.202 \n",
            "(epoch: 110, iters: 300, time: 1.697, data: 0.001) D_A: 0.107 G_A: 0.481 cycle_A: 0.709 idt_A: 0.304 D_B: 0.304 G_B: 1.024 cycle_B: 0.993 idt_B: 0.338 \n",
            "(epoch: 110, iters: 400, time: 0.637, data: 0.002) D_A: 0.044 G_A: 0.802 cycle_A: 0.722 idt_A: 0.252 D_B: 0.082 G_B: 0.501 cycle_B: 0.694 idt_B: 0.382 \n",
            "(epoch: 110, iters: 500, time: 0.635, data: 0.001) D_A: 0.134 G_A: 0.332 cycle_A: 0.493 idt_A: 0.298 D_B: 0.290 G_B: 0.252 cycle_B: 0.886 idt_B: 0.304 \n",
            "saving the latest model (epoch 110, total_iters 25000)\n",
            "saving the model at the end of epoch 110, iters 25000\n",
            "End of epoch 110 / 200 \t Time Taken: 257 sec\n",
            "learning rate 0.0001782 -> 0.0001762\n",
            "(epoch: 111, iters: 100, time: 0.636, data: 0.118) D_A: 0.322 G_A: 0.542 cycle_A: 0.648 idt_A: 0.423 D_B: 0.236 G_B: 0.314 cycle_B: 0.491 idt_B: 0.253 \n",
            "(epoch: 111, iters: 200, time: 1.709, data: 0.001) D_A: 0.142 G_A: 0.513 cycle_A: 0.599 idt_A: 0.223 D_B: 0.103 G_B: 0.330 cycle_B: 0.457 idt_B: 0.273 \n",
            "(epoch: 111, iters: 300, time: 0.636, data: 0.001) D_A: 0.243 G_A: 0.296 cycle_A: 0.541 idt_A: 0.381 D_B: 0.122 G_B: 0.099 cycle_B: 0.633 idt_B: 0.263 \n",
            "(epoch: 111, iters: 400, time: 0.637, data: 0.001) D_A: 0.259 G_A: 0.302 cycle_A: 0.557 idt_A: 0.388 D_B: 0.213 G_B: 0.160 cycle_B: 0.933 idt_B: 0.255 \n",
            "(epoch: 111, iters: 500, time: 0.637, data: 0.002) D_A: 0.144 G_A: 0.496 cycle_A: 0.556 idt_A: 0.232 D_B: 0.220 G_B: 0.223 cycle_B: 0.439 idt_B: 0.288 \n",
            "End of epoch 111 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0001762 -> 0.0001743\n",
            "(epoch: 112, iters: 100, time: 1.731, data: 0.113) D_A: 0.056 G_A: 0.544 cycle_A: 0.613 idt_A: 0.397 D_B: 0.180 G_B: 0.505 cycle_B: 0.989 idt_B: 0.292 \n",
            "(epoch: 112, iters: 200, time: 0.636, data: 0.002) D_A: 0.208 G_A: 0.837 cycle_A: 0.659 idt_A: 0.276 D_B: 0.098 G_B: 0.437 cycle_B: 0.486 idt_B: 0.338 \n",
            "(epoch: 112, iters: 300, time: 0.637, data: 0.002) D_A: 0.178 G_A: 0.197 cycle_A: 0.515 idt_A: 0.396 D_B: 0.196 G_B: 0.260 cycle_B: 0.774 idt_B: 0.261 \n",
            "(epoch: 112, iters: 400, time: 0.633, data: 0.002) D_A: 0.248 G_A: 0.385 cycle_A: 0.548 idt_A: 0.344 D_B: 0.120 G_B: 0.222 cycle_B: 0.767 idt_B: 0.262 \n",
            "(epoch: 112, iters: 500, time: 1.922, data: 0.001) D_A: 0.177 G_A: 0.591 cycle_A: 0.892 idt_A: 0.266 D_B: 0.102 G_B: 0.780 cycle_B: 0.509 idt_B: 0.384 \n",
            "End of epoch 112 / 200 \t Time Taken: 257 sec\n",
            "learning rate 0.0001743 -> 0.0001723\n",
            "(epoch: 113, iters: 100, time: 0.636, data: 0.110) D_A: 0.093 G_A: 0.418 cycle_A: 1.110 idt_A: 0.519 D_B: 0.147 G_B: 0.418 cycle_B: 0.984 idt_B: 0.504 \n",
            "(epoch: 113, iters: 200, time: 0.636, data: 0.001) D_A: 0.120 G_A: 0.210 cycle_A: 0.659 idt_A: 0.301 D_B: 0.284 G_B: 0.602 cycle_B: 0.693 idt_B: 0.314 \n",
            "(epoch: 113, iters: 300, time: 0.638, data: 0.002) D_A: 0.326 G_A: 0.602 cycle_A: 0.715 idt_A: 0.342 D_B: 0.093 G_B: 0.123 cycle_B: 0.728 idt_B: 0.384 \n",
            "(epoch: 113, iters: 400, time: 1.749, data: 0.001) D_A: 0.243 G_A: 0.123 cycle_A: 0.644 idt_A: 0.221 D_B: 0.189 G_B: 0.540 cycle_B: 0.582 idt_B: 0.294 \n",
            "(epoch: 113, iters: 500, time: 0.633, data: 0.001) D_A: 0.369 G_A: 0.536 cycle_A: 0.599 idt_A: 0.210 D_B: 0.334 G_B: 0.125 cycle_B: 0.562 idt_B: 0.302 \n",
            "End of epoch 113 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0001723 -> 0.0001703\n",
            "(epoch: 114, iters: 100, time: 0.635, data: 0.104) D_A: 0.193 G_A: 0.559 cycle_A: 0.498 idt_A: 0.219 D_B: 0.095 G_B: 0.372 cycle_B: 0.534 idt_B: 0.251 \n",
            "(epoch: 114, iters: 200, time: 0.635, data: 0.001) D_A: 0.099 G_A: 0.176 cycle_A: 0.626 idt_A: 0.274 D_B: 0.196 G_B: 0.686 cycle_B: 0.756 idt_B: 0.243 \n",
            "(epoch: 114, iters: 300, time: 1.750, data: 0.001) D_A: 0.198 G_A: 0.321 cycle_A: 0.527 idt_A: 0.329 D_B: 0.211 G_B: 0.273 cycle_B: 0.878 idt_B: 0.262 \n",
            "(epoch: 114, iters: 400, time: 0.636, data: 0.002) D_A: 0.138 G_A: 0.053 cycle_A: 0.804 idt_A: 0.217 D_B: 0.231 G_B: 0.254 cycle_B: 0.538 idt_B: 0.295 \n",
            "(epoch: 114, iters: 500, time: 0.636, data: 0.002) D_A: 0.454 G_A: 0.230 cycle_A: 0.678 idt_A: 0.197 D_B: 0.112 G_B: 0.170 cycle_B: 0.382 idt_B: 0.396 \n",
            "End of epoch 114 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0001703 -> 0.0001683\n",
            "(epoch: 115, iters: 100, time: 0.641, data: 0.115) D_A: 0.202 G_A: 0.245 cycle_A: 0.985 idt_A: 0.284 D_B: 0.334 G_B: 0.347 cycle_B: 0.463 idt_B: 0.304 \n",
            "(epoch: 115, iters: 200, time: 1.682, data: 0.001) D_A: 0.197 G_A: 0.324 cycle_A: 0.590 idt_A: 0.291 D_B: 0.132 G_B: 0.471 cycle_B: 0.716 idt_B: 0.339 \n",
            "(epoch: 115, iters: 300, time: 0.638, data: 0.001) D_A: 0.124 G_A: 0.928 cycle_A: 0.628 idt_A: 0.354 D_B: 0.079 G_B: 0.518 cycle_B: 0.782 idt_B: 0.191 \n",
            "(epoch: 115, iters: 400, time: 0.636, data: 0.001) D_A: 0.119 G_A: 0.352 cycle_A: 0.532 idt_A: 0.185 D_B: 0.201 G_B: 0.322 cycle_B: 0.394 idt_B: 0.229 \n",
            "(epoch: 115, iters: 500, time: 0.634, data: 0.002) D_A: 0.195 G_A: 0.368 cycle_A: 0.607 idt_A: 0.253 D_B: 0.201 G_B: 0.336 cycle_B: 0.596 idt_B: 0.299 \n",
            "saving the model at the end of epoch 115, iters 27500\n",
            "End of epoch 115 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0001683 -> 0.0001663\n",
            "(epoch: 116, iters: 100, time: 2.045, data: 0.114) D_A: 0.296 G_A: 0.903 cycle_A: 0.815 idt_A: 0.444 D_B: 0.104 G_B: 0.353 cycle_B: 0.550 idt_B: 0.301 \n",
            "(epoch: 116, iters: 200, time: 0.635, data: 0.001) D_A: 0.337 G_A: 0.579 cycle_A: 0.487 idt_A: 0.318 D_B: 0.105 G_B: 0.117 cycle_B: 0.705 idt_B: 0.248 \n",
            "(epoch: 116, iters: 300, time: 0.635, data: 0.001) D_A: 0.081 G_A: 0.596 cycle_A: 0.381 idt_A: 0.235 D_B: 0.220 G_B: 0.522 cycle_B: 0.549 idt_B: 0.181 \n",
            "(epoch: 116, iters: 400, time: 0.634, data: 0.001) D_A: 0.326 G_A: 0.462 cycle_A: 0.423 idt_A: 0.303 D_B: 0.094 G_B: 0.265 cycle_B: 0.477 idt_B: 0.252 \n",
            "(epoch: 116, iters: 500, time: 1.777, data: 0.002) D_A: 0.182 G_A: 0.441 cycle_A: 0.498 idt_A: 0.252 D_B: 0.211 G_B: 0.290 cycle_B: 0.385 idt_B: 0.343 \n",
            "End of epoch 116 / 200 \t Time Taken: 257 sec\n",
            "learning rate 0.0001663 -> 0.0001644\n",
            "(epoch: 117, iters: 100, time: 0.635, data: 0.115) D_A: 0.134 G_A: 0.535 cycle_A: 0.419 idt_A: 0.225 D_B: 0.153 G_B: 0.210 cycle_B: 0.476 idt_B: 0.149 \n",
            "(epoch: 117, iters: 200, time: 0.633, data: 0.001) D_A: 0.167 G_A: 0.338 cycle_A: 1.013 idt_A: 0.226 D_B: 0.263 G_B: 0.289 cycle_B: 0.447 idt_B: 0.384 \n",
            "(epoch: 117, iters: 300, time: 0.636, data: 0.002) D_A: 0.126 G_A: 0.441 cycle_A: 0.843 idt_A: 0.246 D_B: 0.069 G_B: 0.254 cycle_B: 0.577 idt_B: 0.236 \n",
            "(epoch: 117, iters: 400, time: 1.707, data: 0.001) D_A: 0.424 G_A: 0.351 cycle_A: 0.563 idt_A: 0.255 D_B: 0.370 G_B: 0.157 cycle_B: 0.477 idt_B: 0.320 \n",
            "(epoch: 117, iters: 500, time: 0.635, data: 0.002) D_A: 0.086 G_A: 0.457 cycle_A: 0.786 idt_A: 0.437 D_B: 0.084 G_B: 0.609 cycle_B: 0.920 idt_B: 0.206 \n",
            "End of epoch 117 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0001644 -> 0.0001624\n",
            "(epoch: 118, iters: 100, time: 0.637, data: 0.104) D_A: 0.198 G_A: 0.296 cycle_A: 0.727 idt_A: 0.197 D_B: 0.174 G_B: 0.289 cycle_B: 0.383 idt_B: 0.383 \n",
            "(epoch: 118, iters: 200, time: 0.637, data: 0.002) D_A: 0.092 G_A: 0.439 cycle_A: 0.559 idt_A: 0.297 D_B: 0.216 G_B: 0.877 cycle_B: 0.677 idt_B: 0.398 \n",
            "(epoch: 118, iters: 300, time: 1.838, data: 0.002) D_A: 0.321 G_A: 0.660 cycle_A: 0.628 idt_A: 0.336 D_B: 0.197 G_B: 0.239 cycle_B: 0.542 idt_B: 0.308 \n",
            "(epoch: 118, iters: 400, time: 0.637, data: 0.001) D_A: 0.287 G_A: 0.558 cycle_A: 0.748 idt_A: 0.292 D_B: 0.070 G_B: 0.377 cycle_B: 0.761 idt_B: 0.257 \n",
            "(epoch: 118, iters: 500, time: 0.634, data: 0.001) D_A: 0.045 G_A: 0.788 cycle_A: 0.767 idt_A: 0.246 D_B: 0.164 G_B: 0.140 cycle_B: 0.596 idt_B: 0.400 \n",
            "End of epoch 118 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0001624 -> 0.0001604\n",
            "(epoch: 119, iters: 100, time: 0.633, data: 0.111) D_A: 0.327 G_A: 0.624 cycle_A: 0.709 idt_A: 0.260 D_B: 0.200 G_B: 0.420 cycle_B: 0.611 idt_B: 0.253 \n",
            "(epoch: 119, iters: 200, time: 1.975, data: 0.001) D_A: 0.226 G_A: 0.528 cycle_A: 0.587 idt_A: 0.428 D_B: 0.174 G_B: 0.299 cycle_B: 0.995 idt_B: 0.310 \n",
            "(epoch: 119, iters: 300, time: 0.634, data: 0.001) D_A: 0.221 G_A: 0.677 cycle_A: 0.690 idt_A: 0.331 D_B: 0.030 G_B: 1.350 cycle_B: 0.685 idt_B: 0.301 \n",
            "(epoch: 119, iters: 400, time: 0.635, data: 0.001) D_A: 0.223 G_A: 0.183 cycle_A: 0.559 idt_A: 0.503 D_B: 0.162 G_B: 0.516 cycle_B: 1.166 idt_B: 0.237 \n",
            "(epoch: 119, iters: 500, time: 0.634, data: 0.001) D_A: 0.114 G_A: 1.224 cycle_A: 0.505 idt_A: 0.323 D_B: 0.134 G_B: 0.479 cycle_B: 0.746 idt_B: 0.216 \n",
            "End of epoch 119 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0001604 -> 0.0001584\n",
            "(epoch: 120, iters: 100, time: 1.711, data: 0.111) D_A: 0.139 G_A: 0.467 cycle_A: 0.553 idt_A: 0.349 D_B: 0.214 G_B: 0.286 cycle_B: 0.651 idt_B: 0.184 \n",
            "(epoch: 120, iters: 200, time: 0.637, data: 0.001) D_A: 0.162 G_A: 0.064 cycle_A: 0.494 idt_A: 0.291 D_B: 0.276 G_B: 0.528 cycle_B: 0.683 idt_B: 0.271 \n",
            "(epoch: 120, iters: 300, time: 0.635, data: 0.001) D_A: 0.194 G_A: 0.181 cycle_A: 0.604 idt_A: 0.171 D_B: 0.074 G_B: 0.601 cycle_B: 0.421 idt_B: 0.292 \n",
            "(epoch: 120, iters: 400, time: 0.632, data: 0.001) D_A: 0.135 G_A: 0.517 cycle_A: 0.466 idt_A: 0.221 D_B: 0.181 G_B: 0.332 cycle_B: 0.612 idt_B: 0.314 \n",
            "(epoch: 120, iters: 500, time: 1.814, data: 0.001) D_A: 0.121 G_A: 0.361 cycle_A: 0.584 idt_A: 0.178 D_B: 0.170 G_B: 0.415 cycle_B: 0.435 idt_B: 0.170 \n",
            "saving the latest model (epoch 120, total_iters 30000)\n",
            "saving the model at the end of epoch 120, iters 30000\n",
            "End of epoch 120 / 200 \t Time Taken: 258 sec\n",
            "learning rate 0.0001584 -> 0.0001564\n",
            "(epoch: 121, iters: 100, time: 0.639, data: 0.114) D_A: 0.193 G_A: 0.251 cycle_A: 0.728 idt_A: 0.509 D_B: 0.266 G_B: 0.079 cycle_B: 0.711 idt_B: 0.253 \n",
            "(epoch: 121, iters: 200, time: 0.632, data: 0.001) D_A: 0.153 G_A: 0.408 cycle_A: 0.790 idt_A: 0.377 D_B: 0.146 G_B: 0.355 cycle_B: 0.573 idt_B: 0.223 \n",
            "(epoch: 121, iters: 300, time: 0.633, data: 0.001) D_A: 0.172 G_A: 0.682 cycle_A: 0.426 idt_A: 0.234 D_B: 0.128 G_B: 0.725 cycle_B: 0.478 idt_B: 0.163 \n",
            "(epoch: 121, iters: 400, time: 1.713, data: 0.001) D_A: 0.235 G_A: 0.274 cycle_A: 0.903 idt_A: 0.580 D_B: 0.145 G_B: 0.351 cycle_B: 1.154 idt_B: 0.492 \n",
            "(epoch: 121, iters: 500, time: 0.639, data: 0.001) D_A: 0.115 G_A: 0.758 cycle_A: 0.456 idt_A: 0.211 D_B: 0.214 G_B: 0.299 cycle_B: 0.411 idt_B: 0.286 \n",
            "End of epoch 121 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0001564 -> 0.0001545\n",
            "(epoch: 122, iters: 100, time: 0.636, data: 0.117) D_A: 0.247 G_A: 0.151 cycle_A: 0.679 idt_A: 0.351 D_B: 0.519 G_B: 1.630 cycle_B: 0.709 idt_B: 0.216 \n",
            "(epoch: 122, iters: 200, time: 0.634, data: 0.002) D_A: 0.089 G_A: 0.260 cycle_A: 0.521 idt_A: 0.236 D_B: 0.085 G_B: 0.773 cycle_B: 0.609 idt_B: 0.259 \n",
            "(epoch: 122, iters: 300, time: 2.058, data: 0.001) D_A: 0.064 G_A: 0.664 cycle_A: 0.482 idt_A: 0.283 D_B: 0.100 G_B: 0.485 cycle_B: 0.617 idt_B: 0.263 \n",
            "(epoch: 122, iters: 400, time: 0.636, data: 0.002) D_A: 0.304 G_A: 0.167 cycle_A: 0.539 idt_A: 0.334 D_B: 0.437 G_B: 0.267 cycle_B: 0.572 idt_B: 0.272 \n",
            "(epoch: 122, iters: 500, time: 0.635, data: 0.001) D_A: 0.227 G_A: 0.222 cycle_A: 0.418 idt_A: 0.259 D_B: 0.231 G_B: 0.428 cycle_B: 0.516 idt_B: 0.234 \n",
            "End of epoch 122 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0001545 -> 0.0001525\n",
            "(epoch: 123, iters: 100, time: 0.635, data: 0.111) D_A: 0.298 G_A: 0.248 cycle_A: 0.699 idt_A: 0.264 D_B: 0.209 G_B: 0.312 cycle_B: 0.543 idt_B: 0.370 \n",
            "(epoch: 123, iters: 200, time: 1.801, data: 0.001) D_A: 0.078 G_A: 0.155 cycle_A: 0.537 idt_A: 0.386 D_B: 0.122 G_B: 1.358 cycle_B: 0.644 idt_B: 0.351 \n",
            "(epoch: 123, iters: 300, time: 0.633, data: 0.002) D_A: 0.149 G_A: 1.082 cycle_A: 0.610 idt_A: 0.232 D_B: 0.219 G_B: 0.257 cycle_B: 0.731 idt_B: 0.262 \n",
            "(epoch: 123, iters: 400, time: 0.637, data: 0.001) D_A: 0.136 G_A: 0.327 cycle_A: 0.529 idt_A: 0.259 D_B: 0.115 G_B: 0.841 cycle_B: 0.461 idt_B: 0.241 \n",
            "(epoch: 123, iters: 500, time: 0.637, data: 0.001) D_A: 0.106 G_A: 0.557 cycle_A: 0.991 idt_A: 0.185 D_B: 0.249 G_B: 0.147 cycle_B: 0.360 idt_B: 0.582 \n",
            "End of epoch 123 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0001525 -> 0.0001505\n",
            "(epoch: 124, iters: 100, time: 1.820, data: 0.127) D_A: 0.046 G_A: 0.148 cycle_A: 0.553 idt_A: 0.297 D_B: 0.337 G_B: 1.071 cycle_B: 0.654 idt_B: 0.309 \n",
            "(epoch: 124, iters: 200, time: 0.636, data: 0.001) D_A: 0.249 G_A: 0.652 cycle_A: 0.642 idt_A: 0.276 D_B: 0.181 G_B: 0.258 cycle_B: 0.582 idt_B: 0.298 \n",
            "(epoch: 124, iters: 300, time: 0.635, data: 0.002) D_A: 0.194 G_A: 0.269 cycle_A: 0.574 idt_A: 0.194 D_B: 0.381 G_B: 0.767 cycle_B: 0.525 idt_B: 0.307 \n",
            "(epoch: 124, iters: 400, time: 0.635, data: 0.001) D_A: 0.050 G_A: 0.331 cycle_A: 0.465 idt_A: 0.521 D_B: 0.146 G_B: 0.557 cycle_B: 0.802 idt_B: 0.199 \n",
            "(epoch: 124, iters: 500, time: 1.897, data: 0.001) D_A: 0.100 G_A: 0.275 cycle_A: 1.174 idt_A: 0.192 D_B: 0.396 G_B: 0.666 cycle_B: 0.804 idt_B: 0.484 \n",
            "End of epoch 124 / 200 \t Time Taken: 257 sec\n",
            "learning rate 0.0001505 -> 0.0001485\n",
            "(epoch: 125, iters: 100, time: 0.637, data: 0.115) D_A: 0.190 G_A: 0.510 cycle_A: 0.613 idt_A: 0.253 D_B: 0.170 G_B: 0.481 cycle_B: 0.732 idt_B: 0.329 \n",
            "(epoch: 125, iters: 200, time: 0.636, data: 0.001) D_A: 0.104 G_A: 0.631 cycle_A: 0.530 idt_A: 0.204 D_B: 0.073 G_B: 0.536 cycle_B: 0.419 idt_B: 0.251 \n",
            "(epoch: 125, iters: 300, time: 0.634, data: 0.001) D_A: 0.201 G_A: 0.215 cycle_A: 0.477 idt_A: 0.333 D_B: 0.301 G_B: 0.271 cycle_B: 0.844 idt_B: 0.263 \n",
            "(epoch: 125, iters: 400, time: 2.093, data: 0.001) D_A: 0.224 G_A: 0.289 cycle_A: 0.781 idt_A: 0.311 D_B: 0.323 G_B: 0.113 cycle_B: 0.593 idt_B: 0.398 \n",
            "(epoch: 125, iters: 500, time: 0.637, data: 0.002) D_A: 0.243 G_A: 0.136 cycle_A: 0.503 idt_A: 0.298 D_B: 0.508 G_B: 0.780 cycle_B: 0.649 idt_B: 0.298 \n",
            "saving the model at the end of epoch 125, iters 32500\n",
            "End of epoch 125 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0001485 -> 0.0001465\n",
            "(epoch: 126, iters: 100, time: 0.632, data: 0.129) D_A: 0.082 G_A: 0.424 cycle_A: 0.630 idt_A: 0.169 D_B: 0.082 G_B: 0.496 cycle_B: 0.359 idt_B: 0.259 \n",
            "(epoch: 126, iters: 200, time: 0.637, data: 0.001) D_A: 0.262 G_A: 0.296 cycle_A: 0.644 idt_A: 0.447 D_B: 0.130 G_B: 0.427 cycle_B: 0.636 idt_B: 0.263 \n",
            "(epoch: 126, iters: 300, time: 1.905, data: 0.001) D_A: 0.245 G_A: 0.357 cycle_A: 0.372 idt_A: 0.195 D_B: 0.285 G_B: 0.628 cycle_B: 0.633 idt_B: 0.161 \n",
            "(epoch: 126, iters: 400, time: 0.636, data: 0.001) D_A: 0.040 G_A: 0.773 cycle_A: 0.689 idt_A: 0.454 D_B: 0.082 G_B: 0.227 cycle_B: 0.775 idt_B: 0.268 \n",
            "(epoch: 126, iters: 500, time: 0.636, data: 0.001) D_A: 0.096 G_A: 0.273 cycle_A: 0.744 idt_A: 0.406 D_B: 0.113 G_B: 0.269 cycle_B: 0.829 idt_B: 0.549 \n",
            "End of epoch 126 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0001465 -> 0.0001446\n",
            "(epoch: 127, iters: 100, time: 0.634, data: 0.116) D_A: 0.279 G_A: 0.449 cycle_A: 0.489 idt_A: 0.275 D_B: 0.164 G_B: 0.246 cycle_B: 0.905 idt_B: 0.330 \n",
            "(epoch: 127, iters: 200, time: 1.812, data: 0.001) D_A: 0.119 G_A: 0.506 cycle_A: 0.792 idt_A: 0.336 D_B: 0.119 G_B: 0.502 cycle_B: 0.574 idt_B: 0.296 \n",
            "(epoch: 127, iters: 300, time: 0.634, data: 0.001) D_A: 0.264 G_A: 0.400 cycle_A: 0.447 idt_A: 0.219 D_B: 0.158 G_B: 0.276 cycle_B: 0.450 idt_B: 0.210 \n",
            "(epoch: 127, iters: 400, time: 0.634, data: 0.001) D_A: 0.296 G_A: 0.124 cycle_A: 0.596 idt_A: 0.442 D_B: 0.405 G_B: 0.414 cycle_B: 0.648 idt_B: 0.234 \n",
            "(epoch: 127, iters: 500, time: 0.638, data: 0.001) D_A: 0.118 G_A: 0.389 cycle_A: 0.520 idt_A: 0.218 D_B: 0.086 G_B: 0.966 cycle_B: 0.496 idt_B: 0.240 \n",
            "End of epoch 127 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0001446 -> 0.0001426\n",
            "(epoch: 128, iters: 100, time: 2.151, data: 0.108) D_A: 0.056 G_A: 0.689 cycle_A: 0.357 idt_A: 0.280 D_B: 0.226 G_B: 0.541 cycle_B: 0.677 idt_B: 0.186 \n",
            "(epoch: 128, iters: 200, time: 0.636, data: 0.001) D_A: 0.052 G_A: 0.475 cycle_A: 0.538 idt_A: 0.173 D_B: 0.081 G_B: 0.699 cycle_B: 0.467 idt_B: 0.273 \n",
            "(epoch: 128, iters: 300, time: 0.635, data: 0.001) D_A: 0.288 G_A: 0.023 cycle_A: 0.747 idt_A: 0.211 D_B: 0.561 G_B: 0.311 cycle_B: 0.664 idt_B: 0.370 \n",
            "(epoch: 128, iters: 400, time: 0.635, data: 0.001) D_A: 0.173 G_A: 0.210 cycle_A: 0.619 idt_A: 0.287 D_B: 0.110 G_B: 0.688 cycle_B: 0.630 idt_B: 0.405 \n",
            "(epoch: 128, iters: 500, time: 1.793, data: 0.001) D_A: 0.295 G_A: 0.580 cycle_A: 0.747 idt_A: 0.392 D_B: 0.261 G_B: 0.414 cycle_B: 0.615 idt_B: 0.392 \n",
            "End of epoch 128 / 200 \t Time Taken: 257 sec\n",
            "learning rate 0.0001426 -> 0.0001406\n",
            "(epoch: 129, iters: 100, time: 0.635, data: 0.107) D_A: 0.056 G_A: 0.326 cycle_A: 0.786 idt_A: 0.166 D_B: 0.057 G_B: 0.959 cycle_B: 0.427 idt_B: 0.299 \n",
            "(epoch: 129, iters: 200, time: 0.635, data: 0.001) D_A: 0.171 G_A: 0.769 cycle_A: 0.544 idt_A: 0.287 D_B: 0.127 G_B: 0.209 cycle_B: 0.542 idt_B: 0.351 \n",
            "(epoch: 129, iters: 300, time: 0.634, data: 0.001) D_A: 0.188 G_A: 0.179 cycle_A: 0.595 idt_A: 0.176 D_B: 0.220 G_B: 0.737 cycle_B: 0.530 idt_B: 0.336 \n",
            "(epoch: 129, iters: 400, time: 1.874, data: 0.001) D_A: 0.129 G_A: 0.567 cycle_A: 0.649 idt_A: 0.296 D_B: 0.039 G_B: 0.384 cycle_B: 0.488 idt_B: 0.366 \n",
            "(epoch: 129, iters: 500, time: 0.638, data: 0.001) D_A: 0.240 G_A: 0.250 cycle_A: 1.053 idt_A: 0.171 D_B: 0.076 G_B: 0.475 cycle_B: 0.486 idt_B: 0.412 \n",
            "End of epoch 129 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0001406 -> 0.0001386\n",
            "(epoch: 130, iters: 100, time: 0.635, data: 0.102) D_A: 0.227 G_A: 0.553 cycle_A: 0.386 idt_A: 0.304 D_B: 0.066 G_B: 0.822 cycle_B: 0.649 idt_B: 0.384 \n",
            "(epoch: 130, iters: 200, time: 0.635, data: 0.001) D_A: 0.204 G_A: 0.544 cycle_A: 0.405 idt_A: 0.230 D_B: 0.196 G_B: 0.390 cycle_B: 0.451 idt_B: 0.142 \n",
            "(epoch: 130, iters: 300, time: 1.813, data: 0.002) D_A: 0.124 G_A: 0.337 cycle_A: 0.440 idt_A: 0.345 D_B: 0.173 G_B: 0.648 cycle_B: 0.937 idt_B: 0.236 \n",
            "(epoch: 130, iters: 400, time: 0.636, data: 0.001) D_A: 0.350 G_A: 0.332 cycle_A: 0.549 idt_A: 0.147 D_B: 0.242 G_B: 0.167 cycle_B: 0.274 idt_B: 0.293 \n",
            "(epoch: 130, iters: 500, time: 0.636, data: 0.001) D_A: 0.112 G_A: 0.598 cycle_A: 0.657 idt_A: 0.383 D_B: 0.044 G_B: 0.331 cycle_B: 0.648 idt_B: 0.291 \n",
            "saving the latest model (epoch 130, total_iters 35000)\n",
            "saving the model at the end of epoch 130, iters 35000\n",
            "End of epoch 130 / 200 \t Time Taken: 257 sec\n",
            "learning rate 0.0001386 -> 0.0001366\n",
            "(epoch: 131, iters: 100, time: 0.632, data: 0.125) D_A: 0.243 G_A: 0.159 cycle_A: 0.518 idt_A: 0.382 D_B: 0.173 G_B: 0.334 cycle_B: 0.605 idt_B: 0.310 \n",
            "(epoch: 131, iters: 200, time: 2.132, data: 0.001) D_A: 0.234 G_A: 0.633 cycle_A: 0.445 idt_A: 0.275 D_B: 0.092 G_B: 0.231 cycle_B: 0.559 idt_B: 0.172 \n",
            "(epoch: 131, iters: 300, time: 0.637, data: 0.002) D_A: 0.047 G_A: 0.662 cycle_A: 0.419 idt_A: 0.265 D_B: 0.077 G_B: 0.524 cycle_B: 0.599 idt_B: 0.195 \n",
            "(epoch: 131, iters: 400, time: 0.635, data: 0.002) D_A: 0.048 G_A: 0.726 cycle_A: 0.580 idt_A: 0.223 D_B: 0.056 G_B: 0.687 cycle_B: 0.579 idt_B: 0.270 \n",
            "(epoch: 131, iters: 500, time: 0.637, data: 0.002) D_A: 0.080 G_A: 0.627 cycle_A: 0.851 idt_A: 0.353 D_B: 0.072 G_B: 0.580 cycle_B: 0.794 idt_B: 0.392 \n",
            "End of epoch 131 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0001366 -> 0.0001347\n",
            "(epoch: 132, iters: 100, time: 1.876, data: 0.103) D_A: 0.129 G_A: 0.516 cycle_A: 0.400 idt_A: 0.299 D_B: 0.383 G_B: 0.390 cycle_B: 0.497 idt_B: 0.220 \n",
            "(epoch: 132, iters: 200, time: 0.640, data: 0.001) D_A: 0.267 G_A: 0.354 cycle_A: 0.392 idt_A: 0.284 D_B: 0.195 G_B: 0.390 cycle_B: 0.551 idt_B: 0.214 \n",
            "(epoch: 132, iters: 300, time: 0.636, data: 0.001) D_A: 0.314 G_A: 0.134 cycle_A: 0.649 idt_A: 0.286 D_B: 0.233 G_B: 0.264 cycle_B: 0.480 idt_B: 0.363 \n",
            "(epoch: 132, iters: 400, time: 0.634, data: 0.001) D_A: 0.256 G_A: 0.591 cycle_A: 0.533 idt_A: 0.240 D_B: 0.076 G_B: 0.327 cycle_B: 0.494 idt_B: 0.255 \n",
            "(epoch: 132, iters: 500, time: 1.867, data: 0.001) D_A: 0.241 G_A: 0.439 cycle_A: 0.463 idt_A: 0.394 D_B: 0.137 G_B: 0.179 cycle_B: 0.787 idt_B: 0.246 \n",
            "End of epoch 132 / 200 \t Time Taken: 257 sec\n",
            "learning rate 0.0001347 -> 0.0001327\n",
            "(epoch: 133, iters: 100, time: 0.640, data: 0.105) D_A: 0.161 G_A: 0.437 cycle_A: 0.537 idt_A: 0.281 D_B: 0.145 G_B: 0.593 cycle_B: 0.600 idt_B: 0.251 \n",
            "(epoch: 133, iters: 200, time: 0.637, data: 0.002) D_A: 0.236 G_A: 0.468 cycle_A: 0.372 idt_A: 0.217 D_B: 0.289 G_B: 0.489 cycle_B: 0.347 idt_B: 0.152 \n",
            "(epoch: 133, iters: 300, time: 0.632, data: 0.002) D_A: 0.164 G_A: 0.436 cycle_A: 0.605 idt_A: 0.203 D_B: 0.057 G_B: 0.838 cycle_B: 0.426 idt_B: 0.249 \n",
            "(epoch: 133, iters: 400, time: 1.898, data: 0.002) D_A: 0.188 G_A: 0.404 cycle_A: 0.501 idt_A: 0.349 D_B: 0.147 G_B: 0.182 cycle_B: 0.870 idt_B: 0.225 \n",
            "(epoch: 133, iters: 500, time: 0.635, data: 0.001) D_A: 0.086 G_A: 0.614 cycle_A: 0.465 idt_A: 0.187 D_B: 0.063 G_B: 0.504 cycle_B: 0.457 idt_B: 0.208 \n",
            "End of epoch 133 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0001327 -> 0.0001307\n",
            "(epoch: 134, iters: 100, time: 0.637, data: 0.114) D_A: 0.110 G_A: 0.586 cycle_A: 0.623 idt_A: 0.390 D_B: 0.278 G_B: 0.217 cycle_B: 0.762 idt_B: 0.250 \n",
            "(epoch: 134, iters: 200, time: 0.639, data: 0.001) D_A: 0.214 G_A: 0.333 cycle_A: 0.452 idt_A: 0.381 D_B: 0.255 G_B: 0.183 cycle_B: 0.773 idt_B: 0.196 \n",
            "(epoch: 134, iters: 300, time: 2.000, data: 0.001) D_A: 0.111 G_A: 0.382 cycle_A: 0.715 idt_A: 0.476 D_B: 0.147 G_B: 0.948 cycle_B: 1.059 idt_B: 0.367 \n",
            "(epoch: 134, iters: 400, time: 0.634, data: 0.001) D_A: 0.103 G_A: 0.608 cycle_A: 0.414 idt_A: 0.274 D_B: 0.090 G_B: 0.570 cycle_B: 0.600 idt_B: 0.219 \n",
            "(epoch: 134, iters: 500, time: 0.637, data: 0.001) D_A: 0.122 G_A: 0.162 cycle_A: 0.452 idt_A: 0.237 D_B: 0.194 G_B: 0.734 cycle_B: 0.719 idt_B: 0.342 \n",
            "End of epoch 134 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0001307 -> 0.0001287\n",
            "(epoch: 135, iters: 100, time: 0.632, data: 0.111) D_A: 0.107 G_A: 0.687 cycle_A: 0.301 idt_A: 0.159 D_B: 0.155 G_B: 0.484 cycle_B: 0.604 idt_B: 0.141 \n",
            "(epoch: 135, iters: 200, time: 1.774, data: 0.001) D_A: 0.050 G_A: 0.458 cycle_A: 0.677 idt_A: 0.346 D_B: 0.209 G_B: 0.623 cycle_B: 0.710 idt_B: 0.322 \n",
            "(epoch: 135, iters: 300, time: 0.635, data: 0.002) D_A: 0.142 G_A: 0.564 cycle_A: 0.470 idt_A: 0.278 D_B: 0.236 G_B: 0.436 cycle_B: 0.638 idt_B: 0.229 \n",
            "(epoch: 135, iters: 400, time: 0.639, data: 0.002) D_A: 0.218 G_A: 0.528 cycle_A: 0.448 idt_A: 0.245 D_B: 0.241 G_B: 0.200 cycle_B: 0.490 idt_B: 0.260 \n",
            "(epoch: 135, iters: 500, time: 0.639, data: 0.002) D_A: 0.149 G_A: 0.375 cycle_A: 0.761 idt_A: 0.175 D_B: 0.138 G_B: 0.645 cycle_B: 0.401 idt_B: 0.347 \n",
            "saving the model at the end of epoch 135, iters 37500\n",
            "End of epoch 135 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0001287 -> 0.0001267\n",
            "(epoch: 136, iters: 100, time: 1.884, data: 0.115) D_A: 0.265 G_A: 0.481 cycle_A: 0.583 idt_A: 0.230 D_B: 0.271 G_B: 0.507 cycle_B: 0.744 idt_B: 0.305 \n",
            "(epoch: 136, iters: 200, time: 0.636, data: 0.002) D_A: 0.184 G_A: 0.498 cycle_A: 0.461 idt_A: 0.295 D_B: 0.191 G_B: 0.302 cycle_B: 0.725 idt_B: 0.251 \n",
            "(epoch: 136, iters: 300, time: 0.639, data: 0.001) D_A: 0.442 G_A: 0.507 cycle_A: 0.815 idt_A: 0.208 D_B: 0.132 G_B: 0.422 cycle_B: 0.476 idt_B: 0.339 \n",
            "(epoch: 136, iters: 400, time: 0.636, data: 0.001) D_A: 0.053 G_A: 0.302 cycle_A: 0.643 idt_A: 0.257 D_B: 0.092 G_B: 0.412 cycle_B: 0.740 idt_B: 0.250 \n",
            "(epoch: 136, iters: 500, time: 2.048, data: 0.001) D_A: 0.100 G_A: 0.385 cycle_A: 0.371 idt_A: 0.198 D_B: 0.265 G_B: 0.500 cycle_B: 0.536 idt_B: 0.178 \n",
            "End of epoch 136 / 200 \t Time Taken: 257 sec\n",
            "learning rate 0.0001267 -> 0.0001248\n",
            "(epoch: 137, iters: 100, time: 0.633, data: 0.104) D_A: 0.033 G_A: 0.390 cycle_A: 0.621 idt_A: 0.183 D_B: 0.235 G_B: 0.468 cycle_B: 0.416 idt_B: 0.302 \n",
            "(epoch: 137, iters: 200, time: 0.636, data: 0.001) D_A: 0.251 G_A: 0.361 cycle_A: 0.551 idt_A: 0.323 D_B: 0.173 G_B: 0.279 cycle_B: 0.449 idt_B: 0.273 \n",
            "(epoch: 137, iters: 300, time: 0.634, data: 0.002) D_A: 0.095 G_A: 0.543 cycle_A: 0.340 idt_A: 0.446 D_B: 0.060 G_B: 0.660 cycle_B: 0.931 idt_B: 0.175 \n",
            "(epoch: 137, iters: 400, time: 1.783, data: 0.001) D_A: 0.140 G_A: 0.274 cycle_A: 0.452 idt_A: 0.170 D_B: 0.102 G_B: 0.730 cycle_B: 0.481 idt_B: 0.275 \n",
            "(epoch: 137, iters: 500, time: 0.636, data: 0.001) D_A: 0.038 G_A: 0.618 cycle_A: 0.428 idt_A: 0.247 D_B: 0.092 G_B: 0.634 cycle_B: 0.325 idt_B: 0.266 \n",
            "End of epoch 137 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0001248 -> 0.0001228\n",
            "(epoch: 138, iters: 100, time: 0.637, data: 0.113) D_A: 0.125 G_A: 0.321 cycle_A: 0.420 idt_A: 0.246 D_B: 0.190 G_B: 0.600 cycle_B: 0.509 idt_B: 0.218 \n",
            "(epoch: 138, iters: 200, time: 0.637, data: 0.001) D_A: 0.151 G_A: 0.356 cycle_A: 0.872 idt_A: 0.226 D_B: 0.190 G_B: 0.445 cycle_B: 0.346 idt_B: 0.548 \n",
            "(epoch: 138, iters: 300, time: 1.873, data: 0.001) D_A: 0.271 G_A: 0.286 cycle_A: 0.360 idt_A: 0.278 D_B: 0.065 G_B: 0.295 cycle_B: 0.479 idt_B: 0.176 \n",
            "(epoch: 138, iters: 400, time: 0.636, data: 0.002) D_A: 0.201 G_A: 0.540 cycle_A: 0.732 idt_A: 0.307 D_B: 0.293 G_B: 0.301 cycle_B: 0.726 idt_B: 0.252 \n",
            "(epoch: 138, iters: 500, time: 0.634, data: 0.001) D_A: 0.295 G_A: 0.181 cycle_A: 0.745 idt_A: 0.216 D_B: 0.280 G_B: 0.363 cycle_B: 0.511 idt_B: 0.477 \n",
            "End of epoch 138 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0001228 -> 0.0001208\n",
            "(epoch: 139, iters: 100, time: 0.638, data: 0.107) D_A: 0.208 G_A: 0.751 cycle_A: 0.735 idt_A: 0.258 D_B: 0.197 G_B: 0.281 cycle_B: 0.539 idt_B: 0.370 \n",
            "(epoch: 139, iters: 200, time: 2.170, data: 0.002) D_A: 0.144 G_A: 0.276 cycle_A: 0.687 idt_A: 0.281 D_B: 0.124 G_B: 0.333 cycle_B: 0.525 idt_B: 0.323 \n",
            "(epoch: 139, iters: 300, time: 0.636, data: 0.001) D_A: 0.216 G_A: 0.290 cycle_A: 0.387 idt_A: 0.287 D_B: 0.073 G_B: 0.104 cycle_B: 0.637 idt_B: 0.214 \n",
            "(epoch: 139, iters: 400, time: 0.634, data: 0.001) D_A: 0.217 G_A: 0.550 cycle_A: 0.690 idt_A: 0.196 D_B: 0.172 G_B: 0.227 cycle_B: 0.446 idt_B: 0.326 \n",
            "(epoch: 139, iters: 500, time: 0.633, data: 0.002) D_A: 0.068 G_A: 0.268 cycle_A: 0.600 idt_A: 0.387 D_B: 0.274 G_B: 0.477 cycle_B: 0.729 idt_B: 0.335 \n",
            "End of epoch 139 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0001208 -> 0.0001188\n",
            "(epoch: 140, iters: 100, time: 1.749, data: 0.115) D_A: 0.040 G_A: 0.845 cycle_A: 0.635 idt_A: 0.181 D_B: 0.117 G_B: 0.438 cycle_B: 0.442 idt_B: 0.263 \n",
            "(epoch: 140, iters: 200, time: 0.636, data: 0.001) D_A: 0.267 G_A: 0.791 cycle_A: 0.342 idt_A: 0.255 D_B: 0.161 G_B: 0.332 cycle_B: 0.665 idt_B: 0.181 \n",
            "(epoch: 140, iters: 300, time: 0.638, data: 0.001) D_A: 0.108 G_A: 0.562 cycle_A: 0.432 idt_A: 0.272 D_B: 0.294 G_B: 0.331 cycle_B: 0.585 idt_B: 0.197 \n",
            "(epoch: 140, iters: 400, time: 0.638, data: 0.001) D_A: 0.220 G_A: 0.524 cycle_A: 0.790 idt_A: 0.309 D_B: 0.089 G_B: 0.603 cycle_B: 0.539 idt_B: 0.280 \n",
            "(epoch: 140, iters: 500, time: 1.828, data: 0.001) D_A: 0.107 G_A: 0.808 cycle_A: 0.837 idt_A: 0.339 D_B: 0.158 G_B: 0.701 cycle_B: 0.780 idt_B: 0.341 \n",
            "saving the latest model (epoch 140, total_iters 40000)\n",
            "saving the model at the end of epoch 140, iters 40000\n",
            "End of epoch 140 / 200 \t Time Taken: 258 sec\n",
            "learning rate 0.0001188 -> 0.0001168\n",
            "(epoch: 141, iters: 100, time: 0.635, data: 0.111) D_A: 0.144 G_A: 0.598 cycle_A: 0.590 idt_A: 0.398 D_B: 0.336 G_B: 0.262 cycle_B: 0.638 idt_B: 0.182 \n",
            "(epoch: 141, iters: 200, time: 0.638, data: 0.001) D_A: 0.121 G_A: 0.366 cycle_A: 0.524 idt_A: 0.231 D_B: 0.098 G_B: 0.870 cycle_B: 0.718 idt_B: 0.294 \n",
            "(epoch: 141, iters: 300, time: 0.637, data: 0.001) D_A: 0.178 G_A: 0.648 cycle_A: 0.593 idt_A: 0.264 D_B: 0.114 G_B: 0.295 cycle_B: 0.563 idt_B: 0.331 \n",
            "(epoch: 141, iters: 400, time: 1.869, data: 0.001) D_A: 0.137 G_A: 0.358 cycle_A: 0.558 idt_A: 0.455 D_B: 0.257 G_B: 0.164 cycle_B: 0.623 idt_B: 0.340 \n",
            "(epoch: 141, iters: 500, time: 0.636, data: 0.001) D_A: 0.091 G_A: 0.501 cycle_A: 0.530 idt_A: 0.427 D_B: 0.070 G_B: 0.403 cycle_B: 0.763 idt_B: 0.307 \n",
            "End of epoch 141 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0001168 -> 0.0001149\n",
            "(epoch: 142, iters: 100, time: 0.635, data: 0.107) D_A: 0.273 G_A: 0.627 cycle_A: 0.639 idt_A: 0.211 D_B: 0.067 G_B: 0.274 cycle_B: 0.559 idt_B: 0.273 \n",
            "(epoch: 142, iters: 200, time: 0.636, data: 0.001) D_A: 0.142 G_A: 0.369 cycle_A: 0.457 idt_A: 0.286 D_B: 0.164 G_B: 0.736 cycle_B: 0.403 idt_B: 0.192 \n",
            "(epoch: 142, iters: 300, time: 2.185, data: 0.001) D_A: 0.070 G_A: 0.633 cycle_A: 0.353 idt_A: 0.263 D_B: 0.145 G_B: 0.924 cycle_B: 0.508 idt_B: 0.177 \n",
            "(epoch: 142, iters: 400, time: 0.637, data: 0.001) D_A: 0.226 G_A: 0.212 cycle_A: 0.531 idt_A: 0.258 D_B: 0.227 G_B: 0.587 cycle_B: 0.535 idt_B: 0.264 \n",
            "(epoch: 142, iters: 500, time: 0.637, data: 0.002) D_A: 0.130 G_A: 0.537 cycle_A: 0.594 idt_A: 0.351 D_B: 0.136 G_B: 0.423 cycle_B: 0.686 idt_B: 0.313 \n",
            "End of epoch 142 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0001149 -> 0.0001129\n",
            "(epoch: 143, iters: 100, time: 0.637, data: 0.114) D_A: 0.278 G_A: 0.892 cycle_A: 0.499 idt_A: 0.244 D_B: 0.266 G_B: 0.179 cycle_B: 0.483 idt_B: 0.260 \n",
            "(epoch: 143, iters: 200, time: 1.948, data: 0.001) D_A: 0.094 G_A: 0.808 cycle_A: 0.457 idt_A: 0.172 D_B: 0.042 G_B: 0.393 cycle_B: 0.394 idt_B: 0.217 \n",
            "(epoch: 143, iters: 300, time: 0.634, data: 0.002) D_A: 0.141 G_A: 0.453 cycle_A: 0.830 idt_A: 0.241 D_B: 0.173 G_B: 0.319 cycle_B: 0.545 idt_B: 0.414 \n",
            "(epoch: 143, iters: 400, time: 0.636, data: 0.001) D_A: 0.083 G_A: 0.178 cycle_A: 0.507 idt_A: 0.407 D_B: 0.185 G_B: 0.281 cycle_B: 0.770 idt_B: 0.261 \n",
            "(epoch: 143, iters: 500, time: 0.638, data: 0.001) D_A: 0.171 G_A: 0.580 cycle_A: 0.578 idt_A: 0.277 D_B: 0.048 G_B: 0.295 cycle_B: 0.465 idt_B: 0.245 \n",
            "End of epoch 143 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0001129 -> 0.0001109\n",
            "(epoch: 144, iters: 100, time: 1.884, data: 0.101) D_A: 0.105 G_A: 0.411 cycle_A: 0.696 idt_A: 0.321 D_B: 0.077 G_B: 0.472 cycle_B: 0.649 idt_B: 0.373 \n",
            "(epoch: 144, iters: 200, time: 0.639, data: 0.002) D_A: 0.070 G_A: 0.471 cycle_A: 0.683 idt_A: 0.163 D_B: 0.091 G_B: 0.297 cycle_B: 0.335 idt_B: 0.291 \n",
            "(epoch: 144, iters: 300, time: 0.635, data: 0.002) D_A: 0.123 G_A: 0.357 cycle_A: 0.617 idt_A: 0.291 D_B: 0.362 G_B: 0.273 cycle_B: 0.595 idt_B: 0.387 \n",
            "(epoch: 144, iters: 400, time: 0.637, data: 0.001) D_A: 0.126 G_A: 0.875 cycle_A: 0.511 idt_A: 0.441 D_B: 0.129 G_B: 0.814 cycle_B: 0.717 idt_B: 0.307 \n",
            "(epoch: 144, iters: 500, time: 2.130, data: 0.002) D_A: 0.218 G_A: 0.266 cycle_A: 0.942 idt_A: 0.317 D_B: 0.124 G_B: 0.429 cycle_B: 0.725 idt_B: 0.339 \n",
            "End of epoch 144 / 200 \t Time Taken: 257 sec\n",
            "learning rate 0.0001109 -> 0.0001089\n",
            "(epoch: 145, iters: 100, time: 0.634, data: 0.114) D_A: 0.252 G_A: 0.443 cycle_A: 0.484 idt_A: 0.289 D_B: 0.309 G_B: 0.130 cycle_B: 0.489 idt_B: 0.282 \n",
            "(epoch: 145, iters: 200, time: 0.633, data: 0.001) D_A: 0.154 G_A: 0.447 cycle_A: 0.635 idt_A: 0.230 D_B: 0.224 G_B: 0.109 cycle_B: 0.695 idt_B: 0.359 \n",
            "(epoch: 145, iters: 300, time: 0.637, data: 0.001) D_A: 0.198 G_A: 0.357 cycle_A: 0.409 idt_A: 0.487 D_B: 0.298 G_B: 0.385 cycle_B: 0.784 idt_B: 0.298 \n",
            "(epoch: 145, iters: 400, time: 1.802, data: 0.001) D_A: 0.163 G_A: 0.341 cycle_A: 0.665 idt_A: 0.261 D_B: 0.313 G_B: 0.521 cycle_B: 0.588 idt_B: 0.329 \n",
            "(epoch: 145, iters: 500, time: 0.634, data: 0.002) D_A: 0.166 G_A: 0.513 cycle_A: 0.712 idt_A: 0.256 D_B: 0.103 G_B: 0.407 cycle_B: 0.652 idt_B: 0.366 \n",
            "saving the model at the end of epoch 145, iters 42500\n",
            "End of epoch 145 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0001089 -> 0.0001069\n",
            "(epoch: 146, iters: 100, time: 0.634, data: 0.106) D_A: 0.275 G_A: 0.215 cycle_A: 0.426 idt_A: 0.366 D_B: 0.220 G_B: 0.276 cycle_B: 0.656 idt_B: 0.288 \n",
            "(epoch: 146, iters: 200, time: 0.636, data: 0.001) D_A: 0.293 G_A: 0.348 cycle_A: 0.578 idt_A: 0.229 D_B: 0.245 G_B: 0.137 cycle_B: 0.467 idt_B: 0.349 \n",
            "(epoch: 146, iters: 300, time: 1.946, data: 0.001) D_A: 0.132 G_A: 0.407 cycle_A: 0.459 idt_A: 0.223 D_B: 0.196 G_B: 0.362 cycle_B: 0.454 idt_B: 0.264 \n",
            "(epoch: 146, iters: 400, time: 0.637, data: 0.001) D_A: 0.073 G_A: 0.463 cycle_A: 0.361 idt_A: 0.298 D_B: 0.117 G_B: 0.445 cycle_B: 0.707 idt_B: 0.199 \n",
            "(epoch: 146, iters: 500, time: 0.636, data: 0.001) D_A: 0.143 G_A: 0.395 cycle_A: 0.232 idt_A: 0.361 D_B: 0.181 G_B: 0.401 cycle_B: 0.783 idt_B: 0.118 \n",
            "End of epoch 146 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0001069 -> 0.0001050\n",
            "(epoch: 147, iters: 100, time: 0.632, data: 0.101) D_A: 0.160 G_A: 0.453 cycle_A: 0.629 idt_A: 0.307 D_B: 0.161 G_B: 0.293 cycle_B: 0.655 idt_B: 0.422 \n",
            "(epoch: 147, iters: 200, time: 2.133, data: 0.002) D_A: 0.146 G_A: 0.585 cycle_A: 0.446 idt_A: 0.326 D_B: 0.244 G_B: 0.322 cycle_B: 0.578 idt_B: 0.215 \n",
            "(epoch: 147, iters: 300, time: 0.636, data: 0.001) D_A: 0.073 G_A: 0.326 cycle_A: 0.611 idt_A: 0.294 D_B: 0.122 G_B: 0.453 cycle_B: 0.658 idt_B: 0.241 \n",
            "(epoch: 147, iters: 400, time: 0.636, data: 0.001) D_A: 0.283 G_A: 0.441 cycle_A: 0.522 idt_A: 0.268 D_B: 0.175 G_B: 0.492 cycle_B: 0.514 idt_B: 0.267 \n",
            "(epoch: 147, iters: 500, time: 0.637, data: 0.001) D_A: 0.079 G_A: 0.620 cycle_A: 0.499 idt_A: 0.304 D_B: 0.115 G_B: 0.565 cycle_B: 0.519 idt_B: 0.266 \n",
            "End of epoch 147 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0001050 -> 0.0001030\n",
            "(epoch: 148, iters: 100, time: 1.963, data: 0.117) D_A: 0.139 G_A: 0.387 cycle_A: 0.393 idt_A: 0.263 D_B: 0.175 G_B: 0.316 cycle_B: 0.530 idt_B: 0.232 \n",
            "(epoch: 148, iters: 200, time: 0.636, data: 0.001) D_A: 0.079 G_A: 0.973 cycle_A: 0.821 idt_A: 0.357 D_B: 0.245 G_B: 0.691 cycle_B: 0.842 idt_B: 0.290 \n",
            "(epoch: 148, iters: 300, time: 0.636, data: 0.001) D_A: 0.156 G_A: 0.424 cycle_A: 0.409 idt_A: 0.194 D_B: 0.141 G_B: 0.432 cycle_B: 0.325 idt_B: 0.208 \n",
            "(epoch: 148, iters: 400, time: 0.634, data: 0.001) D_A: 0.348 G_A: 0.491 cycle_A: 0.414 idt_A: 0.239 D_B: 0.162 G_B: 0.127 cycle_B: 0.414 idt_B: 0.269 \n",
            "(epoch: 148, iters: 500, time: 1.851, data: 0.001) D_A: 0.168 G_A: 0.588 cycle_A: 0.759 idt_A: 0.377 D_B: 0.264 G_B: 0.271 cycle_B: 0.739 idt_B: 0.354 \n",
            "End of epoch 148 / 200 \t Time Taken: 257 sec\n",
            "learning rate 0.0001030 -> 0.0001010\n",
            "(epoch: 149, iters: 100, time: 0.637, data: 0.108) D_A: 0.096 G_A: 0.793 cycle_A: 0.727 idt_A: 0.189 D_B: 0.209 G_B: 0.210 cycle_B: 0.411 idt_B: 0.665 \n",
            "(epoch: 149, iters: 200, time: 0.632, data: 0.002) D_A: 0.142 G_A: 0.136 cycle_A: 0.533 idt_A: 0.238 D_B: 0.338 G_B: 0.633 cycle_B: 0.513 idt_B: 0.331 \n",
            "(epoch: 149, iters: 300, time: 0.633, data: 0.001) D_A: 0.211 G_A: 0.478 cycle_A: 0.510 idt_A: 0.239 D_B: 0.171 G_B: 0.175 cycle_B: 0.507 idt_B: 0.242 \n",
            "(epoch: 149, iters: 400, time: 2.125, data: 0.001) D_A: 0.331 G_A: 0.269 cycle_A: 0.483 idt_A: 0.299 D_B: 0.272 G_B: 0.126 cycle_B: 0.654 idt_B: 0.200 \n",
            "(epoch: 149, iters: 500, time: 0.634, data: 0.001) D_A: 0.074 G_A: 0.575 cycle_A: 0.344 idt_A: 0.183 D_B: 0.102 G_B: 0.836 cycle_B: 0.398 idt_B: 0.357 \n",
            "End of epoch 149 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0001010 -> 0.0000990\n",
            "(epoch: 150, iters: 100, time: 0.640, data: 0.111) D_A: 0.265 G_A: 0.267 cycle_A: 0.598 idt_A: 0.277 D_B: 0.115 G_B: 0.647 cycle_B: 0.726 idt_B: 0.297 \n",
            "(epoch: 150, iters: 200, time: 0.633, data: 0.001) D_A: 0.104 G_A: 0.939 cycle_A: 0.682 idt_A: 0.193 D_B: 0.146 G_B: 0.459 cycle_B: 0.403 idt_B: 0.294 \n",
            "(epoch: 150, iters: 300, time: 1.890, data: 0.002) D_A: 0.415 G_A: 1.000 cycle_A: 0.893 idt_A: 0.218 D_B: 0.120 G_B: 0.129 cycle_B: 0.391 idt_B: 0.326 \n",
            "(epoch: 150, iters: 400, time: 0.634, data: 0.001) D_A: 0.222 G_A: 0.252 cycle_A: 0.559 idt_A: 0.235 D_B: 0.194 G_B: 0.527 cycle_B: 0.507 idt_B: 0.296 \n",
            "(epoch: 150, iters: 500, time: 0.638, data: 0.001) D_A: 0.181 G_A: 0.285 cycle_A: 0.559 idt_A: 0.143 D_B: 0.315 G_B: 0.559 cycle_B: 0.289 idt_B: 0.265 \n",
            "saving the latest model (epoch 150, total_iters 45000)\n",
            "saving the model at the end of epoch 150, iters 45000\n",
            "End of epoch 150 / 200 \t Time Taken: 257 sec\n",
            "learning rate 0.0000990 -> 0.0000970\n",
            "(epoch: 151, iters: 100, time: 0.634, data: 0.116) D_A: 0.192 G_A: 0.459 cycle_A: 0.387 idt_A: 0.223 D_B: 0.104 G_B: 0.623 cycle_B: 0.474 idt_B: 0.296 \n",
            "(epoch: 151, iters: 200, time: 2.043, data: 0.001) D_A: 0.102 G_A: 0.519 cycle_A: 0.542 idt_A: 0.257 D_B: 0.139 G_B: 0.507 cycle_B: 0.447 idt_B: 0.277 \n",
            "(epoch: 151, iters: 300, time: 0.636, data: 0.001) D_A: 0.168 G_A: 0.341 cycle_A: 0.461 idt_A: 0.364 D_B: 0.142 G_B: 0.320 cycle_B: 0.703 idt_B: 0.238 \n",
            "(epoch: 151, iters: 400, time: 0.633, data: 0.001) D_A: 0.151 G_A: 0.843 cycle_A: 0.416 idt_A: 0.269 D_B: 0.327 G_B: 0.222 cycle_B: 0.467 idt_B: 0.259 \n",
            "(epoch: 151, iters: 500, time: 0.636, data: 0.001) D_A: 0.159 G_A: 0.642 cycle_A: 1.041 idt_A: 0.303 D_B: 0.190 G_B: 0.419 cycle_B: 0.498 idt_B: 0.368 \n",
            "End of epoch 151 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0000970 -> 0.0000950\n",
            "(epoch: 152, iters: 100, time: 1.880, data: 0.110) D_A: 0.253 G_A: 0.151 cycle_A: 0.544 idt_A: 0.285 D_B: 0.322 G_B: 0.880 cycle_B: 0.586 idt_B: 0.304 \n",
            "(epoch: 152, iters: 200, time: 0.635, data: 0.001) D_A: 0.074 G_A: 0.540 cycle_A: 0.372 idt_A: 0.238 D_B: 0.131 G_B: 0.499 cycle_B: 0.452 idt_B: 0.211 \n",
            "(epoch: 152, iters: 300, time: 0.636, data: 0.001) D_A: 0.171 G_A: 0.504 cycle_A: 0.641 idt_A: 0.250 D_B: 0.207 G_B: 0.307 cycle_B: 0.557 idt_B: 0.251 \n",
            "(epoch: 152, iters: 400, time: 0.635, data: 0.002) D_A: 0.151 G_A: 0.134 cycle_A: 0.491 idt_A: 0.315 D_B: 0.284 G_B: 0.644 cycle_B: 0.760 idt_B: 0.277 \n",
            "(epoch: 152, iters: 500, time: 1.852, data: 0.001) D_A: 0.270 G_A: 0.142 cycle_A: 1.023 idt_A: 0.312 D_B: 0.200 G_B: 0.358 cycle_B: 0.843 idt_B: 0.305 \n",
            "End of epoch 152 / 200 \t Time Taken: 257 sec\n",
            "learning rate 0.0000950 -> 0.0000931\n",
            "(epoch: 153, iters: 100, time: 0.637, data: 0.115) D_A: 0.237 G_A: 0.384 cycle_A: 0.411 idt_A: 0.284 D_B: 0.147 G_B: 0.326 cycle_B: 0.450 idt_B: 0.214 \n",
            "(epoch: 153, iters: 200, time: 0.638, data: 0.002) D_A: 0.274 G_A: 0.434 cycle_A: 0.575 idt_A: 0.447 D_B: 0.102 G_B: 0.132 cycle_B: 0.914 idt_B: 0.268 \n",
            "(epoch: 153, iters: 300, time: 0.631, data: 0.001) D_A: 0.108 G_A: 0.317 cycle_A: 0.567 idt_A: 0.418 D_B: 0.363 G_B: 0.413 cycle_B: 0.327 idt_B: 0.267 \n",
            "(epoch: 153, iters: 400, time: 1.844, data: 0.001) D_A: 0.394 G_A: 0.089 cycle_A: 0.677 idt_A: 0.239 D_B: 0.149 G_B: 0.285 cycle_B: 0.445 idt_B: 0.381 \n",
            "(epoch: 153, iters: 500, time: 0.636, data: 0.002) D_A: 0.130 G_A: 0.343 cycle_A: 0.559 idt_A: 0.217 D_B: 0.168 G_B: 0.707 cycle_B: 0.483 idt_B: 0.325 \n",
            "End of epoch 153 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0000931 -> 0.0000911\n",
            "(epoch: 154, iters: 100, time: 0.634, data: 0.109) D_A: 0.063 G_A: 0.524 cycle_A: 0.596 idt_A: 0.145 D_B: 0.036 G_B: 1.023 cycle_B: 0.370 idt_B: 0.273 \n",
            "(epoch: 154, iters: 200, time: 0.636, data: 0.002) D_A: 0.112 G_A: 0.316 cycle_A: 0.910 idt_A: 0.313 D_B: 0.144 G_B: 0.437 cycle_B: 0.695 idt_B: 0.274 \n",
            "(epoch: 154, iters: 300, time: 1.964, data: 0.001) D_A: 0.220 G_A: 0.753 cycle_A: 0.383 idt_A: 0.224 D_B: 0.134 G_B: 0.569 cycle_B: 0.514 idt_B: 0.165 \n",
            "(epoch: 154, iters: 400, time: 0.632, data: 0.002) D_A: 0.225 G_A: 0.438 cycle_A: 0.405 idt_A: 0.168 D_B: 0.171 G_B: 0.346 cycle_B: 0.328 idt_B: 0.203 \n",
            "(epoch: 154, iters: 500, time: 0.634, data: 0.002) D_A: 0.227 G_A: 0.501 cycle_A: 0.423 idt_A: 0.207 D_B: 0.099 G_B: 0.210 cycle_B: 0.445 idt_B: 0.281 \n",
            "End of epoch 154 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0000911 -> 0.0000891\n",
            "(epoch: 155, iters: 100, time: 0.639, data: 0.110) D_A: 0.160 G_A: 0.769 cycle_A: 0.638 idt_A: 0.227 D_B: 0.126 G_B: 0.597 cycle_B: 0.480 idt_B: 0.371 \n",
            "(epoch: 155, iters: 200, time: 1.877, data: 0.002) D_A: 0.089 G_A: 0.800 cycle_A: 0.805 idt_A: 0.332 D_B: 0.142 G_B: 0.567 cycle_B: 0.694 idt_B: 0.286 \n",
            "(epoch: 155, iters: 300, time: 0.638, data: 0.001) D_A: 0.066 G_A: 0.257 cycle_A: 0.670 idt_A: 0.244 D_B: 0.271 G_B: 0.328 cycle_B: 0.512 idt_B: 0.323 \n",
            "(epoch: 155, iters: 400, time: 0.635, data: 0.001) D_A: 0.071 G_A: 0.526 cycle_A: 0.449 idt_A: 0.254 D_B: 0.175 G_B: 0.744 cycle_B: 0.607 idt_B: 0.231 \n",
            "(epoch: 155, iters: 500, time: 0.636, data: 0.001) D_A: 0.096 G_A: 0.287 cycle_A: 0.519 idt_A: 0.266 D_B: 0.157 G_B: 1.130 cycle_B: 0.648 idt_B: 0.306 \n",
            "saving the model at the end of epoch 155, iters 47500\n",
            "End of epoch 155 / 200 \t Time Taken: 257 sec\n",
            "learning rate 0.0000891 -> 0.0000871\n",
            "(epoch: 156, iters: 100, time: 2.073, data: 0.113) D_A: 0.355 G_A: 0.084 cycle_A: 0.411 idt_A: 0.316 D_B: 0.377 G_B: 1.040 cycle_B: 0.480 idt_B: 0.215 \n",
            "(epoch: 156, iters: 200, time: 0.638, data: 0.002) D_A: 0.385 G_A: 0.268 cycle_A: 0.434 idt_A: 0.227 D_B: 0.398 G_B: 0.308 cycle_B: 0.384 idt_B: 0.244 \n",
            "(epoch: 156, iters: 300, time: 0.633, data: 0.001) D_A: 0.197 G_A: 0.465 cycle_A: 0.542 idt_A: 0.463 D_B: 0.272 G_B: 0.270 cycle_B: 0.926 idt_B: 0.236 \n",
            "(epoch: 156, iters: 400, time: 0.636, data: 0.002) D_A: 0.090 G_A: 0.343 cycle_A: 0.471 idt_A: 0.180 D_B: 0.153 G_B: 0.372 cycle_B: 0.440 idt_B: 0.257 \n",
            "(epoch: 156, iters: 500, time: 2.218, data: 0.002) D_A: 0.130 G_A: 0.359 cycle_A: 0.281 idt_A: 0.403 D_B: 0.214 G_B: 0.380 cycle_B: 0.842 idt_B: 0.149 \n",
            "End of epoch 156 / 200 \t Time Taken: 257 sec\n",
            "learning rate 0.0000871 -> 0.0000851\n",
            "(epoch: 157, iters: 100, time: 0.635, data: 0.111) D_A: 0.190 G_A: 0.413 cycle_A: 0.678 idt_A: 0.234 D_B: 0.100 G_B: 0.391 cycle_B: 0.419 idt_B: 0.350 \n",
            "(epoch: 157, iters: 200, time: 0.639, data: 0.002) D_A: 0.179 G_A: 0.462 cycle_A: 0.736 idt_A: 0.272 D_B: 0.159 G_B: 0.306 cycle_B: 0.494 idt_B: 0.318 \n",
            "(epoch: 157, iters: 300, time: 0.633, data: 0.002) D_A: 0.096 G_A: 0.369 cycle_A: 0.405 idt_A: 0.212 D_B: 0.095 G_B: 0.526 cycle_B: 0.443 idt_B: 0.195 \n",
            "(epoch: 157, iters: 400, time: 1.965, data: 0.001) D_A: 0.132 G_A: 0.597 cycle_A: 0.533 idt_A: 0.214 D_B: 0.197 G_B: 0.526 cycle_B: 0.443 idt_B: 0.301 \n",
            "(epoch: 157, iters: 500, time: 0.638, data: 0.001) D_A: 0.338 G_A: 0.443 cycle_A: 0.593 idt_A: 0.231 D_B: 0.329 G_B: 0.128 cycle_B: 0.415 idt_B: 0.335 \n",
            "End of epoch 157 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0000851 -> 0.0000832\n",
            "(epoch: 158, iters: 100, time: 0.634, data: 0.112) D_A: 0.206 G_A: 0.160 cycle_A: 0.626 idt_A: 0.279 D_B: 0.207 G_B: 0.469 cycle_B: 0.552 idt_B: 0.321 \n",
            "(epoch: 158, iters: 200, time: 0.636, data: 0.001) D_A: 0.139 G_A: 0.276 cycle_A: 0.603 idt_A: 0.308 D_B: 0.067 G_B: 0.573 cycle_B: 0.571 idt_B: 0.341 \n",
            "(epoch: 158, iters: 300, time: 2.027, data: 0.001) D_A: 0.348 G_A: 0.265 cycle_A: 0.546 idt_A: 0.273 D_B: 0.163 G_B: 0.269 cycle_B: 0.503 idt_B: 0.204 \n",
            "(epoch: 158, iters: 400, time: 0.635, data: 0.001) D_A: 0.070 G_A: 0.550 cycle_A: 0.655 idt_A: 0.324 D_B: 0.108 G_B: 0.414 cycle_B: 0.569 idt_B: 0.351 \n",
            "(epoch: 158, iters: 500, time: 0.637, data: 0.001) D_A: 0.150 G_A: 0.438 cycle_A: 0.412 idt_A: 0.183 D_B: 0.191 G_B: 0.819 cycle_B: 0.344 idt_B: 0.207 \n",
            "End of epoch 158 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0000832 -> 0.0000812\n",
            "(epoch: 159, iters: 100, time: 0.635, data: 0.108) D_A: 0.144 G_A: 0.545 cycle_A: 0.368 idt_A: 0.221 D_B: 0.187 G_B: 0.247 cycle_B: 0.508 idt_B: 0.161 \n",
            "(epoch: 159, iters: 200, time: 2.024, data: 0.001) D_A: 0.101 G_A: 0.698 cycle_A: 0.616 idt_A: 0.302 D_B: 0.076 G_B: 0.833 cycle_B: 0.560 idt_B: 0.274 \n",
            "(epoch: 159, iters: 300, time: 0.635, data: 0.001) D_A: 0.184 G_A: 0.458 cycle_A: 0.463 idt_A: 0.271 D_B: 0.135 G_B: 0.502 cycle_B: 0.485 idt_B: 0.220 \n",
            "(epoch: 159, iters: 400, time: 0.638, data: 0.001) D_A: 0.310 G_A: 0.480 cycle_A: 0.753 idt_A: 0.175 D_B: 0.387 G_B: 0.082 cycle_B: 0.302 idt_B: 0.334 \n",
            "(epoch: 159, iters: 500, time: 0.638, data: 0.001) D_A: 0.123 G_A: 0.681 cycle_A: 0.674 idt_A: 0.239 D_B: 0.142 G_B: 0.399 cycle_B: 0.423 idt_B: 0.305 \n",
            "End of epoch 159 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0000812 -> 0.0000792\n",
            "(epoch: 160, iters: 100, time: 2.088, data: 0.108) D_A: 0.112 G_A: 0.675 cycle_A: 0.572 idt_A: 0.171 D_B: 0.105 G_B: 0.492 cycle_B: 0.386 idt_B: 0.174 \n",
            "(epoch: 160, iters: 200, time: 0.632, data: 0.001) D_A: 0.143 G_A: 0.451 cycle_A: 0.380 idt_A: 0.233 D_B: 0.197 G_B: 0.791 cycle_B: 0.397 idt_B: 0.269 \n",
            "(epoch: 160, iters: 300, time: 0.637, data: 0.001) D_A: 0.323 G_A: 0.143 cycle_A: 0.552 idt_A: 0.215 D_B: 0.273 G_B: 0.209 cycle_B: 0.491 idt_B: 0.281 \n",
            "(epoch: 160, iters: 400, time: 0.634, data: 0.001) D_A: 0.259 G_A: 0.368 cycle_A: 0.372 idt_A: 0.181 D_B: 0.421 G_B: 0.066 cycle_B: 0.361 idt_B: 0.208 \n",
            "(epoch: 160, iters: 500, time: 1.987, data: 0.001) D_A: 0.137 G_A: 0.647 cycle_A: 0.288 idt_A: 0.252 D_B: 0.215 G_B: 0.370 cycle_B: 0.582 idt_B: 0.173 \n",
            "saving the latest model (epoch 160, total_iters 50000)\n",
            "saving the model at the end of epoch 160, iters 50000\n",
            "End of epoch 160 / 200 \t Time Taken: 259 sec\n",
            "learning rate 0.0000792 -> 0.0000772\n",
            "(epoch: 161, iters: 100, time: 0.633, data: 0.126) D_A: 0.131 G_A: 0.413 cycle_A: 0.548 idt_A: 0.314 D_B: 0.169 G_B: 0.833 cycle_B: 0.547 idt_B: 0.272 \n",
            "(epoch: 161, iters: 200, time: 0.636, data: 0.001) D_A: 0.294 G_A: 0.637 cycle_A: 0.734 idt_A: 0.290 D_B: 0.193 G_B: 0.431 cycle_B: 0.678 idt_B: 0.336 \n",
            "(epoch: 161, iters: 300, time: 0.634, data: 0.001) D_A: 0.080 G_A: 0.589 cycle_A: 0.591 idt_A: 0.364 D_B: 0.190 G_B: 0.322 cycle_B: 0.707 idt_B: 0.275 \n",
            "(epoch: 161, iters: 400, time: 2.200, data: 0.001) D_A: 0.179 G_A: 0.411 cycle_A: 0.359 idt_A: 0.255 D_B: 0.142 G_B: 0.432 cycle_B: 0.495 idt_B: 0.199 \n",
            "(epoch: 161, iters: 500, time: 0.635, data: 0.001) D_A: 0.260 G_A: 0.439 cycle_A: 0.571 idt_A: 0.270 D_B: 0.287 G_B: 0.108 cycle_B: 0.585 idt_B: 0.184 \n",
            "End of epoch 161 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0000772 -> 0.0000752\n",
            "(epoch: 162, iters: 100, time: 0.636, data: 0.105) D_A: 0.180 G_A: 0.359 cycle_A: 0.555 idt_A: 0.315 D_B: 0.369 G_B: 0.312 cycle_B: 0.664 idt_B: 0.317 \n",
            "(epoch: 162, iters: 200, time: 0.636, data: 0.001) D_A: 0.144 G_A: 0.319 cycle_A: 0.636 idt_A: 0.232 D_B: 0.310 G_B: 0.999 cycle_B: 0.572 idt_B: 0.421 \n",
            "(epoch: 162, iters: 300, time: 1.933, data: 0.001) D_A: 0.110 G_A: 0.388 cycle_A: 0.433 idt_A: 0.255 D_B: 0.091 G_B: 0.659 cycle_B: 0.713 idt_B: 0.225 \n",
            "(epoch: 162, iters: 400, time: 0.634, data: 0.001) D_A: 0.171 G_A: 0.578 cycle_A: 0.676 idt_A: 0.251 D_B: 0.089 G_B: 0.615 cycle_B: 0.492 idt_B: 0.317 \n",
            "(epoch: 162, iters: 500, time: 0.636, data: 0.001) D_A: 0.054 G_A: 0.650 cycle_A: 0.318 idt_A: 0.171 D_B: 0.064 G_B: 0.876 cycle_B: 0.458 idt_B: 0.195 \n",
            "End of epoch 162 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0000752 -> 0.0000733\n",
            "(epoch: 163, iters: 100, time: 0.638, data: 0.105) D_A: 0.176 G_A: 0.847 cycle_A: 0.469 idt_A: 0.246 D_B: 0.099 G_B: 0.675 cycle_B: 0.470 idt_B: 0.282 \n",
            "(epoch: 163, iters: 200, time: 1.865, data: 0.001) D_A: 0.140 G_A: 0.436 cycle_A: 0.750 idt_A: 0.386 D_B: 0.240 G_B: 0.223 cycle_B: 1.066 idt_B: 0.386 \n",
            "(epoch: 163, iters: 300, time: 0.634, data: 0.001) D_A: 0.232 G_A: 0.523 cycle_A: 0.601 idt_A: 0.200 D_B: 0.237 G_B: 0.431 cycle_B: 0.349 idt_B: 0.286 \n",
            "(epoch: 163, iters: 400, time: 0.636, data: 0.001) D_A: 0.111 G_A: 0.305 cycle_A: 0.613 idt_A: 0.182 D_B: 0.129 G_B: 1.206 cycle_B: 0.435 idt_B: 0.312 \n",
            "(epoch: 163, iters: 500, time: 0.636, data: 0.001) D_A: 0.188 G_A: 0.626 cycle_A: 0.273 idt_A: 0.402 D_B: 0.156 G_B: 0.370 cycle_B: 0.805 idt_B: 0.171 \n",
            "End of epoch 163 / 200 \t Time Taken: 255 sec\n",
            "learning rate 0.0000733 -> 0.0000713\n",
            "(epoch: 164, iters: 100, time: 2.184, data: 0.106) D_A: 0.239 G_A: 0.641 cycle_A: 0.475 idt_A: 0.198 D_B: 0.242 G_B: 0.197 cycle_B: 0.368 idt_B: 0.231 \n",
            "(epoch: 164, iters: 200, time: 0.634, data: 0.001) D_A: 0.110 G_A: 0.650 cycle_A: 0.381 idt_A: 0.245 D_B: 0.262 G_B: 0.757 cycle_B: 0.487 idt_B: 0.203 \n",
            "(epoch: 164, iters: 300, time: 0.635, data: 0.001) D_A: 0.188 G_A: 0.236 cycle_A: 0.542 idt_A: 0.338 D_B: 0.275 G_B: 0.394 cycle_B: 0.791 idt_B: 0.314 \n",
            "(epoch: 164, iters: 400, time: 0.634, data: 0.001) D_A: 0.181 G_A: 0.301 cycle_A: 0.518 idt_A: 0.398 D_B: 0.329 G_B: 0.701 cycle_B: 0.726 idt_B: 0.330 \n",
            "(epoch: 164, iters: 500, time: 1.941, data: 0.001) D_A: 0.182 G_A: 0.248 cycle_A: 0.380 idt_A: 0.207 D_B: 0.211 G_B: 1.047 cycle_B: 0.453 idt_B: 0.243 \n",
            "End of epoch 164 / 200 \t Time Taken: 257 sec\n",
            "learning rate 0.0000713 -> 0.0000693\n",
            "(epoch: 165, iters: 100, time: 0.637, data: 0.108) D_A: 0.079 G_A: 0.569 cycle_A: 0.649 idt_A: 0.453 D_B: 0.223 G_B: 0.471 cycle_B: 0.723 idt_B: 0.365 \n",
            "(epoch: 165, iters: 200, time: 0.632, data: 0.001) D_A: 0.163 G_A: 0.323 cycle_A: 0.477 idt_A: 0.333 D_B: 0.184 G_B: 0.632 cycle_B: 0.495 idt_B: 0.244 \n",
            "(epoch: 165, iters: 300, time: 0.634, data: 0.001) D_A: 0.137 G_A: 0.486 cycle_A: 0.939 idt_A: 0.244 D_B: 0.249 G_B: 0.229 cycle_B: 0.350 idt_B: 0.440 \n",
            "(epoch: 165, iters: 400, time: 1.972, data: 0.001) D_A: 0.193 G_A: 0.211 cycle_A: 0.682 idt_A: 0.268 D_B: 0.067 G_B: 0.493 cycle_B: 0.664 idt_B: 0.334 \n",
            "(epoch: 165, iters: 500, time: 0.634, data: 0.001) D_A: 0.259 G_A: 0.423 cycle_A: 0.376 idt_A: 0.300 D_B: 0.072 G_B: 0.295 cycle_B: 0.528 idt_B: 0.174 \n",
            "saving the model at the end of epoch 165, iters 52500\n",
            "End of epoch 165 / 200 \t Time Taken: 257 sec\n",
            "learning rate 0.0000693 -> 0.0000673\n",
            "(epoch: 166, iters: 100, time: 0.637, data: 0.120) D_A: 0.026 G_A: 0.648 cycle_A: 0.616 idt_A: 0.383 D_B: 0.118 G_B: 0.360 cycle_B: 0.568 idt_B: 0.276 \n",
            "(epoch: 166, iters: 200, time: 0.636, data: 0.001) D_A: 0.113 G_A: 0.583 cycle_A: 0.427 idt_A: 0.307 D_B: 0.123 G_B: 0.769 cycle_B: 0.548 idt_B: 0.360 \n",
            "(epoch: 166, iters: 300, time: 2.293, data: 0.001) D_A: 0.091 G_A: 0.456 cycle_A: 0.438 idt_A: 0.181 D_B: 0.140 G_B: 1.022 cycle_B: 0.292 idt_B: 0.158 \n",
            "(epoch: 166, iters: 400, time: 0.637, data: 0.001) D_A: 0.042 G_A: 0.626 cycle_A: 0.465 idt_A: 0.223 D_B: 0.206 G_B: 0.542 cycle_B: 0.518 idt_B: 0.239 \n",
            "(epoch: 166, iters: 500, time: 0.639, data: 0.001) D_A: 0.159 G_A: 0.373 cycle_A: 0.370 idt_A: 0.302 D_B: 0.227 G_B: 0.479 cycle_B: 0.576 idt_B: 0.337 \n",
            "End of epoch 166 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0000673 -> 0.0000653\n",
            "(epoch: 167, iters: 100, time: 0.635, data: 0.105) D_A: 0.389 G_A: 0.271 cycle_A: 0.579 idt_A: 0.289 D_B: 0.147 G_B: 0.365 cycle_B: 0.463 idt_B: 0.230 \n",
            "(epoch: 167, iters: 200, time: 2.100, data: 0.002) D_A: 0.196 G_A: 0.643 cycle_A: 0.354 idt_A: 0.255 D_B: 0.198 G_B: 0.618 cycle_B: 0.382 idt_B: 0.217 \n",
            "(epoch: 167, iters: 300, time: 0.635, data: 0.001) D_A: 0.097 G_A: 0.633 cycle_A: 0.369 idt_A: 0.323 D_B: 0.120 G_B: 0.398 cycle_B: 0.537 idt_B: 0.162 \n",
            "(epoch: 167, iters: 400, time: 0.639, data: 0.002) D_A: 0.214 G_A: 0.750 cycle_A: 0.441 idt_A: 0.354 D_B: 0.115 G_B: 0.417 cycle_B: 0.354 idt_B: 0.213 \n",
            "(epoch: 167, iters: 500, time: 0.637, data: 0.001) D_A: 0.115 G_A: 0.458 cycle_A: 0.653 idt_A: 0.304 D_B: 0.190 G_B: 0.810 cycle_B: 0.466 idt_B: 0.301 \n",
            "End of epoch 167 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0000653 -> 0.0000634\n",
            "(epoch: 168, iters: 100, time: 2.089, data: 0.132) D_A: 0.169 G_A: 0.584 cycle_A: 0.351 idt_A: 0.291 D_B: 0.126 G_B: 0.592 cycle_B: 0.520 idt_B: 0.168 \n",
            "(epoch: 168, iters: 200, time: 0.634, data: 0.001) D_A: 0.225 G_A: 0.544 cycle_A: 0.812 idt_A: 0.283 D_B: 0.183 G_B: 0.422 cycle_B: 0.621 idt_B: 0.347 \n",
            "(epoch: 168, iters: 300, time: 0.634, data: 0.001) D_A: 0.104 G_A: 0.501 cycle_A: 0.326 idt_A: 0.208 D_B: 0.201 G_B: 0.643 cycle_B: 0.473 idt_B: 0.196 \n",
            "(epoch: 168, iters: 400, time: 0.638, data: 0.001) D_A: 0.156 G_A: 0.655 cycle_A: 0.685 idt_A: 0.213 D_B: 0.144 G_B: 0.386 cycle_B: 0.351 idt_B: 0.384 \n",
            "(epoch: 168, iters: 500, time: 2.350, data: 0.002) D_A: 0.159 G_A: 0.575 cycle_A: 0.382 idt_A: 0.244 D_B: 0.121 G_B: 0.397 cycle_B: 0.492 idt_B: 0.192 \n",
            "End of epoch 168 / 200 \t Time Taken: 257 sec\n",
            "learning rate 0.0000634 -> 0.0000614\n",
            "(epoch: 169, iters: 100, time: 0.634, data: 0.119) D_A: 0.111 G_A: 0.525 cycle_A: 0.403 idt_A: 0.168 D_B: 0.133 G_B: 0.657 cycle_B: 0.292 idt_B: 0.207 \n",
            "(epoch: 169, iters: 200, time: 0.635, data: 0.001) D_A: 0.167 G_A: 0.438 cycle_A: 0.498 idt_A: 0.262 D_B: 0.166 G_B: 0.653 cycle_B: 0.512 idt_B: 0.247 \n",
            "(epoch: 169, iters: 300, time: 0.635, data: 0.002) D_A: 0.231 G_A: 0.617 cycle_A: 0.488 idt_A: 0.238 D_B: 0.228 G_B: 0.303 cycle_B: 0.446 idt_B: 0.291 \n",
            "(epoch: 169, iters: 400, time: 2.044, data: 0.001) D_A: 0.235 G_A: 0.429 cycle_A: 0.418 idt_A: 0.244 D_B: 0.172 G_B: 0.270 cycle_B: 0.498 idt_B: 0.220 \n",
            "(epoch: 169, iters: 500, time: 0.636, data: 0.001) D_A: 0.057 G_A: 0.420 cycle_A: 0.495 idt_A: 0.253 D_B: 0.160 G_B: 0.311 cycle_B: 0.538 idt_B: 0.220 \n",
            "End of epoch 169 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0000614 -> 0.0000594\n",
            "(epoch: 170, iters: 100, time: 0.637, data: 0.103) D_A: 0.049 G_A: 0.610 cycle_A: 0.450 idt_A: 0.227 D_B: 0.151 G_B: 0.582 cycle_B: 0.422 idt_B: 0.203 \n",
            "(epoch: 170, iters: 200, time: 0.633, data: 0.001) D_A: 0.096 G_A: 0.408 cycle_A: 0.337 idt_A: 0.254 D_B: 0.059 G_B: 0.587 cycle_B: 0.480 idt_B: 0.212 \n",
            "(epoch: 170, iters: 300, time: 2.030, data: 0.002) D_A: 0.106 G_A: 0.486 cycle_A: 0.485 idt_A: 0.230 D_B: 0.190 G_B: 0.654 cycle_B: 0.441 idt_B: 0.233 \n",
            "(epoch: 170, iters: 400, time: 0.634, data: 0.002) D_A: 0.176 G_A: 0.866 cycle_A: 0.511 idt_A: 0.194 D_B: 0.164 G_B: 0.295 cycle_B: 0.349 idt_B: 0.250 \n",
            "(epoch: 170, iters: 500, time: 0.637, data: 0.002) D_A: 0.159 G_A: 0.489 cycle_A: 0.559 idt_A: 0.224 D_B: 0.150 G_B: 0.564 cycle_B: 0.381 idt_B: 0.363 \n",
            "saving the latest model (epoch 170, total_iters 55000)\n",
            "saving the model at the end of epoch 170, iters 55000\n",
            "End of epoch 170 / 200 \t Time Taken: 257 sec\n",
            "learning rate 0.0000594 -> 0.0000574\n",
            "(epoch: 171, iters: 100, time: 0.635, data: 0.125) D_A: 0.285 G_A: 0.785 cycle_A: 0.535 idt_A: 0.228 D_B: 0.291 G_B: 0.170 cycle_B: 0.451 idt_B: 0.325 \n",
            "(epoch: 171, iters: 200, time: 2.237, data: 0.002) D_A: 0.041 G_A: 0.322 cycle_A: 0.727 idt_A: 0.214 D_B: 0.115 G_B: 0.979 cycle_B: 0.478 idt_B: 0.272 \n",
            "(epoch: 171, iters: 300, time: 0.636, data: 0.002) D_A: 0.087 G_A: 0.526 cycle_A: 0.505 idt_A: 0.342 D_B: 0.098 G_B: 0.215 cycle_B: 0.598 idt_B: 0.313 \n",
            "(epoch: 171, iters: 400, time: 0.636, data: 0.001) D_A: 0.156 G_A: 0.494 cycle_A: 0.434 idt_A: 0.220 D_B: 0.122 G_B: 0.600 cycle_B: 0.478 idt_B: 0.149 \n",
            "(epoch: 171, iters: 500, time: 0.637, data: 0.002) D_A: 0.100 G_A: 0.318 cycle_A: 0.434 idt_A: 0.195 D_B: 0.225 G_B: 0.746 cycle_B: 0.421 idt_B: 0.215 \n",
            "End of epoch 171 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0000574 -> 0.0000554\n",
            "(epoch: 172, iters: 100, time: 2.094, data: 0.108) D_A: 0.203 G_A: 0.425 cycle_A: 0.372 idt_A: 0.228 D_B: 0.103 G_B: 0.402 cycle_B: 0.502 idt_B: 0.217 \n",
            "(epoch: 172, iters: 200, time: 0.635, data: 0.001) D_A: 0.194 G_A: 0.326 cycle_A: 0.380 idt_A: 0.215 D_B: 0.093 G_B: 0.600 cycle_B: 0.423 idt_B: 0.183 \n",
            "(epoch: 172, iters: 300, time: 0.637, data: 0.002) D_A: 0.080 G_A: 0.100 cycle_A: 0.598 idt_A: 0.280 D_B: 0.366 G_B: 0.285 cycle_B: 0.451 idt_B: 0.355 \n",
            "(epoch: 172, iters: 400, time: 0.637, data: 0.001) D_A: 0.321 G_A: 0.525 cycle_A: 0.356 idt_A: 0.315 D_B: 0.253 G_B: 0.310 cycle_B: 0.622 idt_B: 0.181 \n",
            "(epoch: 172, iters: 500, time: 1.990, data: 0.001) D_A: 0.074 G_A: 0.501 cycle_A: 0.382 idt_A: 0.234 D_B: 0.140 G_B: 0.634 cycle_B: 0.636 idt_B: 0.155 \n",
            "End of epoch 172 / 200 \t Time Taken: 257 sec\n",
            "learning rate 0.0000554 -> 0.0000535\n",
            "(epoch: 173, iters: 100, time: 0.637, data: 0.122) D_A: 0.190 G_A: 0.480 cycle_A: 0.451 idt_A: 0.215 D_B: 0.178 G_B: 0.318 cycle_B: 0.417 idt_B: 0.240 \n",
            "(epoch: 173, iters: 200, time: 0.634, data: 0.001) D_A: 0.148 G_A: 0.467 cycle_A: 1.387 idt_A: 0.205 D_B: 0.097 G_B: 0.686 cycle_B: 0.551 idt_B: 0.287 \n",
            "(epoch: 173, iters: 300, time: 0.636, data: 0.001) D_A: 0.191 G_A: 0.366 cycle_A: 0.395 idt_A: 0.291 D_B: 0.089 G_B: 0.313 cycle_B: 0.586 idt_B: 0.156 \n",
            "(epoch: 173, iters: 400, time: 2.309, data: 0.002) D_A: 0.137 G_A: 0.441 cycle_A: 0.417 idt_A: 0.188 D_B: 0.138 G_B: 0.449 cycle_B: 0.376 idt_B: 0.210 \n",
            "(epoch: 173, iters: 500, time: 0.636, data: 0.001) D_A: 0.241 G_A: 0.706 cycle_A: 0.542 idt_A: 0.210 D_B: 0.121 G_B: 0.184 cycle_B: 0.443 idt_B: 0.221 \n",
            "End of epoch 173 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0000535 -> 0.0000515\n",
            "(epoch: 174, iters: 100, time: 0.636, data: 0.125) D_A: 0.039 G_A: 0.527 cycle_A: 0.475 idt_A: 0.272 D_B: 0.164 G_B: 0.374 cycle_B: 0.634 idt_B: 0.207 \n",
            "(epoch: 174, iters: 200, time: 0.634, data: 0.002) D_A: 0.156 G_A: 0.357 cycle_A: 0.510 idt_A: 0.245 D_B: 0.131 G_B: 1.119 cycle_B: 0.466 idt_B: 0.276 \n",
            "(epoch: 174, iters: 300, time: 2.190, data: 0.001) D_A: 0.187 G_A: 0.440 cycle_A: 0.353 idt_A: 0.201 D_B: 0.175 G_B: 0.286 cycle_B: 0.396 idt_B: 0.135 \n",
            "(epoch: 174, iters: 400, time: 0.635, data: 0.001) D_A: 0.092 G_A: 0.587 cycle_A: 0.510 idt_A: 0.267 D_B: 0.268 G_B: 0.773 cycle_B: 0.564 idt_B: 0.276 \n",
            "(epoch: 174, iters: 500, time: 0.635, data: 0.001) D_A: 0.275 G_A: 0.113 cycle_A: 0.533 idt_A: 0.262 D_B: 0.135 G_B: 0.711 cycle_B: 0.602 idt_B: 0.298 \n",
            "End of epoch 174 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0000515 -> 0.0000495\n",
            "(epoch: 175, iters: 100, time: 0.635, data: 0.122) D_A: 0.217 G_A: 0.336 cycle_A: 0.529 idt_A: 0.285 D_B: 0.133 G_B: 0.482 cycle_B: 0.573 idt_B: 0.267 \n",
            "(epoch: 175, iters: 200, time: 2.043, data: 0.002) D_A: 0.092 G_A: 0.465 cycle_A: 0.596 idt_A: 0.249 D_B: 0.095 G_B: 0.562 cycle_B: 0.508 idt_B: 0.250 \n",
            "(epoch: 175, iters: 300, time: 0.636, data: 0.002) D_A: 0.063 G_A: 0.266 cycle_A: 0.492 idt_A: 0.358 D_B: 0.349 G_B: 0.241 cycle_B: 0.872 idt_B: 0.281 \n",
            "(epoch: 175, iters: 400, time: 0.635, data: 0.002) D_A: 0.171 G_A: 0.811 cycle_A: 0.537 idt_A: 0.228 D_B: 0.148 G_B: 0.496 cycle_B: 0.569 idt_B: 0.295 \n",
            "(epoch: 175, iters: 500, time: 0.633, data: 0.001) D_A: 0.148 G_A: 0.367 cycle_A: 0.421 idt_A: 0.232 D_B: 0.142 G_B: 0.420 cycle_B: 0.484 idt_B: 0.240 \n",
            "saving the model at the end of epoch 175, iters 57500\n",
            "End of epoch 175 / 200 \t Time Taken: 257 sec\n",
            "learning rate 0.0000495 -> 0.0000475\n",
            "(epoch: 176, iters: 100, time: 2.431, data: 0.117) D_A: 0.095 G_A: 0.620 cycle_A: 0.463 idt_A: 0.276 D_B: 0.165 G_B: 0.413 cycle_B: 0.590 idt_B: 0.281 \n",
            "(epoch: 176, iters: 200, time: 0.634, data: 0.002) D_A: 0.136 G_A: 0.328 cycle_A: 0.417 idt_A: 0.133 D_B: 0.314 G_B: 0.464 cycle_B: 0.425 idt_B: 0.240 \n",
            "(epoch: 176, iters: 300, time: 0.632, data: 0.001) D_A: 0.081 G_A: 0.785 cycle_A: 0.474 idt_A: 0.196 D_B: 0.143 G_B: 0.753 cycle_B: 0.520 idt_B: 0.218 \n",
            "(epoch: 176, iters: 400, time: 0.634, data: 0.002) D_A: 0.244 G_A: 0.355 cycle_A: 0.353 idt_A: 0.233 D_B: 0.259 G_B: 0.372 cycle_B: 0.472 idt_B: 0.200 \n",
            "(epoch: 176, iters: 500, time: 2.147, data: 0.001) D_A: 0.227 G_A: 0.178 cycle_A: 0.246 idt_A: 0.257 D_B: 0.184 G_B: 0.363 cycle_B: 0.459 idt_B: 0.119 \n",
            "End of epoch 176 / 200 \t Time Taken: 257 sec\n",
            "learning rate 0.0000475 -> 0.0000455\n",
            "(epoch: 177, iters: 100, time: 0.635, data: 0.110) D_A: 0.080 G_A: 0.558 cycle_A: 0.501 idt_A: 0.275 D_B: 0.097 G_B: 0.607 cycle_B: 0.413 idt_B: 0.232 \n",
            "(epoch: 177, iters: 200, time: 0.639, data: 0.002) D_A: 0.085 G_A: 0.845 cycle_A: 0.589 idt_A: 0.394 D_B: 0.108 G_B: 0.478 cycle_B: 0.784 idt_B: 0.272 \n",
            "(epoch: 177, iters: 300, time: 0.634, data: 0.001) D_A: 0.095 G_A: 0.658 cycle_A: 0.491 idt_A: 0.336 D_B: 0.137 G_B: 0.563 cycle_B: 0.558 idt_B: 0.265 \n",
            "(epoch: 177, iters: 400, time: 2.354, data: 0.002) D_A: 0.182 G_A: 0.561 cycle_A: 0.540 idt_A: 0.161 D_B: 0.113 G_B: 0.180 cycle_B: 0.302 idt_B: 0.235 \n",
            "(epoch: 177, iters: 500, time: 0.636, data: 0.001) D_A: 0.144 G_A: 0.450 cycle_A: 0.376 idt_A: 0.234 D_B: 0.178 G_B: 0.681 cycle_B: 0.421 idt_B: 0.286 \n",
            "End of epoch 177 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0000455 -> 0.0000436\n",
            "(epoch: 178, iters: 100, time: 0.636, data: 0.113) D_A: 0.173 G_A: 0.430 cycle_A: 0.382 idt_A: 0.220 D_B: 0.121 G_B: 0.526 cycle_B: 0.406 idt_B: 0.179 \n",
            "(epoch: 178, iters: 200, time: 0.634, data: 0.002) D_A: 0.055 G_A: 0.770 cycle_A: 0.711 idt_A: 0.234 D_B: 0.047 G_B: 0.637 cycle_B: 0.386 idt_B: 0.463 \n",
            "(epoch: 178, iters: 300, time: 2.052, data: 0.002) D_A: 0.108 G_A: 0.431 cycle_A: 0.480 idt_A: 0.333 D_B: 0.138 G_B: 0.714 cycle_B: 0.552 idt_B: 0.320 \n",
            "(epoch: 178, iters: 400, time: 0.637, data: 0.001) D_A: 0.154 G_A: 0.279 cycle_A: 0.432 idt_A: 0.339 D_B: 0.225 G_B: 0.541 cycle_B: 0.569 idt_B: 0.252 \n",
            "(epoch: 178, iters: 500, time: 0.633, data: 0.002) D_A: 0.254 G_A: 0.403 cycle_A: 0.590 idt_A: 0.263 D_B: 0.095 G_B: 0.495 cycle_B: 0.498 idt_B: 0.290 \n",
            "End of epoch 178 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0000436 -> 0.0000416\n",
            "(epoch: 179, iters: 100, time: 0.636, data: 0.115) D_A: 0.107 G_A: 0.501 cycle_A: 0.504 idt_A: 0.230 D_B: 0.155 G_B: 0.511 cycle_B: 0.411 idt_B: 0.257 \n",
            "(epoch: 179, iters: 200, time: 2.139, data: 0.001) D_A: 0.127 G_A: 0.303 cycle_A: 0.414 idt_A: 0.251 D_B: 0.152 G_B: 0.752 cycle_B: 0.487 idt_B: 0.173 \n",
            "(epoch: 179, iters: 300, time: 0.636, data: 0.002) D_A: 0.104 G_A: 0.396 cycle_A: 0.592 idt_A: 0.255 D_B: 0.170 G_B: 0.362 cycle_B: 0.588 idt_B: 0.314 \n",
            "(epoch: 179, iters: 400, time: 0.638, data: 0.001) D_A: 0.151 G_A: 0.460 cycle_A: 0.538 idt_A: 0.274 D_B: 0.186 G_B: 0.341 cycle_B: 0.530 idt_B: 0.296 \n",
            "(epoch: 179, iters: 500, time: 0.636, data: 0.002) D_A: 0.130 G_A: 0.620 cycle_A: 0.536 idt_A: 0.441 D_B: 0.255 G_B: 0.381 cycle_B: 0.822 idt_B: 0.235 \n",
            "End of epoch 179 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0000416 -> 0.0000396\n",
            "(epoch: 180, iters: 100, time: 2.335, data: 0.111) D_A: 0.069 G_A: 0.487 cycle_A: 0.529 idt_A: 0.327 D_B: 0.189 G_B: 0.451 cycle_B: 0.617 idt_B: 0.329 \n",
            "(epoch: 180, iters: 200, time: 0.636, data: 0.002) D_A: 0.290 G_A: 0.487 cycle_A: 0.386 idt_A: 0.246 D_B: 0.169 G_B: 0.284 cycle_B: 0.486 idt_B: 0.221 \n",
            "(epoch: 180, iters: 300, time: 0.635, data: 0.001) D_A: 0.081 G_A: 0.495 cycle_A: 0.510 idt_A: 0.200 D_B: 0.077 G_B: 0.863 cycle_B: 0.368 idt_B: 0.177 \n",
            "(epoch: 180, iters: 400, time: 0.637, data: 0.001) D_A: 0.327 G_A: 0.388 cycle_A: 0.396 idt_A: 0.248 D_B: 0.155 G_B: 0.559 cycle_B: 0.446 idt_B: 0.207 \n",
            "(epoch: 180, iters: 500, time: 2.066, data: 0.002) D_A: 0.098 G_A: 0.453 cycle_A: 0.558 idt_A: 0.295 D_B: 0.081 G_B: 0.982 cycle_B: 0.708 idt_B: 0.465 \n",
            "saving the latest model (epoch 180, total_iters 60000)\n",
            "saving the model at the end of epoch 180, iters 60000\n",
            "End of epoch 180 / 200 \t Time Taken: 259 sec\n",
            "learning rate 0.0000396 -> 0.0000376\n",
            "(epoch: 181, iters: 100, time: 0.636, data: 0.121) D_A: 0.054 G_A: 0.665 cycle_A: 0.352 idt_A: 0.265 D_B: 0.108 G_B: 0.514 cycle_B: 0.411 idt_B: 0.203 \n",
            "(epoch: 181, iters: 200, time: 0.635, data: 0.002) D_A: 0.309 G_A: 0.457 cycle_A: 0.337 idt_A: 0.225 D_B: 0.158 G_B: 0.410 cycle_B: 0.349 idt_B: 0.182 \n",
            "(epoch: 181, iters: 300, time: 0.633, data: 0.002) D_A: 0.105 G_A: 0.653 cycle_A: 0.383 idt_A: 0.221 D_B: 0.222 G_B: 0.437 cycle_B: 0.383 idt_B: 0.204 \n",
            "(epoch: 181, iters: 400, time: 2.059, data: 0.001) D_A: 0.156 G_A: 0.406 cycle_A: 0.511 idt_A: 0.327 D_B: 0.067 G_B: 0.232 cycle_B: 0.681 idt_B: 0.242 \n",
            "(epoch: 181, iters: 500, time: 0.635, data: 0.002) D_A: 0.150 G_A: 0.562 cycle_A: 0.434 idt_A: 0.221 D_B: 0.407 G_B: 0.509 cycle_B: 0.411 idt_B: 0.252 \n",
            "End of epoch 181 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0000376 -> 0.0000356\n",
            "(epoch: 182, iters: 100, time: 0.633, data: 0.114) D_A: 0.118 G_A: 0.560 cycle_A: 0.477 idt_A: 0.251 D_B: 0.064 G_B: 0.790 cycle_B: 0.398 idt_B: 0.402 \n",
            "(epoch: 182, iters: 200, time: 0.637, data: 0.002) D_A: 0.204 G_A: 0.373 cycle_A: 0.302 idt_A: 0.242 D_B: 0.146 G_B: 0.300 cycle_B: 0.484 idt_B: 0.166 \n",
            "(epoch: 182, iters: 300, time: 2.391, data: 0.001) D_A: 0.099 G_A: 0.464 cycle_A: 0.472 idt_A: 0.342 D_B: 0.096 G_B: 0.743 cycle_B: 0.690 idt_B: 0.194 \n",
            "(epoch: 182, iters: 400, time: 0.636, data: 0.002) D_A: 0.370 G_A: 0.526 cycle_A: 0.427 idt_A: 0.227 D_B: 0.314 G_B: 0.310 cycle_B: 0.470 idt_B: 0.340 \n",
            "(epoch: 182, iters: 500, time: 0.636, data: 0.002) D_A: 0.218 G_A: 0.277 cycle_A: 0.685 idt_A: 0.236 D_B: 0.238 G_B: 0.263 cycle_B: 0.508 idt_B: 0.355 \n",
            "End of epoch 182 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0000356 -> 0.0000337\n",
            "(epoch: 183, iters: 100, time: 0.633, data: 0.112) D_A: 0.080 G_A: 0.622 cycle_A: 0.482 idt_A: 0.163 D_B: 0.119 G_B: 0.288 cycle_B: 0.459 idt_B: 0.221 \n",
            "(epoch: 183, iters: 200, time: 2.195, data: 0.002) D_A: 0.243 G_A: 0.721 cycle_A: 0.411 idt_A: 0.220 D_B: 0.238 G_B: 0.381 cycle_B: 0.387 idt_B: 0.229 \n",
            "(epoch: 183, iters: 300, time: 0.636, data: 0.001) D_A: 0.178 G_A: 0.546 cycle_A: 0.441 idt_A: 0.225 D_B: 0.169 G_B: 0.440 cycle_B: 0.425 idt_B: 0.211 \n",
            "(epoch: 183, iters: 400, time: 0.631, data: 0.002) D_A: 0.155 G_A: 0.374 cycle_A: 0.352 idt_A: 0.208 D_B: 0.224 G_B: 0.396 cycle_B: 0.399 idt_B: 0.284 \n",
            "(epoch: 183, iters: 500, time: 0.635, data: 0.001) D_A: 0.059 G_A: 0.656 cycle_A: 0.371 idt_A: 0.266 D_B: 0.123 G_B: 0.869 cycle_B: 0.478 idt_B: 0.187 \n",
            "End of epoch 183 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0000337 -> 0.0000317\n",
            "(epoch: 184, iters: 100, time: 2.434, data: 0.111) D_A: 0.098 G_A: 0.359 cycle_A: 0.384 idt_A: 0.202 D_B: 0.184 G_B: 0.300 cycle_B: 0.452 idt_B: 0.171 \n",
            "(epoch: 184, iters: 200, time: 0.638, data: 0.002) D_A: 0.070 G_A: 0.621 cycle_A: 0.353 idt_A: 0.252 D_B: 0.177 G_B: 0.413 cycle_B: 0.551 idt_B: 0.211 \n",
            "(epoch: 184, iters: 300, time: 0.634, data: 0.002) D_A: 0.260 G_A: 0.543 cycle_A: 0.423 idt_A: 0.210 D_B: 0.090 G_B: 0.307 cycle_B: 0.290 idt_B: 0.220 \n",
            "(epoch: 184, iters: 400, time: 0.634, data: 0.001) D_A: 0.089 G_A: 0.394 cycle_A: 0.350 idt_A: 0.306 D_B: 0.187 G_B: 0.339 cycle_B: 0.488 idt_B: 0.181 \n",
            "(epoch: 184, iters: 500, time: 2.088, data: 0.001) D_A: 0.185 G_A: 0.436 cycle_A: 0.246 idt_A: 0.285 D_B: 0.161 G_B: 0.383 cycle_B: 0.596 idt_B: 0.147 \n",
            "End of epoch 184 / 200 \t Time Taken: 258 sec\n",
            "learning rate 0.0000317 -> 0.0000297\n",
            "(epoch: 185, iters: 100, time: 0.634, data: 0.107) D_A: 0.287 G_A: 0.539 cycle_A: 0.483 idt_A: 0.221 D_B: 0.056 G_B: 0.316 cycle_B: 0.366 idt_B: 0.248 \n",
            "(epoch: 185, iters: 200, time: 0.639, data: 0.001) D_A: 0.105 G_A: 0.431 cycle_A: 0.759 idt_A: 0.234 D_B: 0.167 G_B: 0.806 cycle_B: 0.420 idt_B: 0.335 \n",
            "(epoch: 185, iters: 300, time: 0.633, data: 0.001) D_A: 0.122 G_A: 0.425 cycle_A: 0.471 idt_A: 0.165 D_B: 0.185 G_B: 0.553 cycle_B: 0.310 idt_B: 0.231 \n",
            "(epoch: 185, iters: 400, time: 2.093, data: 0.002) D_A: 0.160 G_A: 0.535 cycle_A: 0.381 idt_A: 0.224 D_B: 0.060 G_B: 0.676 cycle_B: 0.444 idt_B: 0.275 \n",
            "(epoch: 185, iters: 500, time: 0.635, data: 0.002) D_A: 0.164 G_A: 0.775 cycle_A: 0.830 idt_A: 0.287 D_B: 0.066 G_B: 0.445 cycle_B: 0.557 idt_B: 0.729 \n",
            "saving the model at the end of epoch 185, iters 62500\n",
            "End of epoch 185 / 200 \t Time Taken: 257 sec\n",
            "learning rate 0.0000297 -> 0.0000277\n",
            "(epoch: 186, iters: 100, time: 0.634, data: 0.111) D_A: 0.051 G_A: 0.160 cycle_A: 0.354 idt_A: 0.321 D_B: 0.104 G_B: 0.381 cycle_B: 0.628 idt_B: 0.160 \n",
            "(epoch: 186, iters: 200, time: 0.635, data: 0.002) D_A: 0.213 G_A: 0.426 cycle_A: 0.338 idt_A: 0.228 D_B: 0.124 G_B: 0.406 cycle_B: 0.408 idt_B: 0.205 \n",
            "(epoch: 186, iters: 300, time: 2.359, data: 0.001) D_A: 0.145 G_A: 0.358 cycle_A: 0.369 idt_A: 0.247 D_B: 0.124 G_B: 0.622 cycle_B: 0.409 idt_B: 0.204 \n",
            "(epoch: 186, iters: 400, time: 0.638, data: 0.002) D_A: 0.059 G_A: 0.609 cycle_A: 0.514 idt_A: 0.258 D_B: 0.138 G_B: 0.605 cycle_B: 0.501 idt_B: 0.264 \n",
            "(epoch: 186, iters: 500, time: 0.635, data: 0.002) D_A: 0.064 G_A: 0.611 cycle_A: 0.386 idt_A: 0.163 D_B: 0.156 G_B: 0.678 cycle_B: 0.374 idt_B: 0.244 \n",
            "End of epoch 186 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0000277 -> 0.0000257\n",
            "(epoch: 187, iters: 100, time: 0.634, data: 0.110) D_A: 0.151 G_A: 0.257 cycle_A: 0.427 idt_A: 0.175 D_B: 0.114 G_B: 0.625 cycle_B: 0.321 idt_B: 0.261 \n",
            "(epoch: 187, iters: 200, time: 2.103, data: 0.001) D_A: 0.056 G_A: 0.575 cycle_A: 0.319 idt_A: 0.167 D_B: 0.086 G_B: 0.418 cycle_B: 0.292 idt_B: 0.247 \n",
            "(epoch: 187, iters: 300, time: 0.639, data: 0.001) D_A: 0.158 G_A: 0.568 cycle_A: 0.300 idt_A: 0.355 D_B: 0.163 G_B: 0.615 cycle_B: 0.585 idt_B: 0.164 \n",
            "(epoch: 187, iters: 400, time: 0.637, data: 0.002) D_A: 0.136 G_A: 0.552 cycle_A: 0.552 idt_A: 0.186 D_B: 0.101 G_B: 0.512 cycle_B: 0.360 idt_B: 0.232 \n",
            "(epoch: 187, iters: 500, time: 0.633, data: 0.002) D_A: 0.130 G_A: 0.514 cycle_A: 0.363 idt_A: 0.241 D_B: 0.143 G_B: 0.489 cycle_B: 0.508 idt_B: 0.179 \n",
            "End of epoch 187 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0000257 -> 0.0000238\n",
            "(epoch: 188, iters: 100, time: 2.161, data: 0.108) D_A: 0.181 G_A: 0.323 cycle_A: 0.317 idt_A: 0.207 D_B: 0.113 G_B: 0.501 cycle_B: 0.395 idt_B: 0.205 \n",
            "(epoch: 188, iters: 200, time: 0.636, data: 0.001) D_A: 0.147 G_A: 0.260 cycle_A: 0.264 idt_A: 0.164 D_B: 0.140 G_B: 1.082 cycle_B: 0.356 idt_B: 0.146 \n",
            "(epoch: 188, iters: 300, time: 0.634, data: 0.002) D_A: 0.113 G_A: 0.556 cycle_A: 0.296 idt_A: 0.351 D_B: 0.119 G_B: 0.632 cycle_B: 0.771 idt_B: 0.158 \n",
            "(epoch: 188, iters: 400, time: 0.634, data: 0.001) D_A: 0.130 G_A: 0.477 cycle_A: 0.466 idt_A: 0.183 D_B: 0.227 G_B: 0.361 cycle_B: 0.438 idt_B: 0.293 \n",
            "(epoch: 188, iters: 500, time: 2.341, data: 0.002) D_A: 0.099 G_A: 0.262 cycle_A: 0.765 idt_A: 0.261 D_B: 0.090 G_B: 0.481 cycle_B: 0.433 idt_B: 0.295 \n",
            "End of epoch 188 / 200 \t Time Taken: 258 sec\n",
            "learning rate 0.0000238 -> 0.0000218\n",
            "(epoch: 189, iters: 100, time: 0.635, data: 0.117) D_A: 0.059 G_A: 0.355 cycle_A: 0.425 idt_A: 0.168 D_B: 0.147 G_B: 0.456 cycle_B: 0.473 idt_B: 0.266 \n",
            "(epoch: 189, iters: 200, time: 0.629, data: 0.002) D_A: 0.143 G_A: 0.443 cycle_A: 0.324 idt_A: 0.280 D_B: 0.299 G_B: 0.449 cycle_B: 0.504 idt_B: 0.181 \n",
            "(epoch: 189, iters: 300, time: 0.635, data: 0.002) D_A: 0.187 G_A: 0.394 cycle_A: 0.396 idt_A: 0.381 D_B: 0.123 G_B: 0.600 cycle_B: 0.617 idt_B: 0.208 \n",
            "(epoch: 189, iters: 400, time: 2.063, data: 0.001) D_A: 0.246 G_A: 0.283 cycle_A: 0.616 idt_A: 0.254 D_B: 0.159 G_B: 0.601 cycle_B: 0.600 idt_B: 0.249 \n",
            "(epoch: 189, iters: 500, time: 0.631, data: 0.002) D_A: 0.185 G_A: 0.590 cycle_A: 0.421 idt_A: 0.218 D_B: 0.106 G_B: 0.516 cycle_B: 0.517 idt_B: 0.279 \n",
            "End of epoch 189 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0000218 -> 0.0000198\n",
            "(epoch: 190, iters: 100, time: 0.635, data: 0.115) D_A: 0.216 G_A: 0.297 cycle_A: 0.561 idt_A: 0.195 D_B: 0.162 G_B: 0.352 cycle_B: 0.341 idt_B: 0.283 \n",
            "(epoch: 190, iters: 200, time: 0.637, data: 0.001) D_A: 0.197 G_A: 0.371 cycle_A: 0.359 idt_A: 0.220 D_B: 0.163 G_B: 0.469 cycle_B: 0.418 idt_B: 0.201 \n",
            "(epoch: 190, iters: 300, time: 2.514, data: 0.001) D_A: 0.214 G_A: 0.474 cycle_A: 0.245 idt_A: 0.309 D_B: 0.121 G_B: 0.469 cycle_B: 0.447 idt_B: 0.138 \n",
            "(epoch: 190, iters: 400, time: 0.637, data: 0.002) D_A: 0.214 G_A: 0.247 cycle_A: 0.371 idt_A: 0.273 D_B: 0.258 G_B: 0.341 cycle_B: 0.570 idt_B: 0.226 \n",
            "(epoch: 190, iters: 500, time: 0.635, data: 0.001) D_A: 0.074 G_A: 0.761 cycle_A: 0.479 idt_A: 0.313 D_B: 0.053 G_B: 0.781 cycle_B: 0.525 idt_B: 0.414 \n",
            "saving the latest model (epoch 190, total_iters 65000)\n",
            "saving the model at the end of epoch 190, iters 65000\n",
            "End of epoch 190 / 200 \t Time Taken: 258 sec\n",
            "learning rate 0.0000198 -> 0.0000178\n",
            "(epoch: 191, iters: 100, time: 0.636, data: 0.122) D_A: 0.203 G_A: 0.437 cycle_A: 0.872 idt_A: 0.248 D_B: 0.065 G_B: 0.507 cycle_B: 0.539 idt_B: 0.309 \n",
            "(epoch: 191, iters: 200, time: 2.162, data: 0.002) D_A: 0.182 G_A: 0.417 cycle_A: 0.273 idt_A: 0.315 D_B: 0.177 G_B: 0.684 cycle_B: 0.623 idt_B: 0.152 \n",
            "(epoch: 191, iters: 300, time: 0.636, data: 0.001) D_A: 0.092 G_A: 0.355 cycle_A: 0.400 idt_A: 0.247 D_B: 0.067 G_B: 0.565 cycle_B: 0.503 idt_B: 0.190 \n",
            "(epoch: 191, iters: 400, time: 0.635, data: 0.002) D_A: 0.396 G_A: 0.381 cycle_A: 0.329 idt_A: 0.181 D_B: 0.224 G_B: 0.415 cycle_B: 0.358 idt_B: 0.199 \n",
            "(epoch: 191, iters: 500, time: 0.633, data: 0.002) D_A: 0.143 G_A: 0.485 cycle_A: 0.243 idt_A: 0.198 D_B: 0.163 G_B: 0.399 cycle_B: 0.373 idt_B: 0.115 \n",
            "End of epoch 191 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0000178 -> 0.0000158\n",
            "(epoch: 192, iters: 100, time: 2.174, data: 0.111) D_A: 0.061 G_A: 0.312 cycle_A: 0.353 idt_A: 0.237 D_B: 0.226 G_B: 0.608 cycle_B: 0.491 idt_B: 0.217 \n",
            "(epoch: 192, iters: 200, time: 0.635, data: 0.002) D_A: 0.176 G_A: 0.508 cycle_A: 0.621 idt_A: 0.259 D_B: 0.214 G_B: 0.400 cycle_B: 0.449 idt_B: 0.387 \n",
            "(epoch: 192, iters: 300, time: 0.636, data: 0.002) D_A: 0.218 G_A: 0.684 cycle_A: 0.460 idt_A: 0.229 D_B: 0.211 G_B: 0.291 cycle_B: 0.319 idt_B: 0.292 \n",
            "(epoch: 192, iters: 400, time: 0.635, data: 0.001) D_A: 0.096 G_A: 0.636 cycle_A: 0.402 idt_A: 0.175 D_B: 0.116 G_B: 0.744 cycle_B: 0.372 idt_B: 0.223 \n",
            "(epoch: 192, iters: 500, time: 2.475, data: 0.002) D_A: 0.259 G_A: 0.657 cycle_A: 0.334 idt_A: 0.293 D_B: 0.205 G_B: 0.255 cycle_B: 0.473 idt_B: 0.198 \n",
            "End of epoch 192 / 200 \t Time Taken: 258 sec\n",
            "learning rate 0.0000158 -> 0.0000139\n",
            "(epoch: 193, iters: 100, time: 0.634, data: 0.113) D_A: 0.129 G_A: 0.589 cycle_A: 0.521 idt_A: 0.171 D_B: 0.251 G_B: 0.257 cycle_B: 0.364 idt_B: 0.285 \n",
            "(epoch: 193, iters: 200, time: 0.636, data: 0.002) D_A: 0.064 G_A: 0.716 cycle_A: 0.383 idt_A: 0.275 D_B: 0.188 G_B: 0.382 cycle_B: 0.554 idt_B: 0.186 \n",
            "(epoch: 193, iters: 300, time: 0.636, data: 0.001) D_A: 0.053 G_A: 0.331 cycle_A: 0.553 idt_A: 0.282 D_B: 0.134 G_B: 0.750 cycle_B: 0.464 idt_B: 0.315 \n",
            "(epoch: 193, iters: 400, time: 2.235, data: 0.001) D_A: 0.101 G_A: 0.635 cycle_A: 0.344 idt_A: 0.254 D_B: 0.193 G_B: 0.897 cycle_B: 0.485 idt_B: 0.274 \n",
            "(epoch: 193, iters: 500, time: 0.634, data: 0.002) D_A: 0.079 G_A: 0.701 cycle_A: 0.363 idt_A: 0.152 D_B: 0.200 G_B: 0.617 cycle_B: 0.366 idt_B: 0.187 \n",
            "End of epoch 193 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0000139 -> 0.0000119\n",
            "(epoch: 194, iters: 100, time: 0.638, data: 0.114) D_A: 0.194 G_A: 0.352 cycle_A: 0.281 idt_A: 0.219 D_B: 0.234 G_B: 0.322 cycle_B: 0.447 idt_B: 0.163 \n",
            "(epoch: 194, iters: 200, time: 0.640, data: 0.001) D_A: 0.137 G_A: 0.480 cycle_A: 0.496 idt_A: 0.276 D_B: 0.101 G_B: 0.745 cycle_B: 0.479 idt_B: 0.256 \n",
            "(epoch: 194, iters: 300, time: 2.353, data: 0.002) D_A: 0.163 G_A: 0.268 cycle_A: 0.488 idt_A: 0.327 D_B: 0.237 G_B: 0.601 cycle_B: 0.752 idt_B: 0.274 \n",
            "(epoch: 194, iters: 400, time: 0.638, data: 0.002) D_A: 0.253 G_A: 0.706 cycle_A: 0.537 idt_A: 0.223 D_B: 0.219 G_B: 0.368 cycle_B: 0.483 idt_B: 0.294 \n",
            "(epoch: 194, iters: 500, time: 0.635, data: 0.002) D_A: 0.260 G_A: 0.449 cycle_A: 0.327 idt_A: 0.166 D_B: 0.219 G_B: 0.172 cycle_B: 0.235 idt_B: 0.187 \n",
            "End of epoch 194 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0000119 -> 0.0000099\n",
            "(epoch: 195, iters: 100, time: 0.633, data: 0.110) D_A: 0.157 G_A: 0.638 cycle_A: 0.800 idt_A: 0.167 D_B: 0.077 G_B: 0.565 cycle_B: 0.336 idt_B: 0.328 \n",
            "(epoch: 195, iters: 200, time: 2.192, data: 0.002) D_A: 0.199 G_A: 0.609 cycle_A: 0.430 idt_A: 0.181 D_B: 0.333 G_B: 0.663 cycle_B: 0.381 idt_B: 0.198 \n",
            "(epoch: 195, iters: 300, time: 0.635, data: 0.001) D_A: 0.065 G_A: 0.459 cycle_A: 0.426 idt_A: 0.167 D_B: 0.232 G_B: 0.478 cycle_B: 0.343 idt_B: 0.208 \n",
            "(epoch: 195, iters: 400, time: 0.635, data: 0.002) D_A: 0.229 G_A: 0.425 cycle_A: 0.772 idt_A: 0.245 D_B: 0.114 G_B: 0.384 cycle_B: 0.452 idt_B: 0.335 \n",
            "(epoch: 195, iters: 500, time: 0.636, data: 0.001) D_A: 0.187 G_A: 0.830 cycle_A: 0.490 idt_A: 0.160 D_B: 0.219 G_B: 0.255 cycle_B: 0.314 idt_B: 0.305 \n",
            "saving the model at the end of epoch 195, iters 67500\n",
            "End of epoch 195 / 200 \t Time Taken: 257 sec\n",
            "learning rate 0.0000099 -> 0.0000079\n",
            "(epoch: 196, iters: 100, time: 2.253, data: 0.110) D_A: 0.162 G_A: 0.513 cycle_A: 0.214 idt_A: 0.253 D_B: 0.072 G_B: 0.505 cycle_B: 0.382 idt_B: 0.099 \n",
            "(epoch: 196, iters: 200, time: 0.634, data: 0.001) D_A: 0.086 G_A: 0.744 cycle_A: 0.365 idt_A: 0.182 D_B: 0.137 G_B: 0.312 cycle_B: 0.423 idt_B: 0.179 \n",
            "(epoch: 196, iters: 300, time: 0.633, data: 0.001) D_A: 0.061 G_A: 0.562 cycle_A: 0.502 idt_A: 0.154 D_B: 0.157 G_B: 0.615 cycle_B: 0.303 idt_B: 0.260 \n",
            "(epoch: 196, iters: 400, time: 0.636, data: 0.002) D_A: 0.250 G_A: 0.579 cycle_A: 0.424 idt_A: 0.245 D_B: 0.113 G_B: 0.403 cycle_B: 0.357 idt_B: 0.242 \n",
            "(epoch: 196, iters: 500, time: 2.382, data: 0.002) D_A: 0.064 G_A: 0.665 cycle_A: 0.603 idt_A: 0.145 D_B: 0.127 G_B: 0.655 cycle_B: 0.365 idt_B: 0.300 \n",
            "End of epoch 196 / 200 \t Time Taken: 258 sec\n",
            "learning rate 0.0000079 -> 0.0000059\n",
            "(epoch: 197, iters: 100, time: 0.636, data: 0.120) D_A: 0.090 G_A: 0.479 cycle_A: 0.405 idt_A: 0.294 D_B: 0.193 G_B: 0.389 cycle_B: 0.705 idt_B: 0.299 \n",
            "(epoch: 197, iters: 200, time: 0.634, data: 0.002) D_A: 0.125 G_A: 0.396 cycle_A: 0.368 idt_A: 0.257 D_B: 0.171 G_B: 0.892 cycle_B: 0.358 idt_B: 0.205 \n",
            "(epoch: 197, iters: 300, time: 0.633, data: 0.001) D_A: 0.115 G_A: 0.559 cycle_A: 0.442 idt_A: 0.217 D_B: 0.319 G_B: 0.298 cycle_B: 0.487 idt_B: 0.255 \n",
            "(epoch: 197, iters: 400, time: 2.234, data: 0.002) D_A: 0.272 G_A: 0.308 cycle_A: 0.503 idt_A: 0.322 D_B: 0.145 G_B: 0.402 cycle_B: 0.677 idt_B: 0.274 \n",
            "(epoch: 197, iters: 500, time: 0.638, data: 0.001) D_A: 0.214 G_A: 0.293 cycle_A: 0.453 idt_A: 0.198 D_B: 0.100 G_B: 0.391 cycle_B: 0.388 idt_B: 0.200 \n",
            "End of epoch 197 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0000059 -> 0.0000040\n",
            "(epoch: 198, iters: 100, time: 0.634, data: 0.121) D_A: 0.088 G_A: 0.609 cycle_A: 0.406 idt_A: 0.230 D_B: 0.231 G_B: 0.596 cycle_B: 0.445 idt_B: 0.179 \n",
            "(epoch: 198, iters: 200, time: 0.632, data: 0.002) D_A: 0.060 G_A: 0.364 cycle_A: 0.440 idt_A: 0.285 D_B: 0.226 G_B: 0.487 cycle_B: 0.546 idt_B: 0.244 \n",
            "(epoch: 198, iters: 300, time: 2.530, data: 0.002) D_A: 0.110 G_A: 0.838 cycle_A: 0.895 idt_A: 0.242 D_B: 0.105 G_B: 0.661 cycle_B: 0.400 idt_B: 0.451 \n",
            "(epoch: 198, iters: 400, time: 0.636, data: 0.002) D_A: 0.224 G_A: 0.308 cycle_A: 0.653 idt_A: 0.328 D_B: 0.107 G_B: 0.407 cycle_B: 0.646 idt_B: 0.258 \n",
            "(epoch: 198, iters: 500, time: 0.636, data: 0.002) D_A: 0.121 G_A: 0.605 cycle_A: 0.407 idt_A: 0.251 D_B: 0.194 G_B: 0.650 cycle_B: 0.404 idt_B: 0.227 \n",
            "End of epoch 198 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0000040 -> 0.0000020\n",
            "(epoch: 199, iters: 100, time: 0.633, data: 0.118) D_A: 0.080 G_A: 0.601 cycle_A: 0.495 idt_A: 0.258 D_B: 0.166 G_B: 0.517 cycle_B: 0.502 idt_B: 0.265 \n",
            "(epoch: 199, iters: 200, time: 2.200, data: 0.002) D_A: 0.194 G_A: 0.441 cycle_A: 0.383 idt_A: 0.208 D_B: 0.077 G_B: 0.426 cycle_B: 0.390 idt_B: 0.162 \n",
            "(epoch: 199, iters: 300, time: 0.637, data: 0.001) D_A: 0.100 G_A: 0.481 cycle_A: 0.377 idt_A: 0.139 D_B: 0.133 G_B: 0.490 cycle_B: 0.383 idt_B: 0.244 \n",
            "(epoch: 199, iters: 400, time: 0.634, data: 0.002) D_A: 0.184 G_A: 0.300 cycle_A: 0.540 idt_A: 0.274 D_B: 0.135 G_B: 0.515 cycle_B: 0.483 idt_B: 0.296 \n",
            "(epoch: 199, iters: 500, time: 0.636, data: 0.002) D_A: 0.273 G_A: 0.413 cycle_A: 0.645 idt_A: 0.200 D_B: 0.116 G_B: 0.469 cycle_B: 0.376 idt_B: 0.430 \n",
            "End of epoch 199 / 200 \t Time Taken: 256 sec\n",
            "learning rate 0.0000020 -> 0.0000000\n",
            "(epoch: 200, iters: 100, time: 2.222, data: 0.117) D_A: 0.208 G_A: 0.418 cycle_A: 0.632 idt_A: 0.215 D_B: 0.129 G_B: 0.616 cycle_B: 0.406 idt_B: 0.301 \n",
            "(epoch: 200, iters: 200, time: 0.635, data: 0.002) D_A: 0.103 G_A: 0.606 cycle_A: 0.381 idt_A: 0.182 D_B: 0.189 G_B: 0.728 cycle_B: 0.419 idt_B: 0.298 \n",
            "(epoch: 200, iters: 300, time: 0.635, data: 0.002) D_A: 0.135 G_A: 0.540 cycle_A: 0.277 idt_A: 0.203 D_B: 0.085 G_B: 0.531 cycle_B: 0.404 idt_B: 0.137 \n",
            "(epoch: 200, iters: 400, time: 0.633, data: 0.001) D_A: 0.132 G_A: 0.543 cycle_A: 0.320 idt_A: 0.152 D_B: 0.186 G_B: 0.359 cycle_B: 0.269 idt_B: 0.156 \n",
            "(epoch: 200, iters: 500, time: 2.549, data: 0.001) D_A: 0.117 G_A: 0.677 cycle_A: 0.246 idt_A: 0.194 D_B: 0.145 G_B: 0.341 cycle_B: 0.375 idt_B: 0.125 \n",
            "saving the latest model (epoch 200, total_iters 70000)\n",
            "saving the model at the end of epoch 200, iters 70000\n",
            "End of epoch 200 / 200 \t Time Taken: 259 sec\n"
          ]
        }
      ],
      "source": [
        "#Training mit CycleGAN. Wir verwenden hierfür die T4-GPU von Google Colab\n",
        "\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Pfad zu den Checkpoints\n",
        "checkpoints_dir = '/content/drive/MyDrive/ML4B_cycleGAN/checkpoints/baroque2realism/'\n",
        "\n",
        "# Lade den Checkpoint\n",
        "checkpoint_G_A = torch.load(f'{checkpoints_dir}/latest_net_G_A.pth')\n",
        "\n",
        "# Versuche, den epoch_count aus dem Checkpoint zu laden\n",
        "epoch_count = checkpoint_G_A.get('epoch_count', 61)  # Standardmäßig auf 61 setzen, falls nicht vorhanden\n",
        "print(f\"Resuming training from epoch {epoch_count}\")\n",
        "\n",
        "# Jetzt führe das Training mit dem geladenen epoch_count aus\n",
        "!python train.py --dataroot /content/drive/MyDrive/ML4B_cycleGAN/data \\\n",
        "                 --name baroque2realism \\\n",
        "                 --model cycle_gan \\\n",
        "                 --checkpoints_dir /content/drive/MyDrive/ML4B_cycleGAN/checkpoints \\\n",
        "                 --continue_train \\\n",
        "                 --epoch_count {epoch_count}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "Rt8Bsncw6vzR",
        "outputId": "42a4d78f-a049-449c-9552-298cf4abd129"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU connection is active.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-31647cc94725>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GPU connection is active.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 10 Minuten Pause zwischen den Ausgaben\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Da die Laufzeit nach Inaktivität automatisch getrennt wird, benötigen wir während das Training läuft einen Code mit regelmäßiger Ausgabe\n",
        "\n",
        "import time\n",
        "\n",
        "while True:\n",
        "    print(\"GPU connection is active.\")\n",
        "    time.sleep(600)  # 10 Minuten Pause zwischen den Ausgaben\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujImNH02jJQk",
        "outputId": "921c0a71-8ba8-46f7-92ab-c7d285f511c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Anzahl der echten Bilder: 400\n",
            "Anzahl der generierten Bilder: 400\n",
            "Anzahl der rekonstruierten Bilder: 400\n",
            "Anzahl der Identitätsbilder: 400\n"
          ]
        }
      ],
      "source": [
        "# Ordnen der Bilder für die quantitative Evaluation des Models\n",
        "\n",
        "import os\n",
        "\n",
        "# Verzeichnis, in dem die Bilder gespeichert sind\n",
        "image_dir = '/content/drive/MyDrive/ML4B_cycleGAN/checkpoints/baroque2realism/web/images'\n",
        "\n",
        "# Listen für die Bilder der einzelnen Kategorien, da Bilder in real (original), fake (generiert), rec (rekonstruiert) und idt (Identitätsbilder) kategorisiert sind.\n",
        "real_images = []\n",
        "fake_images = []\n",
        "rec_images = []\n",
        "idt_images = []\n",
        "\n",
        "# Durchlaufe alle Bilder im Verzeichnis und ordne sie je nach Kategorie\n",
        "for filename in os.listdir(image_dir):\n",
        "    if filename.endswith(\".png\") or filename.endswith(\".jpg\"):\n",
        "        # Bestimme die Kategorie anhand des Dateinamens\n",
        "        if 'real' in filename:\n",
        "            real_images.append(os.path.join(image_dir, filename))\n",
        "        elif 'fake' in filename:\n",
        "            fake_images.append(os.path.join(image_dir, filename))\n",
        "        elif 'rec' in filename:\n",
        "            rec_images.append(os.path.join(image_dir, filename))\n",
        "        elif 'idt' in filename:\n",
        "            idt_images.append(os.path.join(image_dir, filename))\n",
        "\n",
        "# Ausgabe der Anzahl der Bilder in jeder Kategorie\n",
        "print(f\"Anzahl der echten Bilder: {len(real_images)}\")\n",
        "print(f\"Anzahl der generierten Bilder: {len(fake_images)}\")\n",
        "print(f\"Anzahl der rekonstruierten Bilder: {len(rec_images)}\")\n",
        "print(f\"Anzahl der Identitätsbilder: {len(idt_images)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-U71z51Wspj3",
        "outputId": "4951ff2c-87d0-4dca-b4c9-065a072ea87e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Anzahl der echten Bilder: 400\n",
            "Anzahl der generierten Bilder: 400\n",
            "FID: 105.95823669433594\n"
          ]
        }
      ],
      "source": [
        "# Berechnen des FID-Wert: misst den Unterschied zwischen den echten und den generierten Bildern. \n",
        "# unser Wert liegt bei 105.95823669433594. --> Viel Raum für Verbesserung\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from torchvision.models import inception_v3\n",
        "from scipy.linalg import sqrtm\n",
        "from PIL import Image\n",
        "\n",
        "def calculate_fid(real_images, generated_images):\n",
        "    # Lade das vortrainierte InceptionV3-Modell\n",
        "    inception_model = inception_v3(pretrained=True, transform_input=False)\n",
        "    inception_model.eval()\n",
        "\n",
        "    # Transformationen für die Bilder\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(299),\n",
        "        transforms.CenterCrop(299),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
        "    ])\n",
        "\n",
        "    def get_inception_features(image_paths):\n",
        "        features = []\n",
        "        for image_path in image_paths:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            image = transform(image).unsqueeze(0)  # Hinzufügen der Batch-Dimension\n",
        "            with torch.no_grad():\n",
        "                feature = inception_model(image)\n",
        "            features.append(feature)\n",
        "        return torch.cat(features, dim=0)\n",
        "\n",
        "    real_features = get_inception_features(real_images)\n",
        "    generated_features = get_inception_features(generated_images)\n",
        "\n",
        "    # Berechne den FID-Wert\n",
        "    mu1, sigma1 = real_features.mean(dim=0), torch.cov(real_features.T)\n",
        "    mu2, sigma2 = generated_features.mean(dim=0), torch.cov(generated_features.T)\n",
        "\n",
        "    diff = mu1 - mu2\n",
        "    covmean = sqrtm(sigma1 @ sigma2)\n",
        "\n",
        "    # Konvertiere den covmean in ein Torch Tensor\n",
        "    if isinstance(covmean, np.ndarray):  # Falls der covmean ein numpy.ndarray ist\n",
        "        covmean = torch.tensor(covmean)\n",
        "\n",
        "    # Falls der FID-Wert komplex ist, extrahiere nur den realen Teil\n",
        "    if torch.is_complex(covmean):\n",
        "        covmean = covmean.real\n",
        "\n",
        "    fid = (diff @ diff.T) + torch.trace(sigma1 + sigma2 - 2 * covmean)\n",
        "\n",
        "    # Falls der FID-Wert komplex ist, extrahiere nur den realen Teil\n",
        "    if torch.is_complex(fid):\n",
        "        fid = fid.real\n",
        "\n",
        "    return fid.item()\n",
        "\n",
        "\n",
        "# Verzeichnis, in dem die Bilder gespeichert sind\n",
        "image_dir = '/content/drive/MyDrive/ML4B_cycleGAN/checkpoints/baroque2realism/web/images'\n",
        "\n",
        "# Listen für die Bilder der einzelnen Kategorien\n",
        "real_images = []\n",
        "fake_images = []\n",
        "\n",
        "# Durchlaufe alle Bilder im Verzeichnis\n",
        "for filename in os.listdir(image_dir):\n",
        "    if filename.endswith(\".png\") or filename.endswith(\".jpg\"):\n",
        "        # Bestimme die Kategorie anhand des Dateinamens\n",
        "        if 'real' in filename:\n",
        "            real_images.append(os.path.join(image_dir, filename))\n",
        "        elif 'fake' in filename:\n",
        "            fake_images.append(os.path.join(image_dir, filename))\n",
        "\n",
        "# Ausgabe der Anzahl der Bilder in jeder Kategorie\n",
        "print(f\"Anzahl der echten Bilder: {len(real_images)}\")\n",
        "print(f\"Anzahl der generierten Bilder: {len(fake_images)}\")\n",
        "\n",
        "# Berechne den FID-Wert zwischen echten und generierten Bildern\n",
        "fid_value = calculate_fid(real_images, fake_images)\n",
        "print(f\"FID: {fid_value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFRUgZm2uFfK",
        "outputId": "613fc44c-51f4-4332-e452-fd4763b3136e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inception Score: 6.659684658050537 ± 0.8439376354217529\n"
          ]
        }
      ],
      "source": [
        "# Berechnen des Inception Scores:  berücksichtigt sowohl die Schärfe, Klarheit und Vielfalt der Bilder.\n",
        "# Unser Inception Score von 6.66 ist bereits ganz gut, könnte aber besser sein. Die geringe Standardabweichung von 0.84 zeigt, dass der Score über die Bilder konsistent bleibt.\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from torchvision.models import inception_v3\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def calculate_inception_score(image_paths, splits=10):\n",
        "    # Lade das vortrainierte InceptionV3 Modell\n",
        "    inception_model = inception_v3(pretrained=True, transform_input=False)\n",
        "    inception_model.eval()\n",
        "\n",
        "    # Transformationen für die Bilder (Größe 299x299 für InceptionV3)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(299),\n",
        "        transforms.CenterCrop(299),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),  # Normalisierung für Inception\n",
        "    ])\n",
        "\n",
        "    def get_inception_features(image_paths):\n",
        "        features = []\n",
        "        for image_path in image_paths:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            image = transform(image).unsqueeze(0)  # Hinzufügen der Batch-Dimension\n",
        "            with torch.no_grad():\n",
        "                feature = inception_model(image)\n",
        "            features.append(feature)\n",
        "        return torch.cat(features, dim=0)\n",
        "\n",
        "    # Holen Sie sich die Inception-Features für alle Bilder\n",
        "    inception_features = get_inception_features(image_paths)\n",
        "\n",
        "    # Berechne den Inception Score\n",
        "    scores = []\n",
        "    for i in range(splits):\n",
        "        # Berechne die log-Softmax-Werte\n",
        "        start = i * len(image_paths) // splits\n",
        "        end = (i + 1) * len(image_paths) // splits\n",
        "        split_features = inception_features[start:end]\n",
        "\n",
        "        softmax_probs = torch.nn.functional.softmax(split_features, dim=1)\n",
        "\n",
        "        # Berechne die mittlere Entropie über die Bilder des Splits\n",
        "        p_y = torch.mean(softmax_probs, dim=0)  # die marginale Verteilung von y\n",
        "        kl_divergence = torch.mean(torch.sum(softmax_probs * (torch.log(softmax_probs) - torch.log(p_y)), dim=1))\n",
        "        scores.append(torch.exp(kl_divergence))\n",
        "\n",
        "    # Durchschnitt und Standardabweichung der Scores\n",
        "    return np.mean(scores), np.std(scores)\n",
        "\n",
        "\n",
        "# Verzeichnis, in dem die Bilder gespeichert sind\n",
        "image_dir = '/content/drive/MyDrive/ML4B_cycleGAN/checkpoints/baroque2realism/web/images'\n",
        "\n",
        "# Liste der Bilder (echt oder generiert)\n",
        "image_paths = [os.path.join(image_dir, filename) for filename in os.listdir(image_dir) if filename.endswith(\".png\") or filename.endswith(\".jpg\")]\n",
        "\n",
        "# Berechne den Inception Score\n",
        "inception_score_mean, inception_score_std = calculate_inception_score(image_paths)\n",
        "print(f\"Inception Score: {inception_score_mean} ± {inception_score_std}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySEgfeKMv87P",
        "outputId": "f6609ed6-cad0-4a03-d655-c7e4a86b0e2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perceptual Path Length (PPL): 0.8187612295150757\n"
          ]
        }
      ],
      "source": [
        "# Berechnung des PPL \n",
        "# Unser PPL-Wert von 0.82 zeigt, dass das Modell im Allgemeinen relativ gute kontinuierliche Übergänge und konsistente Bildtransformationen erzeugt.\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "from torchvision.models import inception_v3\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Definiere die Funktion zur Berechnung der Perceptual Path Length (PPL)\n",
        "def calculate_ppl(model, latent_space_samples, num_steps=10):\n",
        "    \"\"\"\n",
        "    Berechnet die Perceptual Path Length (PPL) zwischen zwei Punkten im Latentraum.\n",
        "\n",
        "    :param model: Das Generator-Modell (z.B. CycleGAN oder ein anderes GAN-Modell)\n",
        "    :param latent_space_samples: Eine Liste mit zwei zufälligen Punkten im Latentraum\n",
        "    :param num_steps: Anzahl der Schritte, um den Pfad zwischen den Latentraum-Punkten zu unterteilen\n",
        "    :return: Der berechnete Perceptual Path Length (PPL)\n",
        "    \"\"\"\n",
        "    # Extrahiere die beiden Latentraum-Punkte\n",
        "    z1, z2 = latent_space_samples[0], latent_space_samples[1]\n",
        "\n",
        "    # Linie zwischen den Punkten im Latentraum (Lerp steht für \"linear interpolation\")\n",
        "    path = torch.stack([z1 + (z2 - z1) * (i / num_steps) for i in range(num_steps)])\n",
        "\n",
        "    # Wende das Modell an, um die Bilder entlang des Pfades zu generieren\n",
        "    images = model(path)\n",
        "\n",
        "    # Berechne die Perceptual Distance (Unterschied zwischen den Bildern entlang des Pfades)\n",
        "    perceptual_distance = 0\n",
        "    for i in range(1, len(images)):\n",
        "        perceptual_distance += torch.norm(images[i] - images[i-1])  # Norm als Distanzmaß\n",
        "\n",
        "    # Der Perceptual Path Length ist der durchschnittliche Unterschied über alle Schritte\n",
        "    ppl = perceptual_distance / num_steps\n",
        "\n",
        "    return ppl.item()\n",
        "\n",
        "# Lade ein vortrainiertes InceptionV3-Modell, um Perceptual Features zu extrahieren (optional)\n",
        "inception_model = inception_v3(pretrained=True, transform_input=False)\n",
        "inception_model.eval()\n",
        "\n",
        "# Beispiel: Erzeuge zufällige Latentraum-Punkte und berechne den PPL\n",
        "latent_dim = 100  # Typischer Latentraum-Dimension für viele GANs (kann angepasst werden)\n",
        "z1 = torch.randn(1, latent_dim)  # Zufälliger Punkt 1 im Latentraum\n",
        "z2 = torch.randn(1, latent_dim)  # Zufälliger Punkt 2 im Latentraum\n",
        "\n",
        "# Hier: Beispiel-Modell als Dummy verwenden (Dein tatsächliches Generator-Modell verwenden)\n",
        "class DummyGenerator(nn.Module):\n",
        "    def forward(self, z):\n",
        "        # Dummy-GAN, der die Latentraumpunkte zu Bildern umwandelt\n",
        "        return torch.tanh(z)  # Beispielhafter Generator, ersetze mit deinem echten Generator\n",
        "\n",
        "model = DummyGenerator()\n",
        "\n",
        "# Berechne den PPL\n",
        "latent_space_samples = [z1, z2]\n",
        "ppl_value = calculate_ppl(model, latent_space_samples)\n",
        "\n",
        "print(f\"Perceptual Path Length (PPL): {ppl_value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbnEKZKtrC-p"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5jNVWnaxWs4",
        "outputId": "e89881d0-9b6c-4c89-d80a-4dab9845a4ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.3534434531042553\n",
            "Recall: 0.3424976733477302\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# Berechnung von Precision und Recall\n",
        "# unsere Precision liegt bei 0.3534434531042553 --> ca. 35% der generierten Bilder sind ähnlich zum Original, aber sehr vile false positives.\n",
        "# unser Recall liegt bei 0.3424976733477302 -->  ca. 34 % der echten Bilder werden korrekt vom Modell als ähnliche generierte Bilder erkannt, aber noch viele false negatives\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "def calculate_precision_recall(real_images, generated_images, model, num_classes=1000):\n",
        "    \"\"\"\n",
        "    Berechnet Precision und Recall für generierte und echte Bilder.\n",
        "\n",
        "    :param real_images: Liste der echten Bildpfade\n",
        "    :param generated_images: Liste der generierten Bildpfade\n",
        "    :param model: Ein vortrainiertes Modell, das für die Klassifikation von Bildern verwendet wird\n",
        "    :param num_classes: Anzahl der Klassen im Modell (typischerweise 1000 für ImageNet)\n",
        "    :return: Precision und Recall\n",
        "    \"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(299),\n",
        "        transforms.CenterCrop(299),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    def predict_image(image_path):\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        image = transform(image).unsqueeze(0)  # Hinzufügen der Batch-Dimension\n",
        "        with torch.no_grad():\n",
        "            outputs = model(image)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "        return predicted.item()\n",
        "\n",
        "    real_labels = []\n",
        "    fake_labels = []\n",
        "\n",
        "    # Generiere Labels für echte und generierte Bilder\n",
        "    for image_path in real_images:\n",
        "        real_labels.append(predict_image(image_path))\n",
        "\n",
        "    for image_path in generated_images:\n",
        "        fake_labels.append(predict_image(image_path))\n",
        "\n",
        "    # Berechne Precision und Recall für multiclass\n",
        "    precision = precision_score(real_labels, fake_labels, average='macro', labels=np.unique(fake_labels))\n",
        "    recall = recall_score(real_labels, fake_labels, average='macro', labels=np.unique(fake_labels))\n",
        "\n",
        "    return precision, recall\n",
        "\n",
        "\n",
        "# Beispiel: Verzeichnis mit echten und generierten Bildern\n",
        "image_dir = '/content/drive/MyDrive/ML4B_cycleGAN/checkpoints/baroque2realism/web/images'\n",
        "\n",
        "# Liste der echten und generierten Bilder\n",
        "real_images = [os.path.join(image_dir, filename) for filename in os.listdir(image_dir) if 'real' in filename]\n",
        "generated_images = [os.path.join(image_dir, filename) for filename in os.listdir(image_dir) if 'fake' in filename]\n",
        "\n",
        "# Lade das vortrainierte InceptionV3-Modell\n",
        "model = models.inception_v3(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Berechne Precision und Recall\n",
        "precision, recall = calculate_precision_recall(real_images, generated_images, model)\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
